# Using Intel optimized embeddings API

We extends Langchain embeddings API to support loading optimized embedding models generated by [IntelÂ® Extension for Transformers](https://github.com/intel/intel-extension-for-transformers). This extension is specifically tailored to enhance the functionality and performance of RAG.

Intel optimized embeddings include:
```python
from langchain.embeddings import OptimizedHuggingFaceBgeEmbeddings
from langchain.embeddings import OptimizedHuggingFaceEmbeddings
from langchain.embeddings import OptimizedHuggingFaceInstructEmbeddings
```

## Preparing Environment
```bash
git clone https://github.com/lvliang-intel/intel_genai_kit_langchain.git
cd intel_genai_kit_langchain/libs/langchain/
pip install -e .
cd ../community/
pip install -e .
```

## Generating a quantized embedding model

Here we use [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) as an example to generate a quantized model.

```shell
python quantize_embedding.py --model_name_or_path=BAAI/bge-small-en-v1.5 --output_path=bge-small-en-v1.5-sts-int8-static
```


## Loading model with optimized embeddings API in Langchain

```python
from langchain.embeddings import OptimizedHuggingFaceBgeEmbeddings
model_name = "./bge-small-en-v1.5-sts-int8-static"
encode_kwargs = {'normalize_embeddings': True}
model = OptimizedHuggingFaceBgeEmbeddings(
    model_name=model_name,
    encode_kwargs=encode_kwargs,
    query_instruction="Represent this sentence for searching relevant passages:"
)

documents = ["foo bar"]
output = model.embed_documents(documents)
```
