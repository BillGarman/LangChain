{
 "cells": [
  {
   "cell_type": "raw",
   "id": "bc346658-6820-413a-bd8f-11bd3082fe43",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 0.5\n",
    "title: Why use LCEL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a5ae2-ed21-4923-b98f-723c111bac67",
   "metadata": {},
   "source": [
    ":::tip Previous: [Get started](/docs/expression_language/get_started)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331037f-be3f-4782-856f-d55dab952488",
   "metadata": {},
   "source": [
    "LCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.\n",
    "\n",
    "We laid out the main value props in the [overview](/docs/expression_language) and went through two simple use cases in the [get started](/docs/expression_language/get_started) section. To really understand why LCEL and LangChain in general are useful, it's also helpful to think about how we might recreate similar functionality without them.\n",
    "\n",
    "Let's try this with our [basic example](/docs/expression_language/get_started#basic_example) from the get started page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeac2b8-c441-4d8d-b313-1de0ab9c7e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3621b62-a037-42b8-8faa-59575608bb8b",
   "metadata": {},
   "source": [
    "<div style={{ width: \"50%\" }}>\n",
    "\n",
    "## Without LCEL\n",
    "\n",
    "### Invoke\n",
    "In the simplest case, we just want to be able to pass in a topic string and get back a joke string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e628905c-430e-4e4a-9d7c-c91d2f42052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "def manual_chain(topic: str) -> str:\n",
    "    prompt_value = f\"Tell me a short joke about {topic}\"\n",
    "    client = openai.OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt_value}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0b0513-77b8-4371-a20e-3e487cec7e7f",
   "metadata": {},
   "source": [
    "#### Stream\n",
    "\n",
    "If we want to stream results instead, we'll need to change our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2cc6dc-d70a-4c13-9258-452f14290da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def manual_chain_stream(topic: str) -> Iterator[str]:\n",
    "    prompt_value = f\"Tell me a short joke about {topic}\"\n",
    "    client = openai.OpenAI()\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_value}],\n",
    "        stream=True,\n",
    "    )\n",
    "    for response in stream:\n",
    "        content = response.choices[0].delta.content\n",
    "        if content is not None:\n",
    "            yield content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b41e78-ddeb-44d0-a58b-a0ea0c99a761",
   "metadata": {},
   "source": [
    "#### Batch\n",
    "\n",
    "If we want to run on a batch of inputs in parallel, we'll again need a new function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b492f13-73a6-48ed-8d4f-9ad634da9988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def manual_chain_batch(topics: list) -> list:\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        return list(executor.map(manual_chain, topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ba36f-eec1-4fc1-8cfe-fa242a7f7809",
   "metadata": {},
   "source": [
    "#### Async\n",
    "\n",
    "If you needed an asynchronous version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eabe6621-e815-41e3-9c9d-5aa561a69835",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def manual_chain_async(topic: str) -> str:\n",
    "    prompt_value = f\"Tell me a short joke about {topic}\"\n",
    "    client = openai.AsyncOpenAI()\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt_value}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6888245-1ebe-4768-a53b-e1fef6a8b379",
   "metadata": {},
   "source": [
    "#### LLM instead of chat model\n",
    "\n",
    "If we want to use a completion endpoint instead of a chat endpoint: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aca946b-acaa-4f7e-a3d0-ad8e3225e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_chain_completion(topic: str) -> str:\n",
    "    prompt_value = f\"Tell me a short joke about {topic}\"\n",
    "    client = openai.OpenAI()\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt_value,\n",
    "    )\n",
    "    return response.choices[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca115eaf-59ef-45c1-aac1-e8b0ce7db250",
   "metadata": {},
   "source": [
    "#### Different model provider\n",
    "\n",
    "If we want to use Anthropic instead of OpenAI: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde2ceb0-f65e-487b-9a32-137b0e9d79d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "\n",
    "def manual_chain_anthropic(topic: str) -> str:\n",
    "    prompt_value = f\"Human:\\n\\nTell me a short joke about {topic}\\n\\nAssistant:\"\n",
    "    client = anthropic.Anthropic()\n",
    "    response = client.completions.create(\n",
    "        model=\"claude-2\",\n",
    "        prompt=prompt_value,\n",
    "        max_tokens_to_sample=256,\n",
    "    )\n",
    "    return response.completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370dd4d7-b825-40c4-ae3c-2693cba2f22a",
   "metadata": {},
   "source": [
    "#### Logging\n",
    "\n",
    "If we want to log our intermediate results (we'll `print` here for illustrative purposes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a3c51-926d-48c6-b9ae-42bf8f14ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_chain_anthropic_logging(topic: str) -> str:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = f\"Human:\\n\\nTell me a short joke about {topic}\\n\\nAssistant:\"\n",
    "    print(f\"Formatted prompt: {prompt_value}\")\n",
    "    client = anthropic.Anthropic()\n",
    "    response = client.completions.create(\n",
    "        model=\"claude-2\",\n",
    "        prompt=prompt_value,\n",
    "        max_tokens_to_sample=256,\n",
    "    )\n",
    "    print(f\"Output: {response.completion}\")\n",
    "    return response.completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ce3c5-27a7-4954-9f0e-b94313597135",
   "metadata": {},
   "source": [
    "#### Fallbacks\n",
    "\n",
    "If you wanted to add retry or fallback logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49d512-bc83-4c5f-b56e-934b8343b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_chain_with_fallback(topic: str) -> str:\n",
    "    try:\n",
    "        return manual_chain(topic)\n",
    "    except Exception:\n",
    "        return manual_chain_anthropic(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ef59b5-2ce3-479e-a7ac-79e1e2f30e9c",
   "metadata": {},
   "source": [
    "</div>\n",
    "<div style={{ width: \"50%\" }}>\n",
    "### With LCEL\n",
    "\n",
    "Now let's take a look at how all of this work with LCEL. We'll use our chain from before (and for ease of use take in a string instead of a dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc0de76a-daf5-4ec0-ba7f-c63225821591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = {\"topic\": RunnablePassthrough()} | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d85dda-d63c-459f-99ec-5d6d669b5b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9eb899-e7c8-4ab5-aecd-d305cd716082",
   "metadata": {},
   "source": [
    "#### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f15ae5-8353-4fe6-b506-73c67ec9c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chain.stream(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff0ae2-f2ca-4463-bacb-634fc788b5bb",
   "metadata": {},
   "source": [
    "#### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9f4a7-5ded-47fb-9057-adb04ed3382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c49198-3ac3-4805-b898-063c45ce89fb",
   "metadata": {},
   "source": [
    "#### Async\n",
    "```python\n",
    "chain.ainvoke(\"ice cream)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c184ca63-e74d-478c-980c-2c19b459cccd",
   "metadata": {},
   "source": [
    "#### LLM instead of chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18118e-e901-42ec-a4a0-75d011bec10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "llm_chain = {\"topic\": RunnablePassthrough()} | prompt | llm | output_parser\n",
    "llm_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de0201-3980-4f78-b89e-c8c59f1c4e7d",
   "metadata": {},
   "source": [
    "If we wanted, we could even make the choice of chat model or llm runtime configurable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937fa94a-b019-450b-bec5-b6e3443fa903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "configurable_model = model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"model\"), default_key=\"chat_openai\", openai=llm\n",
    ")\n",
    "configurable_chain = {\"topic\": RunnablePassthrough()} | prompt | llm | output_parser\n",
    "configurable_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187eb0b-e86b-4845-a2b3-2355781e1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "configurable_chain.invoke(\"ice cream\", config={\"configurable\": {\"model\": \"openai\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900a52e-f858-4604-9413-7fa7cb04a8a5",
   "metadata": {},
   "source": [
    "#### Different model provider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b323c-f573-452a-8f81-98eb8d6906f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "\n",
    "anthropic = ChatAnthropic(model=\"claude-2\")\n",
    "anthropic_chain = {\"topic\": RunnablePassthrough()} | prompt | anthropic | output_parser\n",
    "anthropic_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e16de-a8db-4689-aeef-b2e76d9071cd",
   "metadata": {},
   "source": [
    "#### Logging\n",
    "\n",
    "By turning on LangSmith, every step of every chain is automatically logged. We set these environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6204f21-d2e7-4ac6-871f-b60b34e5bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842ec53-b58a-4689-97da-32ed17003981",
   "metadata": {},
   "source": [
    "And then get a trace of every chain run: {trace}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4274f4bd-3a78-4a28-a531-28ea7ac1efae",
   "metadata": {},
   "source": [
    "#### Fallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d8a0f-66eb-4c35-9529-74bec44ce4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallback_chain = chain.with_fallbacks([anthropic_chain])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58af836-26bd-4eab-97a0-76dd56d53430",
   "metadata": {},
   "source": [
    "</div>\n",
    "\n",
    "### Full code comparison\n",
    "<div style={{ width: \"50%\" }}>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3d71d-8c69-4dc4-81b7-95cd46b271c2",
   "metadata": {},
   "source": [
    "Our full code **with LCEL** looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c469a-545e-434e-bd6e-99745dd880a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chat_models import ChatAnthropic, ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "\n",
    "chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "openai = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "anthropic = ChatAnthropic(model=\"claude-2\")\n",
    "model = chat_openai.with_fallbacks([anthropic]).configurable_alternatives(\n",
    "    ConfigurableField(id=\"model\"),\n",
    "    default_key=\"chat_openai\",\n",
    "    openai=openai,\n",
    "    anthropic=anthropic,\n",
    ")\n",
    "\n",
    "chain = {\"topic\": RunnablePassthrough()} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a925003-4a1f-406f-87f2-1fd8965b9f87",
   "metadata": {},
   "source": [
    "</div>\n",
    "\n",
    "<div style={{ width: \"50%\" }}>\n",
    "Our code **without LCEL** might look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25837c5-829b-42a3-92b4-7e25831350c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Iterator, List, Tuple\n",
    "\n",
    "import openai\n",
    "\n",
    "prompt_template = \"Tell me a short joke about {topic}\"\n",
    "\n",
    "\n",
    "def manual_chain(topic: str, *, model: str = \"chat_openai\") -> str:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "\n",
    "    if model == \"chat_openai\":\n",
    "        print(f\"Full prompt: {prompt_value}\")\n",
    "        response = openai.OpenAI().chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt_value}]\n",
    "        )\n",
    "        output = response.choices[0].message.content\n",
    "    elif model == \"openai\":\n",
    "        print(f\"Full prompt: {prompt_value}\")\n",
    "        response = openai.OpenAI().completions.create(\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            prompt=prompt_value,\n",
    "        )\n",
    "        output = response.choices[0].text\n",
    "    elif model == \"anthropic\":\n",
    "        prompt_value = f\"Human:\\n\\n{prompt_value}\\n\\nAssistant:\"\n",
    "        print(f\"Full prompt: {prompt_value}\")\n",
    "        response = anthropic.Anthropic().completions.create(\n",
    "            model=\"claude-2\",\n",
    "            prompt=prompt_value,\n",
    "            max_tokens_to_sample=256,\n",
    "        )\n",
    "        output = response.completion\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid model {model}. Should be one of chat_openai, openai, anthropic.\"\n",
    "        )\n",
    "    print(f\"Output: {output}\")\n",
    "    return output\n",
    "\n",
    "\n",
    "def manual_chain_with_fallbacks(\n",
    "    topic: str, *, model: str = \"chat_openai\", fallbacks: Tuple[str] = (\"anthropic\",)\n",
    ") -> str:\n",
    "    for fallback in fallbacks:\n",
    "        try:\n",
    "            return manual_chain(topic, model=model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error {e}\")\n",
    "            model = fallback\n",
    "    raise e\n",
    "\n",
    "\n",
    "def manual_chain_batch(\n",
    "    topics: List[str],\n",
    "    *,\n",
    "    model: str = \"chat_openai\",\n",
    "    fallbacks: Tuple[str] = (\"anthropic\",),\n",
    ") -> List[str]:\n",
    "    models = [model] * len(topics)\n",
    "    fallbacks_list = [fallbacks] * len(topics)\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        return list(\n",
    "            executor.map(manual_chain_with_fallbacks, topics, models, fallbacks_list)\n",
    "        )\n",
    "\n",
    "\n",
    "def manual_chain_stream(topic: str, *, model: str = \"chat_openai\") -> Iterator[str]:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "\n",
    "    if model == \"chat_openai\":\n",
    "        print(f\"Full prompt: {prompt_value}\")\n",
    "        stream = openai.OpenAI().chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_value}],\n",
    "            stream=True,\n",
    "        )\n",
    "        for response in stream:\n",
    "            content = response.choices[0].delta.content\n",
    "            if content is not None:\n",
    "                yield content\n",
    "    elif model == \"openai\":\n",
    "        print(f\"Full prompt: {prompt_value}\")\n",
    "        stream = openai.OpenAI().completions.create(\n",
    "            model=\"gpt-3.5-turbo-instruct\", prompt=prompt_value, stream=True\n",
    "        )\n",
    "        for response in stream:\n",
    "            yield response.choices[0].text\n",
    "    elif model == \"anthropic\":\n",
    "        prompt_value = f\"Human:\\n\\n{prompt_value}\\n\\nAssistant:\"\n",
    "        print(f\"Full prompt: {prompt_value}\")\n",
    "        stream = anthropic.Anthropic().completions.create(\n",
    "            model=\"claude-2\", prompt=prompt_value, max_tokens_to_sample=256, stream=True\n",
    "        )\n",
    "        for response in stream:\n",
    "            yield response.completion\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid model {model}. Should be one of chat_openai, openai, anthropic.\"\n",
    "        )\n",
    "\n",
    "\n",
    "async def manual_chain_async(topic: str, *, model: str = \"chat_openai\") -> str:\n",
    "    # You get the idea :)\n",
    "    ...\n",
    "\n",
    "\n",
    "async def manual_chain_async_batch(\n",
    "    topics: List[str], *, model: str = \"chat_openai\"\n",
    ") -> List[str]:\n",
    "    ...\n",
    "\n",
    "\n",
    "async def manual_chain_async_stream(\n",
    "    topic: str, *, model: str = \"chat_openai\"\n",
    ") -> Iterator[str]:\n",
    "    ...\n",
    "\n",
    "\n",
    "def manual_chain_stream_with_fallbacks(\n",
    "    topic: str, *, model: str = \"chat_openai\", fallbacks: Tuple[str] = (\"anthropic\",)\n",
    ") -> Iterator[str]:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f608f388-b277-4242-a151-962d9eaa5b4e",
   "metadata": {},
   "source": [
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf3abfe-259c-470e-ba54-6cf729dfec02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-venv",
   "language": "python",
   "name": "poetry-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
