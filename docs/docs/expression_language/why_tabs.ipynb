{
 "cells": [
  {
   "cell_type": "raw",
   "id": "bc346658-6820-413a-bd8f-11bd3082fe43",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 0.5\n",
    "title: Why use LCEL (tabs)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a5ae2-ed21-4923-b98f-723c111bac67",
   "metadata": {},
   "source": [
    ":::tip Previous: [Get started](/docs/expression_language/get_started)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331037f-be3f-4782-856f-d55dab952488",
   "metadata": {},
   "source": [
    "LCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.\n",
    "\n",
    "We laid out the main value props in the [overview](/docs/expression_language) and went through two simple use cases in the [get started](/docs/expression_language/get_started) section. To really understand why LCEL and LangChain in general are useful, it's also helpful to think about how we might recreate similar functionality without them.\n",
    "\n",
    "Let's try this with our [basic example](/docs/expression_language/get_started#basic_example) from the get started page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeac2b8-c441-4d8d-b313-1de0ab9c7e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b7c6232-8642-4ef6-8886-53b184e77eee",
   "metadata": {},
   "source": [
    "import Tabs from '@theme/Tabs';\n",
    "import TabItem from '@theme/TabItem';\n",
    "import CodeBlock from \"@theme/CodeBlock\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3621b62-a037-42b8-8faa-59575608bb8b",
   "metadata": {},
   "source": [
    "## Invoke\n",
    "In the simplest case, we just want to be able to pass in a topic string and get back a joke string:\n",
    "\n",
    "\n",
    "<Tabs>\n",
    "  <TabItem value=\"without_lcel\" label=\"Without LCEL\" default>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e628905c-430e-4e4a-9d7c-c91d2f42052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "def manual_chain(topic: str) -> str:\n",
    "    prompt_value = f\"Tell me a short joke about {topic}\"\n",
    "    message = {\"role\": \"user\", \"content\": prompt_value}\n",
    "    client = openai.OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=[message],\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3b527-c09e-4c77-9711-c3cc4506cd95",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "\n",
    "<TabItem value=\"lcel\" label=\"LCEL\" default>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a7cf8-1bc7-405c-bb0d-f2ab2ba3b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0b0513-77b8-4371-a20e-3e487cec7e7f",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "</Tabs>\n",
    "\n",
    "## Stream\n",
    "If we want to stream results instead, we'll need to change our function:\n",
    "\n",
    "<Tabs>\n",
    "  <TabItem value=\"without_lcel\" label=\"Without LCEL\" default>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2cc6dc-d70a-4c13-9258-452f14290da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def manual_chain_stream(topic: str) -> Iterator[str]:\n",
    "    prompt_value = f\"Tell me a short joke about {topic}\"\n",
    "    message = {\"role\": \"user\", \"content\": prompt_value}\n",
    "    client = openai.OpenAI()\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[message],\n",
    "        stream=True,\n",
    "    )\n",
    "    for response in stream:\n",
    "        content = response.choices[0].delta.content\n",
    "        if content is not None:\n",
    "            yield content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e36b0e-c7dc-4130-a51b-189d4b756c7f",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "\n",
    "<TabItem value=\"lcel\" label=\"LCEL\" default>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e1a9c-2a18-4669-b0de-136f39197786",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chain.stream(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b41e78-ddeb-44d0-a58b-a0ea0c99a761",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "</Tabs>\n",
    "\n",
    "\n",
    "## Batch\n",
    "\n",
    "If we want to run on a batch of inputs in parallel, we'll again need a new function:\n",
    "\n",
    "<Tabs>\n",
    "  <TabItem value=\"without_lcel\" label=\"Without LCEL\" default>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b492f13-73a6-48ed-8d4f-9ad634da9988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def manual_chain_batch(topics: list) -> list:\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        return list(executor.map(manual_chain, topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e9d34-6775-43c1-93d8-684b58e341ab",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "\n",
    "<TabItem value=\"lcel\" label=\"LCEL\" default>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55b292-4e97-4d09-8e71-c71b4d853526",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ba36f-eec1-4fc1-8cfe-fa242a7f7809",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "</Tabs>\n",
    "\n",
    "## Async\n",
    "\n",
    "If you needed an asynchronous version:\n",
    "\n",
    "<Tabs>\n",
    "  <TabItem value=\"without_lcel\" label=\"Without LCEL\" default>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eabe6621-e815-41e3-9c9d-5aa561a69835",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def manual_chain_async(topic: str) -> str:\n",
    "    prompt_value = f\"Tell me a short joke about {topic}\"\n",
    "    message = {\"role\": \"user\", \"content\": prompt_value}\n",
    "    client = openai.AsyncOpenAI()\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=[message]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f209290-498c-4c17-839e-ee9002919846",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "\n",
    "<TabItem value=\"lcel\" label=\"LCEL\">\n",
    "\n",
    "```python\n",
    "chain.ainvoke(\"ice cream)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6888245-1ebe-4768-a53b-e1fef6a8b379",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "</Tabs>\n",
    "\n",
    "## LLM instead of chat model\n",
    "\n",
    "If we want to use a completion endpoint instead of a chat endpoint: \n",
    "\n",
    "<Tabs>\n",
    "  <TabItem value=\"without_lcel\" label=\"Without LCEL\" default>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aca946b-acaa-4f7e-a3d0-ad8e3225e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_chain_completion(topic: str) -> str:\n",
    "    prompt_value = f\"Tell me a short joke about {topic}\"\n",
    "    client = openai.OpenAI()\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt_value,\n",
    "    )\n",
    "    return response.choices[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45342cd6-58c2-4543-9392-773e05ef06e7",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "\n",
    "<TabItem value=\"lcel\" label=\"LCEL\" default>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56efc0c-88e0-4cf8-a46a-e8e9b9cd6805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "llm_chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a91eee-d017-420d-b215-f663dcbf8ed2",
   "metadata": {},
   "source": [
    "\n",
    "If we wanted, we could even make the choice of chat model or llm runtime configurable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76809d14-e77a-4125-a2ea-efbebf0b47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "\n",
    "configurable_model = model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"model\"), \n",
    "    default_key=\"chat_openai\", \n",
    "    openai=llm\n",
    ")\n",
    "configurable_chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | configurable_model \n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d94d0-cd42-4195-80b8-ef2e12503d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses chat model by default\n",
    "configurable_chain.invoke(\"ice cream\")\n",
    "# Uses LLM\n",
    "configurable_chain.invoke(\n",
    "    \"ice cream\", \n",
    "    config={\"configurable\": {\"model\": \"openai\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca115eaf-59ef-45c1-aac1-e8b0ce7db250",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "</Tabs>\n",
    "\n",
    "## Different model provider\n",
    "\n",
    "If we want to use Anthropic instead of OpenAI: \n",
    "\n",
    "<Tabs>\n",
    "  <TabItem value=\"without_lcel\" label=\"Without LCEL\" default>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde2ceb0-f65e-487b-9a32-137b0e9d79d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "\n",
    "def manual_chain_anthropic(topic: str) -> str:\n",
    "    prompt_value = (\n",
    "        f\"Human:\\n\\nTell me a short joke about {topic}\\n\\nAssistant:\"\n",
    "    )\n",
    "    client = anthropic.Anthropic()\n",
    "    response = client.completions.create(\n",
    "        model=\"claude-2\",\n",
    "        prompt=prompt_value,\n",
    "        max_tokens_to_sample=256,\n",
    "    )\n",
    "    return response.completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0c9f8-e316-42e1-af85-cabeba4b7059",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "\n",
    "<TabItem value=\"lcel\" label=\"LCEL\" default>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b800d1-5954-41a4-80b0-f00a7908961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "\n",
    "\n",
    "anthropic_chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | ChatAnthropic(model=\"claude-2\")\n",
    "    | output_parser\n",
    ")\n",
    "anthropic_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370dd4d7-b825-40c4-ae3c-2693cba2f22a",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "</Tabs>\n",
    "\n",
    "## Logging\n",
    "\n",
    "If we want to log our intermediate results (we'll `print` here for illustrative purposes):\n",
    "\n",
    "<Tabs>\n",
    "  <TabItem value=\"without_lcel\" label=\"Without LCEL\" default>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a3c51-926d-48c6-b9ae-42bf8f14ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_chain_anthropic_logging(topic: str) -> str:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = (\n",
    "        f\"Human:\\n\\nTell me a short joke about {topic}\\n\\nAssistant:\"\n",
    "    )\n",
    "    print(f\"Formatted prompt: {prompt_value}\")\n",
    "    client = anthropic.Anthropic()\n",
    "    response = client.completions.create(\n",
    "        model=\"claude-2\",\n",
    "        prompt=prompt_value,\n",
    "        max_tokens_to_sample=256,\n",
    "    )\n",
    "    print(f\"Output: {response.completion}\")\n",
    "    return response.completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd20fd-43cd-4aaf-866f-a53d1f20312d",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "\n",
    "<TabItem value=\"lcel\" label=\"LCEL\" default>\n",
    "\n",
    "By turning on LangSmith, every step of every chain is automatically logged. We set these environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6204f21-d2e7-4ac6-871f-b60b34e5bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ce3c5-27a7-4954-9f0e-b94313597135",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "</Tabs>\n",
    "\n",
    "## Fallbacks\n",
    "\n",
    "If you wanted to add retry or fallback logic:\n",
    "\n",
    "<Tabs>\n",
    "  <TabItem value=\"without_lcel\" label=\"Without LCEL\" default>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49d512-bc83-4c5f-b56e-934b8343b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_chain_with_fallback(topic: str) -> str:\n",
    "    try:\n",
    "        return manual_chain(topic)\n",
    "    except Exception:\n",
    "        return manual_chain_anthropic(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ef59b5-2ce3-479e-a7ac-79e1e2f30e9c",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "\n",
    "<TabItem value=\"lcel\" label=\"LCEL\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d8a0f-66eb-4c35-9529-74bec44ce4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fallback_chain = chain.with_fallbacks([anthropic_chain])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58af836-26bd-4eab-97a0-76dd56d53430",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "</Tabs>\n",
    "\n",
    "## Full code comparison\n",
    "\n",
    "<Tabs>\n",
    "  <TabItem value=\"without_lcel\" label=\"Without LCEL\" default>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25837c5-829b-42a3-92b4-7e25831350c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Iterator, List, Tuple\n",
    "\n",
    "import openai\n",
    "\n",
    "prompt_template = \"Tell me a short joke about {topic}\"\n",
    "\n",
    "\n",
    "def manual_chain(topic: str, *, model: str = \"chat_openai\") -> str:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "\n",
    "    if model == \"chat_openai\":\n",
    "        print(f\"Full prompt: {prompt_value}\")\n",
    "        message = {\"role\": \"user\", \"content\": prompt_value}\n",
    "        response = openai.OpenAI().chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\", \n",
    "            messages=[message],\n",
    "        )\n",
    "        output = response.choices[0].message.content\n",
    "    elif model == \"openai\":\n",
    "        print(f\"Full prompt: {prompt_value}\")\n",
    "        response = openai.OpenAI().completions.create(\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            prompt=prompt_value,\n",
    "        )\n",
    "        output = response.choices[0].text\n",
    "    elif model == \"anthropic\":\n",
    "        prompt_value = f\"Human:\\n\\n{prompt_value}\\n\\nAssistant:\"\n",
    "        print(f\"Full prompt: {prompt_value}\")\n",
    "        response = anthropic.Anthropic().completions.create(\n",
    "            model=\"claude-2\",\n",
    "            prompt=prompt_value,\n",
    "            max_tokens_to_sample=256,\n",
    "        )\n",
    "        output = response.completion\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid model {model}.\" \n",
    "            \"Should be one of chat_openai, openai, anthropic.\"\n",
    "        )\n",
    "    print(f\"Output: {output}\")\n",
    "    return output\n",
    "\n",
    "\n",
    "def manual_chain_with_fallbacks(\n",
    "    topic: str, \n",
    "    *, \n",
    "    model: str = \"chat_openai\", \n",
    "    fallbacks: Tuple[str] = (\"anthropic\",)\n",
    ") -> str:\n",
    "    for fallback in fallbacks:\n",
    "        try:\n",
    "            return manual_chain(topic, model=model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error {e}\")\n",
    "            model = fallback\n",
    "    raise e\n",
    "\n",
    "\n",
    "def manual_chain_batch(\n",
    "    topics: List[str],\n",
    "    *,\n",
    "    model: str = \"chat_openai\",\n",
    "    fallbacks: Tuple[str] = (\"anthropic\",),\n",
    ") -> List[str]:\n",
    "    models = [model] * len(topics)\n",
    "    fallbacks_list = [fallbacks] * len(topics)\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        return list(\n",
    "            executor.map(\n",
    "                manual_chain_with_fallbacks, \n",
    "                topics, \n",
    "                models, \n",
    "                fallbacks_list\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def manual_chain_stream(\n",
    "    topic: str, \n",
    "    *, \n",
    "    model: str = \"chat_openai\"\n",
    ") -> Iterator[str]:\n",
    "    print(f\"Input: {topic}\")\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "\n",
    "    if model == \"chat_openai\":\n",
    "        print(f\"Full prompt: {prompt_value}\")\n",
    "        stream = openai.OpenAI().chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_value}],\n",
    "            stream=True,\n",
    "        )\n",
    "        for response in stream:\n",
    "            content = response.choices[0].delta.content\n",
    "            if content is not None:\n",
    "                yield content\n",
    "    elif model == \"openai\":\n",
    "        print(f\"Full prompt: {prompt_value}\")\n",
    "        stream = openai.OpenAI().completions.create(\n",
    "            model=\"gpt-3.5-turbo-instruct\", \n",
    "            prompt=prompt_value, \n",
    "            stream=True\n",
    "        )\n",
    "        for response in stream:\n",
    "            yield response.choices[0].text\n",
    "    elif model == \"anthropic\":\n",
    "        prompt_value = f\"Human:\\n\\n{prompt_value}\\n\\nAssistant:\"\n",
    "        print(f\"Full prompt: {prompt_value}\")\n",
    "        stream = anthropic.Anthropic().completions.create(\n",
    "            model=\"claude-2\", \n",
    "            prompt=prompt_value, \n",
    "            max_tokens_to_sample=256, \n",
    "            stream=True,\n",
    "        )\n",
    "        for response in stream:\n",
    "            yield response.completion\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid model {model}.\"\n",
    "            \"Should be one of chat_openai, openai, anthropic.\"\n",
    "        )\n",
    "\n",
    "\n",
    "async def manual_chain_async(\n",
    "    topic: str, \n",
    "    *, \n",
    "    model: str = \"chat_openai\"\n",
    ") -> str:\n",
    "    # You get the idea :)\n",
    "    ...\n",
    "\n",
    "\n",
    "async def manual_chain_async_batch(\n",
    "    topics: List[str], *, model: str = \"chat_openai\"\n",
    ") -> List[str]:\n",
    "    ...\n",
    "\n",
    "\n",
    "async def manual_chain_async_stream(\n",
    "    topic: str, *, model: str = \"chat_openai\"\n",
    ") -> Iterator[str]:\n",
    "    ...\n",
    "\n",
    "\n",
    "def manual_chain_stream_with_fallbacks(\n",
    "    topic: str, \n",
    "    *, \n",
    "    model: str = \"chat_openai\", \n",
    "    fallbacks: Tuple[str] = (\"anthropic\",),\n",
    ") -> Iterator[str]:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3d71d-8c69-4dc4-81b7-95cd46b271c2",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "\n",
    "<TabItem value=\"lcel\" label=\"LCEL\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c469a-545e-434e-bd6e-99745dd880a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chat_models import ChatAnthropic, ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "openai = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "anthropic = ChatAnthropic(model=\"claude-2\")\n",
    "model = chat_openai.with_fallbacks([anthropic]).configurable_alternatives(\n",
    "    ConfigurableField(id=\"model\"),\n",
    "    default_key=\"chat_openai\",\n",
    "    openai=openai,\n",
    "    anthropic=anthropic,\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da90cf29-107d-4288-b355-e0666378a701",
   "metadata": {},
   "source": [
    "</TabItem>\n",
    "\n",
    "</Tabs>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-venv",
   "language": "python",
   "name": "poetry-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
