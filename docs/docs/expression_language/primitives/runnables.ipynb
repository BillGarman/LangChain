{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 0\n",
    "title: \"Runnables\"\n",
    "keywords: [Runnable, Runnables, LCEL]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnables\n",
    "\n",
    "`Runnables` are, abstractly, objects that implement the `.invoke()` method to perform some logic, then return the result. This can be calling an LLM, retrieving documents, formatting input or output, or even something custom. They can also override [other methods](/docs/expression_language/interface), such as `.stream()`, to support custom behavior for batching and streaming, but these will all delegate to this single `.invoke()` method by default.\n",
    "\n",
    "It can be helpful to think of a `Runnable` as a protocol or interface rather than a concrete object - there are no concrete `Runnable` instances, only implementations. Most components in LangChain are runnables.\n",
    "\n",
    "## Chaining\n",
    "\n",
    "One important consequence of this shared single call is that it allows any two runnables to be \"chained\" together into sequences using the pipe operator (`|`), where the output of the previous runnable's `.invoke()` call is passed as input to the next runnable. The resulting `RunnableSequence` is itself a runnable, which means it can be invoked, streamed, or piped just like any other runnable.\n",
    "\n",
    "For example, a common pattern in LangChain is using a prompt to format input into an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain langchain-anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "model = ChatAnthropic(model_name=\"claude-3-haiku-20240307\")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts and models are both runnable, so we can chain them together. We can then invoke the resulting sequence like any other runnable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's a bear-y good joke for you:\\n\\nWhy don't bears wear socks?\\nBecause they have bear feet!\", response_metadata={'id': 'msg_01JH6Tzbog746fVgiq3q8an6', 'content': [ContentBlock(text=\"Here's a bear-y good joke for you:\\n\\nWhy don't bears wear socks?\\nBecause they have bear feet!\", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=13, output_tokens=30)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even combine this chain with more runnables to create another chain. This may involve some input/output formatting using other types of runnables, depending on the required inputs and outputs of the chain components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
    "\n",
    "composed_chain = (\n",
    "    chain | {\"joke\": StrOutputParser()} | analysis_prompt | model | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Haha, that bear pun joke is pretty good! I enjoyed the clever play on words with \"bear feet\" instead of \"bare feet.\" Puns can definitely be corny, but I think this one lands well and has a nice silly charm to it. Bear-themed jokes can be a lot of fun, so I\\'d be happy to hear another one if you\\'ve got another punny bear joke up your sleeve!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "While all runnables support the `.stream()` method, which returns a generator that yields output as it becomes available, by default by just returning the result of `.invoke()`, some runnables support multiple chunks. One case where this is useful is when calling LLMs and chat models as calls may be long-running.\n",
    "\n",
    "When streamed, runnables in a chain will automatically aggregate output from previous steps when appropriate. In the above example, the prompt template requires the fully aggregate input from the previous chain to generate a complete input, so the sequence aggregates output from the chain before passing it to the prompt template. This interrupts streaming.\n",
    "\n",
    "On the other hand, many output parsers and formatting components like [`RunnableParallel`](/docs/expression_language/primitives/map) and [`RunnablePassthrough`](/docs/expression_language/primitives/passthrough) are designed to handle individual chunks and transform them into some other output. This means that in the above composed chain, the final `StrOutputParser()` that converts chat model output into a string will not interrupt streaming. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That\n",
      "'s\n",
      " a\n",
      " pretty\n",
      " good\n",
      " bear\n",
      " joke\n",
      "!\n",
      " I\n",
      " ch\n",
      "uck\n",
      "led\n",
      " a\n",
      " bit\n",
      " at\n"
     ]
    }
   ],
   "source": [
    "stream = composed_chain.stream({\"topic\": \"bears\"})\n",
    "\n",
    "i = 0\n",
    "for chunk in stream:\n",
    "    if i < 15:\n",
    "        print(chunk)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we get many chunks, even though the streaming chat model wasn't the final step.\n",
    "\n",
    "The rest of this section contains information about low-level utility primitives and concepts around runnables that are useful for formatting inputs and outputs at various steps. It may be more helpful to use it as a reference as needed rather than studying them all closely immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
