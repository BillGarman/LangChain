{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0bdb3b97-4989-4237-b43b-5943dbbd8302",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 2\n",
    "title: Streaming\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87745bda-cf7c-483c-9bbe-2b6fe3b77362",
   "metadata": {},
   "source": [
    "# Streaming With LangChain\n",
    "\n",
    "Streaming is critical in making applications based on LLMs feel responsive to end-users.\n",
    "\n",
    "Important LangChain primitives like LLMs, parsers, prompts, retrievers, and agents implement the LangChain [Runnable Interface](/docs/expression_language/interface).\n",
    "\n",
    "This interface provides two general approaches to stream content for your application:\n",
    "\n",
    "1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n",
    "2. async `astream_events` and async`astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n",
    "\n",
    "Here, we'll take a look at both approaches, and try to understand a bit of what's happening under the hood. ðŸ¥·"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f7f8b-64b9-4c03-a0a7-9f7247138d82",
   "metadata": {},
   "source": [
    "## Using Stream\n",
    "\n",
    "All `Runnable` objects implement a sync method called `stream` and an async variant called `astream`. \n",
    "\n",
    "These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n",
    "\n",
    "Streaming is only possible if all steps in the program are able to operate directly on an **input stream**, process an input chunk one at a time, and yield a corresponding output chunk.\n",
    "\n",
    "Such logic can range from being trivial (e.g., output the tokens generated by the LLM) to fairly difficult (e.g., streaming partial JSON results before the full JSON is available).\n",
    "\n",
    "The best place to start is with the simplest components, so we can see how streaming works with them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1cb25c-00ab-4ec0-a827-dfd6e12cce28",
   "metadata": {},
   "source": [
    "### LLMs and Chat Models\n",
    "\n",
    "Large language models and their chat variants are the primary bottleneck in LLM based apps. ðŸ™Š\n",
    "\n",
    "Large language models may take up to a **few seconds** to generate a complete response to a query.\n",
    "\n",
    "This is far larger than the **~200-300 ms** threshold at which an application still feels responsive to an end user.\n",
    "\n",
    "The primary solution to this problem is to stream the output from the model **token by token**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "91787fc7-d941-48c0-a8b4-0ee61ab7dd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello|!| My| name| is| Claude|.| I|'m| an| artificial| intelligence| assistant| created| by| An|throp|ic| to| be| helpful|,| harmless|,| and| honest|.||"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "model = ChatAnthropic()\n",
    "\n",
    "chunks = []\n",
    "async for chunk in model.astream('hello. tell me something about yourself'):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66730a87-77d5-40d6-a68f-315121989bd1",
   "metadata": {},
   "source": [
    "Let's inspect one of the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "dade3000-1ac4-4f5c-b5c6-a0217f9f8a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=' Hello')"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a47193-2bd1-46bc-9c7e-ea0f6b08c4a5",
   "metadata": {},
   "source": [
    "We got back something called an `AIMessageChunk`. This chunk represents a part of an `AIMessage`.\n",
    "\n",
    "Message chunks are additive by design -- one can simply add them up to get the state of the response so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d3cf5f38-249c-4da0-94e6-5e5203fad52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=' Hello! My name is')"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffbd9a-3b79-44b6-8883-1371f9460c77",
   "metadata": {},
   "source": [
    "### Chains\n",
    "\n",
    "Virtually all LLM applications involve more steps than just a call to a language model.\n",
    "\n",
    "Let's build a simple chain using `LangChain Expression Language` (`LCEL`) that combines a prompt, model and a parser and verify that streaming works.\n",
    "\n",
    "We will use `StrOutputParser` to parse the output from the model. This is a simple parser that extracts the `content` field from an `AIMessageChunk`, giving us the `token` returned by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "a8562ae2-3fd1-4829-9801-a5a732b1798d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Sure|,| here|'s| a| par|rot| joke| for| you|:\n",
      "\n",
      "|Why| did| the| par|rot| wear| a| rain|coat|?\n",
      "\n",
      "|Because| it| wanted| to| be| a| \"|p|olly|\"| uns|aturated|!||"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "async for chunk in chain.astream({'topic': 'parrot'}):\n",
    "    print(chunk, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b399fb4-5e3c-4581-9570-6df9b42b623d",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "`LangChain Expression Language` provides a **declarative** way to specify a \"program\" by chainining together LangChain primitives. \n",
    "\n",
    "Chains created using LCEL benefit from an automatic implementation of `stream`, and `astream` allowing streaming of final output. (In fact, such chains implement\n",
    "the entire standard Runnable interface.)\n",
    "\n",
    "You do not have to use the `LangChain Expression Language` to use LangChain and can instead rely on a standard **imperative** programming approach by\n",
    "caling `invoke`, `batch` or `stream` on each component individually, assigning the results to variables and then using them downstream as you see fit.\n",
    "\n",
    "You won't get the benefits of using LCEL -- but this might just fine for your needs ðŸ‘Œ!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff2701-8887-486f-8b3b-eb26383d4bb6",
   "metadata": {},
   "source": [
    "### Working with Input Streams\n",
    "\n",
    "What if you wanted to stream JSON from the output as it was being generated?\n",
    "\n",
    "If you were to rely on `json.loads` to parse the partial json, the parsing would fail as the partial json wouldn't be valid json.\n",
    "\n",
    "Instead, the parser needs to operate on the **input stream**, and attempt to extract valid json from the partial model output.\n",
    "\n",
    "**note** at the time of writing this parser only works well with OpenAI output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c865d-1d12-474e-9444-8385d45366fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a514fcdb-cb2c-4b94-85ab-c90196741d42",
   "metadata": {},
   "source": [
    "### Model + JsonParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "cf102fe9-5525-4514-b038-35d29af6228b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'countries': []}\n",
      "{'countries': [{}]}\n",
      "{'countries': [{'name': ''}]}\n",
      "{'countries': [{'name': 'France'}]}\n",
      "{'countries': [{'name': 'France', 'population': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': '67'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,784'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,784'}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,784'}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,784'}, {'name': 'Japan'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,784'}, {'name': 'Japan', 'population': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,784'}, {'name': 'Japan', 'population': '126'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,784'}, {'name': 'Japan', 'population': '126,'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,784'}, {'name': 'Japan', 'population': '126,010'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,784'}, {'name': 'Japan', 'population': '126,010,'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,784'}, {'name': 'Japan', 'population': '126,010,000'}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = model | JsonOutputParser()\n",
    "\n",
    "async for text in chain.astream('output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\"'):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8a9f0-4627-4ddb-a6e8-7b7f386bffb9",
   "metadata": {},
   "source": [
    "### Runnable Lambdas as Generators\n",
    "\n",
    "An important thing to remember is that all parts of the chain must be able to operate on **input streams** rather than on `inputs` or the final result will not stream properly!\n",
    "\n",
    "Let's use the previous example and add a bit of logic on top that breaks streaming using a function that extracts the countries names from the JSON.\n",
    "\n",
    ":::{.callout-tip}\n",
    "Even a chain contains steps that do not operate on input streams, `astream_events` (described below) will yield streaming results from intermediate steps, which may be just fine for your application!\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d9c90117-9faa-4a01-b484-0db071808d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'Spain', 'Japan']|"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "def _extract_country_names(inputs):\n",
    "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
    "    if not isinstance(inputs, dict):\n",
    "        return ''\n",
    "\n",
    "    if 'countries' not in inputs:\n",
    "        return ''\n",
    "\n",
    "    countries = inputs['countries']\n",
    "\n",
    "    if not isinstance(countries, list):\n",
    "        return ''\n",
    "\n",
    "    country_names = [\n",
    "        country.get('name')\n",
    "        for country in countries\n",
    "        if isinstance(country, dict)\n",
    "    ]\n",
    "    return country_names\n",
    "  \n",
    "\n",
    "chain = model | JsonOutputParser() | _extract_country_names\n",
    "\n",
    "async for text in chain.astream('output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\"'):\n",
    "    print(text, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638df0b-9386-4dd8-badd-7b8b052724e8",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "Using a generator function (a function that uses `yield`) allows writing code that operators on **input streams**\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "15984b2b-315a-4119-945b-2a3dabea3082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France|Spain|Japan|"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "async def _extract_country_names(input_stream):\n",
    "    \"\"\"A function that operates on input streams.\"\"\"\n",
    "    country_names_so_far = set()\n",
    "\n",
    "    async for input in input_stream:\n",
    "        if not isinstance(input, dict):\n",
    "            continue\n",
    "\n",
    "        if 'countries' not in input:\n",
    "            continue\n",
    "\n",
    "        countries = input['countries']\n",
    "\n",
    "        if not isinstance(countries, list):\n",
    "            continue\n",
    "\n",
    "        for country in countries:\n",
    "            name = country.get('name')\n",
    "            if not name:\n",
    "                continue\n",
    "            if name not in country_names_so_far:\n",
    "                yield name\n",
    "                country_names_so_far.add(name)\n",
    "\n",
    "chain = model | JsonOutputParser() | _extract_country_names\n",
    "\n",
    "async for text in chain.astream('output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\"'):\n",
    "    print(text, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf65b7-aa47-4321-98c7-a0abe43b833a",
   "metadata": {},
   "source": [
    "### Components without .stream?\n",
    "\n",
    "Some components like Retrievers \n",
    "\n",
    "Retrievers are not able to stream their results currently and they do not operate on partial input ðŸ¤¨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b9b1c00d-8b44-40d0-9e2b-8a70d238f82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(page_content='harrison worked at kensho'),\n",
       "  Document(page_content='harrison likes spicy food')]]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\", \n",
    "    \"harrison likes spicy food\"], \n",
    "     embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "chunks = [chunk for chunk in retriever.stream('where did harrison work?')]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3e71b-439e-418f-8a8a-5232fba3d9fd",
   "metadata": {},
   "source": [
    "This is OK ðŸ¥¹! Streaming doesn't have to be implemented for all components as in some cases it may be impossible or effectivley operate on the input stream.\n",
    "\n",
    "An LCEL chain constructed using \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "957447e6-1e60-41ef-8c10-2654bd9e738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = (\n",
    "    {\n",
    "        \"context\": retriever.with_config(run_name=\"Docs\"),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "94e50b5d-bf51-4eee-9da0-ee40dd9ce42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|H|arrison| worked| at| Kens|ho|,| a| renowned| technology| company| known| for| its| innovative| solutions|.| The| company|'s| headquarters| were| located| in| a| bustling| city|,| offering| a| vibrant| work| environment|.| Kens|ho| was| known| for| its| collaborative| culture|,| where| employees| were| encouraged| to| think| outside| the| box| and| push| the| boundaries| of| technology|.| The| office| space| at| Kens|ho| was| modern| and| sleek|,| equipped| with| state|-of|-the|-art| facilities| to| foster| creativity| and| productivity|.||"
     ]
    }
   ],
   "source": [
    "for chunk in retrieval_chain.stream(\"Where did harrison work? Write 3 made up sentences about this place.\"):\n",
    "    print(chunk, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ab012-7de5-48c7-97f4-c2dc1036b579",
   "metadata": {},
   "source": [
    "## Using Stream Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ec135-3348-4041-8f55-bf3e59b3b2d0",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c00df46e-7f6b-4e06-8abf-801898c8d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "async for event in model.astream_events('hello', version='v1'):\n",
    "    events.append(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ce31b525-f47d-4828-85a7-912ce9f2e79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chat_model_start',\n",
       "  'run_id': '652436d1-1ad1-4560-817b-a92981f6a995',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'data': {'input': 'hello'}},\n",
       " {'event': 'on_chat_model_stream',\n",
       "  'run_id': '652436d1-1ad1-4560-817b-a92981f6a995',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'name': 'ChatOpenAI',\n",
       "  'data': {'chunk': AIMessageChunk(content='')}},\n",
       " {'event': 'on_chat_model_stream',\n",
       "  'run_id': '652436d1-1ad1-4560-817b-a92981f6a995',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'name': 'ChatOpenAI',\n",
       "  'data': {'chunk': AIMessageChunk(content='Hello')}}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "76cfe826-ee63-4310-ad48-55a95eb3b9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chat_model_stream',\n",
       "  'run_id': '652436d1-1ad1-4560-817b-a92981f6a995',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'name': 'ChatOpenAI',\n",
       "  'data': {'chunk': AIMessageChunk(content='')}},\n",
       " {'event': 'on_chat_model_end',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'run_id': '652436d1-1ad1-4560-817b-a92981f6a995',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'data': {'output': AIMessageChunk(content='Hello! How can I assist you today?')}}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5996d852-e8f3-4458-8387-4a4bac231189",
   "metadata": {},
   "source": [
    "### Models with prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4960a6fa-b06e-4d8e-b35a-1fe67b269af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Sure|,| here|'s| a| par|rot| joke| for| you|:\n",
      "\n",
      "|Why| don|'t| scientists| trust| par|rots|?\n",
      "\n",
      "|Because| they| always| talk| in| f|owl| language|!||"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "chain = prompt | model\n",
    "\n",
    "async for chunk in chain.astream({'topic': 'parrot'}):\n",
    "    print(chunk.content, end='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "da22c2bd-4694-4d30-b4ee-8efc2f20431b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtell me a joke about \u001b[39m\u001b[38;5;132;01m{topic}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m model\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mastream_events({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparrot\u001b[39m\u001b[38;5;124m'\u001b[39m}, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk)\n",
      "File \u001b[0;32m~/src/langchain/libs/core/langchain_core/runnables/base.py:889\u001b[0m, in \u001b[0;36mRunnable.astream_events\u001b[0;34m(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m root_name \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name())\n\u001b[1;32m    887\u001b[0m \u001b[38;5;66;03m# Ignoring mypy complaint about too many different union combinations\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# This arises because many of the argument types are unions\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m _astream_log_implementation(  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    892\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    893\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    894\u001b[0m     diff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    895\u001b[0m     with_streamed_output_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    897\u001b[0m ):\n\u001b[1;32m    898\u001b[0m     run_log \u001b[38;5;241m=\u001b[39m run_log \u001b[38;5;241m+\u001b[39m log\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m encountered_start_event:\n\u001b[1;32m    901\u001b[0m         \u001b[38;5;66;03m# Yield the start event for the root runnable.\u001b[39;00m\n",
      "File \u001b[0;32m~/src/langchain/libs/core/langchain_core/tracers/log_stream.py:610\u001b[0m, in \u001b[0;36m_astream_log_implementation\u001b[0;34m(runnable, input, config, stream, diff, with_streamed_output_list, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;66;03m# Wait for the runnable to finish, if not cancelled (eg. by break)\u001b[39;00m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m task\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/src/langchain/libs/core/langchain_core/tracers/log_stream.py:565\u001b[0m, in \u001b[0;36m_astream_log_implementation.<locals>.consume_astream\u001b[0;34m()\u001b[0m\n\u001b[1;32m    562\u001b[0m prev_final_output: Optional[Output] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    563\u001b[0m final_output: Optional[Output] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m runnable\u001b[38;5;241m.\u001b[39mastream(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    566\u001b[0m     prev_final_output \u001b[38;5;241m=\u001b[39m final_output\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/src/langchain/libs/core/langchain_core/language_models/chat_models.py:269\u001b[0m, in \u001b[0;36mBaseChatModel.astream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 269\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_messages()\n\u001b[1;32m    270\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_invocation_params(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    271\u001b[0m     options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n",
      "File \u001b[0;32m~/src/langchain/libs/core/langchain_core/language_models/chat_models.py:149\u001b[0m, in \u001b[0;36mBaseChatModel._convert_input\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "chain = prompt | model\n",
    "\n",
    "async for chunk in chain.astream_events({'topic': 'parrot'}, version='v1'):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46fe7fc-1be2-4976-b80f-0cf0a27a17f2",
   "metadata": {},
   "source": [
    "# Prompt + Model + Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5af9e-f781-4b8b-a5af-ca28a506000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser#ma\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da271b-3ae8-4479-8c83-0e32b278c5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "007dac83-2edd-4e7e-9507-5c6912a81f28",
   "metadata": {},
   "source": [
    "# Add retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a02244-3a2c-4bc2-b5fd-5db7e6e0e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\n",
    "        \"context\": retriever.with_config(run_name=\"Docs\"),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async for chunk in retrieval_chain.astream_log(\n",
    "    \"where did harrison work?\", include_names=[\"Docs\"]\n",
    "):\n",
    "    print(\"-\" * 40)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40cf1f86-1331-46ec-8f8a-3cbef859f72d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (1698548667.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    chain.astream({'topic': \"\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "chain.astream({'topic': '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa44fc-a28b-423d-9066-51e678365c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
