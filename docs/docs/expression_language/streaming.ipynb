{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0bdb3b97-4989-4237-b43b-5943dbbd8302",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 2\n",
    "title: Streaming\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87745bda-cf7c-483c-9bbe-2b6fe3b77362",
   "metadata": {},
   "source": [
    "# Streaming With LangChain\n",
    "\n",
    "Streaming is critical in making applications based on LLMs feel responsive to end-users.\n",
    "\n",
    "Important LangChain primitives like LLMs, parsers, prompts, retrievers, and agents implement the LangChain [Runnable Interface](/docs/expression_language/interface).\n",
    "\n",
    "This interface provides two general approaches to stream content for your application:\n",
    "\n",
    "1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n",
    "2. async `astream_events` and async`astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n",
    "\n",
    "Here, we'll take a look at both approaches, and try to understand a bit of what's happening under the hood. ðŸ¥·"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f7f8b-64b9-4c03-a0a7-9f7247138d82",
   "metadata": {},
   "source": [
    "## Using Stream\n",
    "\n",
    "All `Runnable` objects implement a sync method called `stream` and an async variant called `astream`. \n",
    "\n",
    "These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n",
    "\n",
    "Streaming is only possible if all steps in the program are able to operate directly on an **input stream**, process an input chunk one at a time, and yield a corresponding output chunk.\n",
    "\n",
    "Such logic can range from being trivial (e.g., output the tokens generated by the LLM) to fairly difficult (e.g., streaming partial JSON results before the full JSON is available).\n",
    "\n",
    "The best place to start is with the simplest components, so we can see how streaming works with them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1cb25c-00ab-4ec0-a827-dfd6e12cce28",
   "metadata": {},
   "source": [
    "### LLMs and Chat Models\n",
    "\n",
    "Large language models and their chat variants are the primary bottleneck in LLM based apps. ðŸ™Š\n",
    "\n",
    "Large language models may take up to a **few seconds** to generate a complete response to a query.\n",
    "\n",
    "This is far larger than the **~200-300 ms** threshold at which an application still feels responsive to an end user.\n",
    "\n",
    "The primary solution to this problem is to stream the output from the model **token by token**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "91787fc7-d941-48c0-a8b4-0ee61ab7dd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello|!| My| name| is| Claude|.| I|'m| an| artificial| intelligence| assistant| created| by| An|throp|ic| to| be| helpful|,| harmless|,| and| honest|.||"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic()\n",
    "\n",
    "chunks = []\n",
    "async for chunk in model.astream(\"hello. tell me something about yourself\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66730a87-77d5-40d6-a68f-315121989bd1",
   "metadata": {},
   "source": [
    "Let's inspect one of the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "dade3000-1ac4-4f5c-b5c6-a0217f9f8a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=' Hello')"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a47193-2bd1-46bc-9c7e-ea0f6b08c4a5",
   "metadata": {},
   "source": [
    "We got back something called an `AIMessageChunk`. This chunk represents a part of an `AIMessage`.\n",
    "\n",
    "Message chunks are additive by design -- one can simply add them up to get the state of the response so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d3cf5f38-249c-4da0-94e6-5e5203fad52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=' Hello! My name is')"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffbd9a-3b79-44b6-8883-1371f9460c77",
   "metadata": {},
   "source": [
    "### Chains\n",
    "\n",
    "Virtually all LLM applications involve more steps than just a call to a language model.\n",
    "\n",
    "Let's build a simple chain using `LangChain Expression Language` (`LCEL`) that combines a prompt, model and a parser and verify that streaming works.\n",
    "\n",
    "We will use `StrOutputParser` to parse the output from the model. This is a simple parser that extracts the `content` field from an `AIMessageChunk`, giving us the `token` returned by the model.\n",
    "\n",
    ":::{.callout-tip}\n",
    "LCEL provides a **declarative** way to specify a \"program\" by chainining together LangChain primitives. Chains created using LCEL benefit from an automatic implementation of `stream`, and `astream` allowing streaming of final output. (In fact, such chains implement\n",
    "the entire standard Runnable interface.)\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a8562ae2-3fd1-4829-9801-a5a732b1798d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here|'s| a| silly| joke| about| a| par|rot|:|\n",
      "\n",
      "What| do| you| call| a| par|rot| that| can| count|?| A| mat|he|-|matic|ian|!||"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "async for chunk in chain.astream({\"topic\": \"parrot\"}):\n",
    "    print(chunk, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b399fb4-5e3c-4581-9570-6df9b42b623d",
   "metadata": {},
   "source": [
    ":::{.callout-info}\n",
    "You do not have to use the `LangChain Expression Language` to use LangChain and can instead rely on a standard **imperative** programming approach by\n",
    "caling `invoke`, `batch` or `stream` on each component individually, assigning the results to variables and then using them downstream as you see fit.\n",
    "\n",
    "If that works for your needs, then that's fine by us ðŸ‘Œ!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff2701-8887-486f-8b3b-eb26383d4bb6",
   "metadata": {},
   "source": [
    "### Working with Input Streams\n",
    "\n",
    "What if you wanted to stream JSON from the output as it was being generated?\n",
    "\n",
    "If you were to rely on `json.loads` to parse the partial json, the parsing would fail as the partial json wouldn't be valid json.\n",
    "\n",
    "Instead, the parser needs to operate on the **input stream**, and attempt to extract valid json from the partial model output.\n",
    "\n",
    "Let's see such a parser in action to understand what this means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "5ff63cce-715a-4561-951f-9321c82e8d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'countries': []}\n",
      "{'countries': [{}]}\n",
      "{'countries': [{'name': ''}]}\n",
      "{'countries': [{'name': 'France'}]}\n",
      "{'countries': [{'name': 'France', 'population': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': '67'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {'name': 'Spain'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {'name': 'Spain', 'population': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {'name': 'Spain', 'population': '46'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {'name': 'Spain', 'population': '46.'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {'name': 'Spain', 'population': '46.94'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {'name': 'Spain', 'population': '46.94 million'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {'name': 'Spain', 'population': '46.94 million'}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {'name': 'Spain', 'population': '46.94 million'}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {'name': 'Spain', 'population': '46.94 million'}, {'name': 'Japan'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {'name': 'Spain', 'population': '46.94 million'}, {'name': 'Japan', 'population': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {'name': 'Spain', 'population': '46.94 million'}, {'name': 'Japan', 'population': '126'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {'name': 'Spain', 'population': '46.94 million'}, {'name': 'Japan', 'population': '126.'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {'name': 'Spain', 'population': '46.94 million'}, {'name': 'Japan', 'population': '126.26'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67.06 million'}, {'name': 'Spain', 'population': '46.94 million'}, {'name': 'Japan', 'population': '126.26 million'}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = model | JsonOutputParser()  # This parser only works with OpenAI right now\n",
    "async for text in chain.astream(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'\n",
    "):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8a9f0-4627-4ddb-a6e8-7b7f386bffb9",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "Any steps in the chain that operate on **full inputs** rather than on **input streams** can break streaming functionality via `stream` or `astream`.\n",
    ":::\n",
    "\n",
    ":::{.callout-tip}\n",
    "`astream_events` (described below) streaming results from intermediate steps. So even if the chain contains steps that only operate on **full inputs**, you may still\n",
    "be able to stream results well for your needs using `astream_events`.\n",
    ":::\n",
    "\n",
    "Let's use the previous example and add a bit of logic on top that breaks streaming using a function that extracts the countries names from the JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d9c90117-9faa-4a01-b484-0db071808d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'Spain', 'Japan']|"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "def _extract_country_names(inputs):\n",
    "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
    "    if not isinstance(inputs, dict):\n",
    "        return \"\"\n",
    "\n",
    "    if \"countries\" not in inputs:\n",
    "        return \"\"\n",
    "\n",
    "    countries = inputs[\"countries\"]\n",
    "\n",
    "    if not isinstance(countries, list):\n",
    "        return \"\"\n",
    "\n",
    "    country_names = [\n",
    "        country.get(\"name\") for country in countries if isinstance(country, dict)\n",
    "    ]\n",
    "    return country_names\n",
    "\n",
    "\n",
    "chain = model | JsonOutputParser() | _extract_country_names\n",
    "\n",
    "async for text in chain.astream(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\"'\n",
    "):\n",
    "    print(text, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638df0b-9386-4dd8-badd-7b8b052724e8",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "Using a generator function (a function that uses `yield`) allows writing code that operators on **input streams**\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "15984b2b-315a-4119-945b-2a3dabea3082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "async def _extract_country_names(input_stream):\n",
    "    \"\"\"A function that operates on input streams.\"\"\"\n",
    "    country_names_so_far = set()\n",
    "\n",
    "    async for input in input_stream:\n",
    "        if not isinstance(input, dict):\n",
    "            continue\n",
    "\n",
    "        if \"countries\" not in input:\n",
    "            continue\n",
    "\n",
    "        countries = input[\"countries\"]\n",
    "\n",
    "        if not isinstance(countries, list):\n",
    "            continue\n",
    "\n",
    "        for country in countries:\n",
    "            name = country.get(\"name\")\n",
    "            if not name:\n",
    "                continue\n",
    "            if name not in country_names_so_far:\n",
    "                yield name\n",
    "                country_names_so_far.add(name)\n",
    "\n",
    "\n",
    "chain = model | JsonOutputParser() | _extract_country_names\n",
    "\n",
    "async for text in chain.astream(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\"'\n",
    "):\n",
    "    print(text, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf65b7-aa47-4321-98c7-a0abe43b833a",
   "metadata": {},
   "source": [
    "### Non-streaming components\n",
    "\n",
    "Some built-in components like Retrievers do not offer any `streaming`. What happens if we try to `stream` them? ðŸ¤¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "b9b1c00d-8b44-40d0-9e2b-8a70d238f82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(page_content='harrison worked at kensho'),\n",
       "  Document(page_content='harrison likes spicy food')]]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\", \"harrison likes spicy food\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3e71b-439e-418f-8a8a-5232fba3d9fd",
   "metadata": {},
   "source": [
    "Stream just yielded the final result from that component. This is OK ðŸ¥¹! \n",
    "\n",
    "Not all components have to implement streaming as in some cases such implementation may be unnecessary, difficult or not make sense.\n",
    "\n",
    "An LCEL chain constructed using using non-streaming components, will still be able to stream in a lot of cases, with streaming of partial output starting after the last non-streaming step in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "957447e6-1e60-41ef-8c10-2654bd9e738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = (\n",
    "    {\n",
    "        \"context\": retriever.with_config(run_name=\"Docs\"),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "94e50b5d-bf51-4eee-9da0-ee40dd9ce42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Based| on| the| given| context|,| Harrison| worked| at| Kens|ho|.| Here| are| three| made|-up| sentences| about| Kens|ho|:\n",
      "\n",
      "|1|.| Kens|ho| is| a| renowned| tech| company| known| for| its| cutting|-edge| innovations| in| artificial| intelligence|.\n",
      "|2|.| Located| in| a| bustling| city|,| Kens|ho| offers| a| vibrant| work| environment| with| state|-of|-the|-art| facilities| and| a| collaborative| culture|.\n",
      "|3|.| Harrison| worked| at| Kens|ho|'s| research| and| development| department|,| where| he| contributed| to| groundbreaking| projects| that| revolution|ized| the| industry|.||"
     ]
    }
   ],
   "source": [
    "for chunk in retrieval_chain.stream(\n",
    "    \"Where did harrison work? Write 3 made up sentences about this place.\"\n",
    "):\n",
    "    print(chunk, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ab012-7de5-48c7-97f4-c2dc1036b579",
   "metadata": {},
   "source": [
    "## Using Stream Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ec135-3348-4041-8f55-bf3e59b3b2d0",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c00df46e-7f6b-4e06-8abf-801898c8d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "async for event in model.astream_events(\"hello\", version=\"v1\"):\n",
    "    events.append(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ce31b525-f47d-4828-85a7-912ce9f2e79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chat_model_start',\n",
       "  'run_id': '652436d1-1ad1-4560-817b-a92981f6a995',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'data': {'input': 'hello'}},\n",
       " {'event': 'on_chat_model_stream',\n",
       "  'run_id': '652436d1-1ad1-4560-817b-a92981f6a995',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'name': 'ChatOpenAI',\n",
       "  'data': {'chunk': AIMessageChunk(content='')}},\n",
       " {'event': 'on_chat_model_stream',\n",
       "  'run_id': '652436d1-1ad1-4560-817b-a92981f6a995',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'name': 'ChatOpenAI',\n",
       "  'data': {'chunk': AIMessageChunk(content='Hello')}}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "76cfe826-ee63-4310-ad48-55a95eb3b9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chat_model_stream',\n",
       "  'run_id': '652436d1-1ad1-4560-817b-a92981f6a995',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'name': 'ChatOpenAI',\n",
       "  'data': {'chunk': AIMessageChunk(content='')}},\n",
       " {'event': 'on_chat_model_end',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'run_id': '652436d1-1ad1-4560-817b-a92981f6a995',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'data': {'output': AIMessageChunk(content='Hello! How can I assist you today?')}}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5996d852-e8f3-4458-8387-4a4bac231189",
   "metadata": {},
   "source": [
    "### Models with prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4960a6fa-b06e-4d8e-b35a-1fe67b269af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Sure|,| here|'s| a| par|rot| joke| for| you|:\n",
      "\n",
      "|Why| don|'t| scientists| trust| par|rots|?\n",
      "\n",
      "|Because| they| always| talk| in| f|owl| language|!||"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "chain = prompt | model\n",
    "\n",
    "async for chunk in chain.astream({\"topic\": \"parrot\"}):\n",
    "    print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "da22c2bd-4694-4d30-b4ee-8efc2f20431b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtell me a joke about \u001b[39m\u001b[38;5;132;01m{topic}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m model\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mastream_events({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparrot\u001b[39m\u001b[38;5;124m'\u001b[39m}, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk)\n",
      "File \u001b[0;32m~/src/langchain/libs/core/langchain_core/runnables/base.py:889\u001b[0m, in \u001b[0;36mRunnable.astream_events\u001b[0;34m(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m root_name \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name())\n\u001b[1;32m    887\u001b[0m \u001b[38;5;66;03m# Ignoring mypy complaint about too many different union combinations\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# This arises because many of the argument types are unions\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m _astream_log_implementation(  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    892\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    893\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    894\u001b[0m     diff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    895\u001b[0m     with_streamed_output_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    897\u001b[0m ):\n\u001b[1;32m    898\u001b[0m     run_log \u001b[38;5;241m=\u001b[39m run_log \u001b[38;5;241m+\u001b[39m log\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m encountered_start_event:\n\u001b[1;32m    901\u001b[0m         \u001b[38;5;66;03m# Yield the start event for the root runnable.\u001b[39;00m\n",
      "File \u001b[0;32m~/src/langchain/libs/core/langchain_core/tracers/log_stream.py:610\u001b[0m, in \u001b[0;36m_astream_log_implementation\u001b[0;34m(runnable, input, config, stream, diff, with_streamed_output_list, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;66;03m# Wait for the runnable to finish, if not cancelled (eg. by break)\u001b[39;00m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m task\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/src/langchain/libs/core/langchain_core/tracers/log_stream.py:565\u001b[0m, in \u001b[0;36m_astream_log_implementation.<locals>.consume_astream\u001b[0;34m()\u001b[0m\n\u001b[1;32m    562\u001b[0m prev_final_output: Optional[Output] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    563\u001b[0m final_output: Optional[Output] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m runnable\u001b[38;5;241m.\u001b[39mastream(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    566\u001b[0m     prev_final_output \u001b[38;5;241m=\u001b[39m final_output\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/src/langchain/libs/core/langchain_core/language_models/chat_models.py:269\u001b[0m, in \u001b[0;36mBaseChatModel.astream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 269\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_messages()\n\u001b[1;32m    270\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_invocation_params(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    271\u001b[0m     options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n",
      "File \u001b[0;32m~/src/langchain/libs/core/langchain_core/language_models/chat_models.py:149\u001b[0m, in \u001b[0;36mBaseChatModel._convert_input\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "chain = prompt | model\n",
    "\n",
    "# async for chunk in chain.astream_events({'topic': 'parrot'}, version='v1'):\n",
    "# print(chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
