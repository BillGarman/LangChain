{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0bdb3b97-4989-4237-b43b-5943dbbd8302",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 1.5\n",
    "title: Streaming\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb7d49db-04d3-4399-bfe1-09f82bbe6015",
   "metadata": {},
   "source": [
    "# Streaming With LangChain\n",
    "\n",
    "Streaming is critical in making applications based on LLMs feel responsive to end-users.\n",
    "\n",
    "Important LangChain primitives like LLMs, parsers, prompts, retrievers, and agents implement the LangChain [Runnable Interface](/docs/expression_language/interface).\n",
    "\n",
    "This interface provides two general approaches to stream content:\n",
    "\n",
    "1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n",
    "2. async `astream_events` and async`astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n",
    "\n",
    "Here, we'll take a look at both approaches, and try to understand a bit of what's happening under the hood. 🥷\n",
    "\n",
    "## Using Stream\n",
    "\n",
    "All `Runnable` objects implement a sync method called `stream` and an async variant called `astream`. \n",
    "\n",
    "These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n",
    "\n",
    "Streaming is only possible if all steps in the program know how to operate an **input stream**, process an input chunk one at a time, and yield a corresponding output chunk.\n",
    "\n",
    "Such processing logic can range from being trivial (e.g., output the tokens generated by the LLM) to fairly difficult (e.g., streaming partial JSON results before the full JSON is available).\n",
    "\n",
    "The best place to start exploring streaming is with the single most important components in LLMs apps -- the LLLMs themselves!\n",
    "\n",
    "### LLMs and Chat Models\n",
    "\n",
    "Large language models and their chat variants are the primary bottleneck in LLM based apps. 🙊\n",
    "\n",
    "Large language models may take up to a **few seconds** to generate a complete response to a query.\n",
    "\n",
    "This is far larger than the **~200-300 ms** threshold at which an application still feels responsive to an end user.\n",
    "\n",
    "The primary solution to this problem is to stream the output from the model **token by token**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91787fc7-d941-48c0-a8b4-0ee61ab7dd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello|!| My| name| is| Claude|.| I|'m| an| artificial| intelligence| assistant| created| by| An|throp|ic| to| be| helpful|,| harmless|,| and| honest|.||"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic()\n",
    "\n",
    "chunks = []\n",
    "async for chunk in model.astream(\"hello. tell me something about yourself\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66730a87-77d5-40d6-a68f-315121989bd1",
   "metadata": {},
   "source": [
    "Let's inspect one of the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dade3000-1ac4-4f5c-b5c6-a0217f9f8a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=' Hello')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a47193-2bd1-46bc-9c7e-ea0f6b08c4a5",
   "metadata": {},
   "source": [
    "We got back something called an `AIMessageChunk`. This chunk represents a part of an `AIMessage`.\n",
    "\n",
    "Message chunks are additive by design -- one can simply add them up to get the state of the response so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3cf5f38-249c-4da0-94e6-5e5203fad52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=' Hello! My name is')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffbd9a-3b79-44b6-8883-1371f9460c77",
   "metadata": {},
   "source": [
    "### Chains\n",
    "\n",
    "Virtually all LLM applications involve more steps than just a call to a language model.\n",
    "\n",
    "Let's build a simple chain using `LangChain Expression Language` (`LCEL`) that combines a prompt, model and a parser and verify that streaming works.\n",
    "\n",
    "We will use `StrOutputParser` to parse the output from the model. This is a simple parser that extracts the `content` field from an `AIMessageChunk`, giving us the `token` returned by the model.\n",
    "\n",
    ":::{.callout-tip}\n",
    "LCEL provides a *declarative* way to specify a \"program\" by chainining together LangChain primitives. Chains created using LCEL benefit from an automatic implementation of `stream`, and `astream` allowing streaming of final output. In fact, chains created with LCEL implement\n",
    "the entire standard Runnable interface.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8562ae2-3fd1-4829-9801-a5a732b1798d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here|'s| a| silly| joke| about| par|rots|:|\n",
      "\n",
      "What| do| you| call| a| par|rot| that| flew| away|?| |\n",
      "A| polygon|!|\n",
      "\n",
      "Get| it|?| Because| if| a| par|rot| flies| far| away|,| eventually| it| becomes| a| dot| on| the| horizon|,| which| looks| like| a| point|,| which| sounds| like| \"|polygon|\"| when| said| out| loud|!| It|'s| a| very| cor|ny| joke| playing| on| words|,| but| I| hope| it| made| you| chuck|le| a| bit|.| Par|rots| can| be| pretty| funny| birds|,| so| there|'s| lots| of| potential| for| bird|-|b|rained| humor| about| them|!||"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "async for chunk in chain.astream({\"topic\": \"parrot\"}):\n",
    "    print(chunk, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b399fb4-5e3c-4581-9570-6df9b42b623d",
   "metadata": {},
   "source": [
    ":::{.callout-info}\n",
    "You do not have to use the `LangChain Expression Language` to use LangChain and can instead rely on a standard **imperative** programming approach by\n",
    "caling `invoke`, `batch` or `stream` on each component individually, assigning the results to variables and then using them downstream as you see fit.\n",
    "\n",
    "If that works for your needs, then that's fine by us 👌!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff2701-8887-486f-8b3b-eb26383d4bb6",
   "metadata": {},
   "source": [
    "### Working with Input Streams\n",
    "\n",
    "What if you wanted to stream JSON from the output as it was being generated?\n",
    "\n",
    "If you were to rely on `json.loads` to parse the partial json, the parsing would fail as the partial json wouldn't be valid json.\n",
    "\n",
    "You'd likely be at a complete loss of what to do and claim that it wasn't possible to stream JSON.\n",
    "\n",
    "Well, turns out there is a way to do it -- the parser needs to operate on the **input stream**, and attempt to extract valid json\n",
    "from partial model output.\n",
    "\n",
    "Let's see such a parser in action to understand what this means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff63cce-715a-4561-951f-9321c82e8d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'countries': []}\n",
      "{'countries': [{}]}\n",
      "{'countries': [{'name': ''}]}\n",
      "{'countries': [{'name': 'France'}]}\n",
      "{'countries': [{'name': 'France', 'population': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': '67'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,778'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,778'}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,778'}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,778'}, {'name': 'Japan'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,778'}, {'name': 'Japan', 'population': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,778'}, {'name': 'Japan', 'population': '126'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,778'}, {'name': 'Japan', 'population': '126,'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,778'}, {'name': 'Japan', 'population': '126,860'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,778'}, {'name': 'Japan', 'population': '126,860,'}]}\n",
      "{'countries': [{'name': 'France', 'population': '67,081,000'}, {'name': 'Spain', 'population': '46,754,778'}, {'name': 'Japan', 'population': '126,860,301'}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = model | JsonOutputParser()  # This parser only works with OpenAI right now\n",
    "async for text in chain.astream(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'\n",
    "):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8a9f0-4627-4ddb-a6e8-7b7f386bffb9",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "Any steps in the chain that operate on **full inputs** rather than on **input streams** can break streaming functionality via `stream` or `astream`.\n",
    ":::\n",
    "\n",
    ":::{.callout-tip}\n",
    "Later, we will discuss the `astream_events` API which streams results from intermediate steps. This API will stream results from intermediate steps even if the chain contains steps that only operate on **finalized inputs**.\n",
    ":::\n",
    "\n",
    "Let's use the previous example and add a bit of logic on top that breaks streaming using a function that extracts the countries names from the finalized JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c90117-9faa-4a01-b484-0db071808d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None]|"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "def _extract_country_names(inputs):\n",
    "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
    "    if not isinstance(inputs, dict):\n",
    "        return \"\"\n",
    "\n",
    "    if \"countries\" not in inputs:\n",
    "        return \"\"\n",
    "\n",
    "    countries = inputs[\"countries\"]\n",
    "\n",
    "    if not isinstance(countries, list):\n",
    "        return \"\"\n",
    "\n",
    "    country_names = [\n",
    "        country.get(\"name\") for country in countries if isinstance(country, dict)\n",
    "    ]\n",
    "    return country_names\n",
    "\n",
    "\n",
    "chain = model | JsonOutputParser() | _extract_country_names\n",
    "\n",
    "async for text in chain.astream(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\"'\n",
    "):\n",
    "    print(text, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638df0b-9386-4dd8-badd-7b8b052724e8",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "Using a generator function (a function that uses `yield`) allows writing code that operators on **input streams**\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15984b2b-315a-4119-945b-2a3dabea3082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France|Spain|Japan|"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "async def _extract_country_names(input_stream):\n",
    "    \"\"\"A function that operates on input streams.\"\"\"\n",
    "    country_names_so_far = set()\n",
    "\n",
    "    async for input in input_stream:\n",
    "        if not isinstance(input, dict):\n",
    "            continue\n",
    "\n",
    "        if \"countries\" not in input:\n",
    "            continue\n",
    "\n",
    "        countries = input[\"countries\"]\n",
    "\n",
    "        if not isinstance(countries, list):\n",
    "            continue\n",
    "\n",
    "        for country in countries:\n",
    "            name = country.get(\"name\")\n",
    "            if not name:\n",
    "                continue\n",
    "            if name not in country_names_so_far:\n",
    "                yield name\n",
    "                country_names_so_far.add(name)\n",
    "\n",
    "\n",
    "chain = model | JsonOutputParser() | _extract_country_names\n",
    "\n",
    "async for text in chain.astream(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\"'\n",
    "):\n",
    "    print(text, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf65b7-aa47-4321-98c7-a0abe43b833a",
   "metadata": {},
   "source": [
    "### Non-streaming components\n",
    "\n",
    "Some built-in components like Retrievers do not offer any `streaming`. What happens if we try to `stream` them? 🤨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9b1c00d-8b44-40d0-9e2b-8a70d238f82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(page_content='harrison worked at kensho'),\n",
       "  Document(page_content='harrison likes spicy food')]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\", \"harrison likes spicy food\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3e71b-439e-418f-8a8a-5232fba3d9fd",
   "metadata": {},
   "source": [
    "Stream just yielded the final result from that component. This is OK 🥹! \n",
    "\n",
    "Not all components have to implement streaming as in some cases such implementation may be unnecessary, difficult or not make sense.\n",
    "\n",
    ":::{.callout-info}\n",
    "An LCEL chain constructed using using non-streaming components, will still be able to stream in a lot of cases, with streaming of partial output starting after the last non-streaming step in the chain.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "957447e6-1e60-41ef-8c10-2654bd9e738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = (\n",
    "    {\n",
    "        \"context\": retriever.with_config(run_name=\"Docs\"),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94e50b5d-bf51-4eee-9da0-ee40dd9ce42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|H|arrison| worked| at| Kens|ho|,| a| renowned| technology| company| specializing| in| artificial| intelligence| and| data| analytics|.| Located| in| the| heart| of| Silicon| Valley|,| Kens|ho| offers| a| vibrant| and| innovative| work| environment|,| fostering| collaboration| and| creativity| among| its| employees|.| With| state|-of|-the|-art| facilities| and| cutting|-edge| technology|,| Harrison| was| part| of| a| dynamic| team| that| revolution|ized| the| way| businesses| approach| data| analysis|.| At| Kens|ho|,| Harrison| had| the| opportunity| to| work| with| industry|-leading| experts| and| contribute| to| groundbreaking| projects| that| shaped| the| future| of| AI| technology|.||"
     ]
    }
   ],
   "source": [
    "for chunk in retrieval_chain.stream(\n",
    "    \"Where did harrison work? Write 3 made up sentences about this place.\"\n",
    "):\n",
    "    print(chunk, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8657aa4e-3469-4b5b-a09c-60b53a23b1e7",
   "metadata": {},
   "source": [
    "Now that we've seen how `stream` and `astream` works. Let's bravely venture into the world of streaming events. 🏞️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb39342f-47b9-4e0a-b88e-29cddccaa32b",
   "metadata": {},
   "source": [
    "## Using Stream Events\n",
    "\n",
    "Event Streaming is a **beta** API, and may change a bit based on feedback. \n",
    "\n",
    ":::{.callout-info}\n",
    "Introduced in langchain-core **0.1.14**.\n",
    ":::\n",
    "\n",
    "When using the astream_events API, for everything to work properly please:\n",
    "\n",
    "* Use `async` throughout the code (e.g., async tools etc)\n",
    "* Propagate callbacks if defining custom functions / runnables\n",
    "* Whenever using runnables without LCEL, make sure to call `.astream()` on LLMs rather than `.ainvoke` to force the LLM to stream tokens.\n",
    "\n",
    "### Event Reference\n",
    "\n",
    "Below is a reference table that shows some events that might be emitted by the various Runnable objects.\n",
    "\n",
    "⚠️ When streaming the inputs for the runnable will not be available until the input stream has been entirely consumed This means that the inputs will be available at for the corresponding `end` hook rather than `start` event.\n",
    "\n",
    "\n",
    "| event                | name             | chunk                           | input                                         | output                                          |\n",
    "|----------------------|------------------|---------------------------------|-----------------------------------------------|-------------------------------------------------|\n",
    "| on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
    "| on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
    "| on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | {\"generations\": [...], \"llm_output\": None, ...} |\n",
    "| on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
    "| on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
    "| on_llm_end           | [model name]     |                                 | 'Hello human!'                                |\n",
    "| on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
    "| on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
    "| on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
    "| on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
    "| on_tool_stream       | some_tool        | {\"x\": 1, \"y\": \"2\"}              |                                               |                                                 |\n",
    "| on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
    "| on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
    "| on_retriever_chunk   | [retriever name] | {documents: [...]}              |                                               |                                                 |\n",
    "| on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | {documents: [...]}                              |\n",
    "| on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
    "| on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
    "\n",
    "\n",
    "\n",
    "If you're on an earlier version of LangChain this API will not exist for you! \n",
    "\n",
    "This is currently a **beta API**, and we would love to hear any feedback about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44b4f8f2-3a92-4bee-b09e-54d37460f5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.14'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain_core\n",
    "\n",
    "langchain_core.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ec135-3348-4041-8f55-bf3e59b3b2d0",
   "metadata": {},
   "source": [
    "### Chat Model\n",
    "\n",
    "Let's start off by looking at the events produced by a chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c00df46e-7f6b-4e06-8abf-801898c8d57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugene/.pyenv/versions/3.11.4/envs/langchain_3_11_4/lib/python3.11/site-packages/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "events = []\n",
    "async for event in model.astream_events(\"hello\", version=\"v1\"):\n",
    "    events.append(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32972939-2995-4b2e-84db-045adb044fad",
   "metadata": {},
   "source": [
    "\n",
    ":::{.callout-info}\n",
    "\n",
    "Hey what's that funny version=\"v1\" parameter in the API?! 😾\n",
    "\n",
    "This is a **beta API**, and we're almost certainly going to make some changes to it.\n",
    "\n",
    "This version parameter will allow us to mimimize such breaking changes to your code. \n",
    "\n",
    "Essentially, we are annoying you now, so we don't have to annoy you later.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2b8f47-da78-4569-a49a-53a8efaa26bc",
   "metadata": {},
   "source": [
    "Let's take a look at the few of the start event and a few of the end events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce31b525-f47d-4828-85a7-912ce9f2e79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chat_model_start',\n",
       "  'run_id': '60b654c8-f516-4a0e-ac75-d7b702789f39',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'data': {'input': 'hello'}},\n",
       " {'event': 'on_chat_model_stream',\n",
       "  'run_id': '60b654c8-f516-4a0e-ac75-d7b702789f39',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'name': 'ChatOpenAI',\n",
       "  'data': {'chunk': AIMessageChunk(content='')}},\n",
       " {'event': 'on_chat_model_stream',\n",
       "  'run_id': '60b654c8-f516-4a0e-ac75-d7b702789f39',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'name': 'ChatOpenAI',\n",
       "  'data': {'chunk': AIMessageChunk(content='Hello')}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76cfe826-ee63-4310-ad48-55a95eb3b9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chat_model_stream',\n",
       "  'run_id': '60b654c8-f516-4a0e-ac75-d7b702789f39',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'name': 'ChatOpenAI',\n",
       "  'data': {'chunk': AIMessageChunk(content='')}},\n",
       " {'event': 'on_chat_model_end',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'run_id': '60b654c8-f516-4a0e-ac75-d7b702789f39',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'data': {'output': AIMessageChunk(content='Hello! How can I assist you today?')}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c8f173-e9c7-4c27-81a5-b7c85c12714d",
   "metadata": {},
   "source": [
    "### Chain\n",
    "\n",
    "Let's re-use example chain that parsed streaming JSON to explore the streaming events API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4328c56c-a303-427b-b1f2-f354e9af555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = model | JsonOutputParser()  # This parser only works with OpenAI right now\n",
    "\n",
    "events = [\n",
    "    event\n",
    "    async for event in chain.astream_events(\n",
    "        'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n",
    "        version=\"v1\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc00b99-a961-4221-a3c7-9d807114bbfb",
   "metadata": {},
   "source": [
    "If you examine at the first few events, you'll notice that there are **3** different start events rather than **2** start events.\n",
    "\n",
    "The three start events correspond to:\n",
    "\n",
    "1. The chain (model + parser)\n",
    "2. The model\n",
    "3. The parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e66ea3d-a450-436a-aaac-d9478abc6c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chain_start',\n",
       "  'run_id': '2ba4da15-91ef-4d47-8fac-a6bd634f31de',\n",
       "  'name': 'RunnableSequence',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}},\n",
       " {'event': 'on_chat_model_start',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'run_id': 'd8a52c93-a3ce-4931-9a95-18a0a609fbec',\n",
       "  'tags': ['seq:step:1'],\n",
       "  'metadata': {},\n",
       "  'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`')]]}}},\n",
       " {'event': 'on_parser_start',\n",
       "  'name': 'JsonOutputParser',\n",
       "  'run_id': '4e5e3ee6-343a-4b8b-a3b5-b9d2fdf9be43',\n",
       "  'tags': ['seq:step:2'],\n",
       "  'metadata': {},\n",
       "  'data': {}}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8512238-d035-4acd-9248-a8570da064c9",
   "metadata": {},
   "source": [
    "What do you think you'd see if you looked at the last 3 events? what about the middle?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c742cfa4-9b03-4a5b-96d9-5fe56e95e3b4",
   "metadata": {},
   "source": [
    "Let's use this API to take output the stream events from the model and the parser. We're ignoring start events, end events and events from the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "630c71d6-8d94-4ce0-a78a-f20e90f628df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat model chunk: ''\n",
      "Parser chunk: {}\n",
      "Chat model chunk: '{\\n'\n",
      "Chat model chunk: ' '\n",
      "Chat model chunk: ' \"'\n",
      "Chat model chunk: 'countries'\n",
      "Chat model chunk: '\":'\n",
      "Parser chunk: {'countries': []}\n",
      "Chat model chunk: ' [\\n'\n",
      "Chat model chunk: '   '\n",
      "Parser chunk: {'countries': [{}]}\n",
      "Chat model chunk: ' {\\n'\n",
      "Chat model chunk: '     '\n",
      "Chat model chunk: ' \"'\n",
      "Chat model chunk: 'name'\n",
      "Chat model chunk: '\":'\n",
      "Parser chunk: {'countries': [{'name': ''}]}\n",
      "Chat model chunk: ' \"'\n",
      "Parser chunk: {'countries': [{'name': 'France'}]}\n",
      "Chat model chunk: 'France'\n",
      "Chat model chunk: '\",\\n'\n",
      "Chat model chunk: '     '\n",
      "Chat model chunk: ' \"'\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "num_events = 0\n",
    "\n",
    "async for event in chain.astream_events(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n",
    "    version=\"v1\",\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(f\"Chat model chunk: {repr(event['data']['chunk'].content)}\", flush=True)\n",
    "    if kind == \"on_parser_stream\":\n",
    "        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)\n",
    "    num_events += 1\n",
    "    if num_events > 30:\n",
    "        # Truncate the output\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ea891-997c-454c-bf60-43124f40ee1b",
   "metadata": {},
   "source": [
    "Because both the model and the parser support streaming, we see sreaming events from both components in real time! Kind of cool isn't it? 🦜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5084148b-bcdc-4373-9caa-6568f03e7b23",
   "metadata": {},
   "source": [
    "### Filtering Events\n",
    "\n",
    "Because this API produces so many events, it is useful to be able to filter on events.\n",
    "\n",
    "You can filter by either component `name`, component `tags` or component `type`.\n",
    "\n",
    "#### By Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f0b581b-be63-4663-baba-c6d2b625cdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_parser_start', 'name': 'my_parser', 'run_id': '6801e146-21a7-4660-8521-a6e2ed6080df', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {}}\n",
      "{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': '6801e146-21a7-4660-8521-a6e2ed6080df', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {}}}\n",
      "{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': '6801e146-21a7-4660-8521-a6e2ed6080df', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': []}}}\n",
      "{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': '6801e146-21a7-4660-8521-a6e2ed6080df', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{}]}}}\n",
      "{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': '6801e146-21a7-4660-8521-a6e2ed6080df', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': ''}]}}}\n",
      "{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': '6801e146-21a7-4660-8521-a6e2ed6080df', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France'}]}}}\n",
      "{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': '6801e146-21a7-4660-8521-a6e2ed6080df', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': ''}]}}}\n",
      "{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': '6801e146-21a7-4660-8521-a6e2ed6080df', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': '67'}]}}}\n",
      "{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': '6801e146-21a7-4660-8521-a6e2ed6080df', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': '67,'}]}}}\n",
      "{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': '6801e146-21a7-4660-8521-a6e2ed6080df', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': '67,081'}]}}}\n",
      "{'event': 'on_parser_stream', 'name': 'my_parser', 'run_id': '6801e146-21a7-4660-8521-a6e2ed6080df', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': '67,081,'}]}}}\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "chain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(\n",
    "    {\"run_name\": \"my_parser\"}\n",
    ")\n",
    "\n",
    "max_events = 0\n",
    "async for event in chain.astream_events(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n",
    "    version=\"v1\",\n",
    "    include_names=[\"my_parser\"],\n",
    "):\n",
    "    print(event)\n",
    "    max_events += 1\n",
    "    if max_events > 10:\n",
    "        # Truncate output\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59d5626-7dba-4eb3-ad81-76c1092c5146",
   "metadata": {},
   "source": [
    "#### By Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "096cd904-72f0-4ebe-a8b7-d0e730faea7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chat_model_start', 'name': 'model', 'run_id': 'c702b10a-7b47-41dd-83d9-0163d9489bce', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`')]]}}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': 'c702b10a-7b47-41dd-83d9-0163d9489bce', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content='')}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': 'c702b10a-7b47-41dd-83d9-0163d9489bce', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content='{\\n')}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': 'c702b10a-7b47-41dd-83d9-0163d9489bce', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' ')}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': 'c702b10a-7b47-41dd-83d9-0163d9489bce', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' \"')}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': 'c702b10a-7b47-41dd-83d9-0163d9489bce', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content='countries')}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': 'c702b10a-7b47-41dd-83d9-0163d9489bce', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content='\":')}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': 'c702b10a-7b47-41dd-83d9-0163d9489bce', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' [\\n')}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': 'c702b10a-7b47-41dd-83d9-0163d9489bce', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content='   ')}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': 'c702b10a-7b47-41dd-83d9-0163d9489bce', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' {\\n')}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'model', 'run_id': 'c702b10a-7b47-41dd-83d9-0163d9489bce', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content='     ')}}\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "chain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(\n",
    "    {\"run_name\": \"my_parser\"}\n",
    ")\n",
    "\n",
    "max_events = 0\n",
    "async for event in chain.astream_events(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n",
    "    version=\"v1\",\n",
    "    include_types=[\"chat_model\"],\n",
    "):\n",
    "    print(event)\n",
    "    max_events += 1\n",
    "    if max_events > 10:\n",
    "        # Truncate output\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ec8dd4-9b5b-4000-b63f-5845bfc5a065",
   "metadata": {},
   "source": [
    "#### By Tags\n",
    "\n",
    ":::{.callout-warning}\n",
    "\n",
    "Tags are inherited by child components of a given runnable. \n",
    "\n",
    "If you're using tags to filter, make sure that this is what you want.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26bac0d2-76d9-446e-b346-82790236b88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'run_id': 'c5699d72-3afb-44d7-bd66-ecf073abe668', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}, 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}}\n",
      "{'event': 'on_chat_model_start', 'name': 'ChatOpenAI', 'run_id': '220e6a6e-f4bc-4614-afdd-4cc709f9d1fb', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`')]]}}}\n",
      "{'event': 'on_parser_start', 'name': 'JsonOutputParser', 'run_id': '6c34c77c-cf48-48c5-a151-3e227634e873', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': '220e6a6e-f4bc-4614-afdd-4cc709f9d1fb', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content='')}}\n",
      "{'event': 'on_parser_stream', 'name': 'JsonOutputParser', 'run_id': '6c34c77c-cf48-48c5-a151-3e227634e873', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {'chunk': {}}}\n",
      "{'event': 'on_chain_stream', 'run_id': 'c5699d72-3afb-44d7-bd66-ecf073abe668', 'tags': ['my_chain'], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': {}}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': '220e6a6e-f4bc-4614-afdd-4cc709f9d1fb', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content='{\\n')}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': '220e6a6e-f4bc-4614-afdd-4cc709f9d1fb', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' ')}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': '220e6a6e-f4bc-4614-afdd-4cc709f9d1fb', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content=' \"')}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': '220e6a6e-f4bc-4614-afdd-4cc709f9d1fb', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content='countries')}}\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOpenAI', 'run_id': '220e6a6e-f4bc-4614-afdd-4cc709f9d1fb', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {}, 'data': {'chunk': AIMessageChunk(content='\":')}}\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "chain = (model | JsonOutputParser()).with_config({\"tags\": [\"my_chain\"]})\n",
    "\n",
    "max_events = 0\n",
    "async for event in chain.astream_events(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n",
    "    version=\"v1\",\n",
    "    include_tags=[\"my_chain\"],\n",
    "):\n",
    "    print(event)\n",
    "    max_events += 1\n",
    "    if max_events > 10:\n",
    "        # Truncate output\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e54c4-61a2-4f6c-aa68-d2b09b5e1d4f",
   "metadata": {},
   "source": [
    "### Non-streaming components\n",
    "\n",
    "Remember how some components don't stream well because they don't operate on **input streams**?\n",
    "\n",
    "While such components can break streaming of the final output when using `astream`, `astream_events` will still yield stream events of intermediate steps that support streaming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e6451d3-3b11-4a71-ae19-998f4c10180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_country_names(inputs):\n",
    "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
    "    if not isinstance(inputs, dict):\n",
    "        return \"\"\n",
    "\n",
    "    if \"countries\" not in inputs:\n",
    "        return \"\"\n",
    "\n",
    "    countries = inputs[\"countries\"]\n",
    "\n",
    "    if not isinstance(countries, list):\n",
    "        return \"\"\n",
    "\n",
    "    country_names = [\n",
    "        country.get(\"name\") for country in countries if isinstance(country, dict)\n",
    "    ]\n",
    "    return country_names\n",
    "\n",
    "\n",
    "chain = (\n",
    "    model | JsonOutputParser() | _extract_country_names\n",
    ")  # This parser only works with OpenAI right now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a972e1a6-80cd-4d59-90a0-73563f1503d4",
   "metadata": {},
   "source": [
    "As expected, the `astream` API doesn't work correctly because `_extract_country_names` doesn't operate on streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9a8fe35-faab-4970-b8c0-5c780845d98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'Spain', 'Japan']\n"
     ]
    }
   ],
   "source": [
    "async for chunk in chain.astream(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b279ea33-54f1-400a-acb1-b8445ccbf1fa",
   "metadata": {},
   "source": [
    "Now, let's confirm that with astream_events we're still seeing streaming output from the model and the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b08215cd-bffa-4e76-aaf3-c52ee34f152c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat model chunk: ''\n",
      "Parser chunk: {}\n",
      "Chat model chunk: '{\\n'\n",
      "Chat model chunk: ' '\n",
      "Chat model chunk: ' \"'\n",
      "Chat model chunk: 'countries'\n",
      "Chat model chunk: '\":'\n",
      "Parser chunk: {'countries': []}\n",
      "Chat model chunk: ' [\\n'\n",
      "Chat model chunk: '   '\n",
      "Parser chunk: {'countries': [{}]}\n",
      "Chat model chunk: ' {\\n'\n",
      "Chat model chunk: '     '\n",
      "Chat model chunk: ' \"'\n",
      "Chat model chunk: 'name'\n",
      "Chat model chunk: '\":'\n",
      "Parser chunk: {'countries': [{'name': ''}]}\n",
      "Chat model chunk: ' \"'\n",
      "Parser chunk: {'countries': [{'name': 'France'}]}\n",
      "Chat model chunk: 'France'\n",
      "Chat model chunk: '\",\\n'\n",
      "Chat model chunk: '     '\n",
      "Chat model chunk: ' \"'\n",
      "Chat model chunk: 'population'\n",
      "Chat model chunk: '\":'\n",
      "Parser chunk: {'countries': [{'name': 'France', 'population': ''}]}\n",
      "Chat model chunk: ' \"'\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "num_events = 0\n",
    "\n",
    "async for event in chain.astream_events(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n",
    "    version=\"v1\",\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(f\"Chat model chunk: {repr(event['data']['chunk'].content)}\", flush=True)\n",
    "    if kind == \"on_parser_stream\":\n",
    "        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)\n",
    "    num_events += 1\n",
    "    if num_events > 30:\n",
    "        # Truncate the output\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91bdd3-f4a3-4b3c-b21a-26365c6c1566",
   "metadata": {},
   "source": [
    "### Propagating Callbacks\n",
    "\n",
    ":::{.callout-attention}\n",
    "\n",
    "If you're using runnables defining custom tools, you should propagate callbacks otherwise no stream events will be generated.\n",
    "\n",
    ":::\n",
    "\n",
    ":::{.callout-info}\n",
    "\n",
    "When using RunnableLambdas or @chain decorator, callbacks are propagated automatically behind the scenes.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1854206d-b3a5-4f91-9e00-bccbaebac61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_tool_start', 'run_id': '4ff0f956-252a-47ea-ba19-c4208b848914', 'name': 'my_func', 'tags': [], 'metadata': {}, 'data': {'input': 'hello'}}\n",
      "{'event': 'on_tool_stream', 'run_id': '4ff0f956-252a-47ea-ba19-c4208b848914', 'tags': [], 'metadata': {}, 'name': 'my_func', 'data': {'chunk': 'olleh'}}\n",
      "{'event': 'on_tool_end', 'name': 'my_func', 'run_id': '4ff0f956-252a-47ea-ba19-c4208b848914', 'tags': [], 'metadata': {}, 'data': {'output': 'olleh'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "def reverse_word(word: str):\n",
    "    return word[::-1]\n",
    "\n",
    "\n",
    "reverse_word = RunnableLambda(reverse_word)\n",
    "\n",
    "\n",
    "@tool\n",
    "def my_func(word: str):\n",
    "    \"\"\"My func\"\"\"\n",
    "    return reverse_word.invoke(word)\n",
    "\n",
    "\n",
    "async for event in my_func.astream_events(\"hello\", version=\"v1\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a20a6cb3-bb43-465c-8cfc-0a7349d70968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_tool_start', 'run_id': '5c08b99f-15d2-4131-a8e0-6d7fc5ce1fd7', 'name': 'correct_tool', 'tags': [], 'metadata': {}, 'data': {'input': 'hello'}}\n",
      "{'event': 'on_chain_start', 'name': 'reverse_word', 'run_id': 'c8c97861-6f1d-4e8d-abcf-7d13f63e2566', 'tags': [], 'metadata': {}, 'data': {'input': 'hello'}}\n",
      "{'event': 'on_chain_end', 'name': 'reverse_word', 'run_id': 'c8c97861-6f1d-4e8d-abcf-7d13f63e2566', 'tags': [], 'metadata': {}, 'data': {'input': 'hello', 'output': 'olleh'}}\n",
      "{'event': 'on_tool_stream', 'run_id': '5c08b99f-15d2-4131-a8e0-6d7fc5ce1fd7', 'tags': [], 'metadata': {}, 'name': 'correct_tool', 'data': {'chunk': 'olleh'}}\n",
      "{'event': 'on_tool_end', 'name': 'correct_tool', 'run_id': '5c08b99f-15d2-4131-a8e0-6d7fc5ce1fd7', 'tags': [], 'metadata': {}, 'data': {'output': 'olleh'}}\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def correct_tool(word: str, callbacks):\n",
    "    \"\"\"A tool that correctly propagates callbacks.\"\"\"\n",
    "    return reverse_word.invoke(word, {\"callbacks\": callbacks})\n",
    "\n",
    "\n",
    "async for event in correct_tool.astream_events(\"hello\", version=\"v1\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ac0a3c1-f3a4-4157-b053-4fec8d2e698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'run_id': '24e3e2d7-8105-42bb-9339-66f3f81083a7', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'data': {'input': '1234'}}\n",
      "{'event': 'on_chain_start', 'name': 'reverse_word', 'run_id': '3591decf-02c0-4533-8d84-e3426b107cb3', 'tags': [], 'metadata': {}, 'data': {'input': '1234'}}\n",
      "{'event': 'on_chain_end', 'name': 'reverse_word', 'run_id': '3591decf-02c0-4533-8d84-e3426b107cb3', 'tags': [], 'metadata': {}, 'data': {'input': '1234', 'output': '4321'}}\n",
      "{'event': 'on_chain_stream', 'run_id': '24e3e2d7-8105-42bb-9339-66f3f81083a7', 'tags': [], 'metadata': {}, 'name': 'reverse_and_double', 'data': {'chunk': '43214321'}}\n",
      "{'event': 'on_chain_end', 'name': 'reverse_and_double', 'run_id': '24e3e2d7-8105-42bb-9339-66f3f81083a7', 'tags': [], 'metadata': {}, 'data': {'output': '43214321'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "async def reverse_and_double(word: str):\n",
    "    return await reverse_word.ainvoke(word) * 2\n",
    "\n",
    "\n",
    "reverse_and_double = RunnableLambda(reverse_and_double)\n",
    "\n",
    "await reverse_and_double.ainvoke(\"1234\")\n",
    "\n",
    "async for event in reverse_and_double.astream_events(\"1234\", version=\"v1\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a34268-9b3d-4857-b4ed-65d95f4a1293",
   "metadata": {},
   "source": [
    "Or you can use the chain decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c896bb94-9d10-41ff-8fe2-d6b05b1ed74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'run_id': '7be3c371-f5bd-47e5-bc80-19f0284582b0', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'data': {'input': '1234'}}\n",
      "{'event': 'on_chain_start', 'name': 'reverse_word', 'run_id': '6fb0970b-855a-4a14-83d6-a5068926f6db', 'tags': [], 'metadata': {}, 'data': {'input': '1234'}}\n",
      "{'event': 'on_chain_end', 'name': 'reverse_word', 'run_id': '6fb0970b-855a-4a14-83d6-a5068926f6db', 'tags': [], 'metadata': {}, 'data': {'input': '1234', 'output': '4321'}}\n",
      "{'event': 'on_chain_stream', 'run_id': '7be3c371-f5bd-47e5-bc80-19f0284582b0', 'tags': [], 'metadata': {}, 'name': 'reverse_and_double', 'data': {'chunk': '43214321'}}\n",
      "{'event': 'on_chain_end', 'name': 'reverse_and_double', 'run_id': '7be3c371-f5bd-47e5-bc80-19f0284582b0', 'tags': [], 'metadata': {}, 'data': {'output': '43214321'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "@chain\n",
    "async def reverse_and_double(word: str):\n",
    "    return await reverse_word.ainvoke(word) * 2\n",
    "\n",
    "\n",
    "await reverse_and_double.ainvoke(\"1234\")\n",
    "\n",
    "async for event in reverse_and_double.astream_events(\"1234\", version=\"v1\"):\n",
    "    print(event)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
