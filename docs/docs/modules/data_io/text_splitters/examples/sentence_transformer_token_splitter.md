# SentenceTransformersTokenTextSplitter

This notebook demonstrates how to use the `SentenceTransformersTokenTextSplitter` text splitter.

Language models have a token limit. You should not exceed the token limit. When you split your text into chunks it is therefore a good idea to count the number of tokens. There are many tokenizers. When you count tokens in your text you should use the same tokenizer as used in the language model. 

The `SentenceTransformersTokenTextSplitter` is a specialized text splitter for use with the sentence-transformer models. The default behaviour is to split the text into chunks that fit the token window of the sentence transformer model that you would like to use.

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file. -->


```python
from langchain.text_splitter import SentenceTransformersTokenTextSplitter
```


```python
splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0)
text = "Lorem "
```


```python
count_start_and_stop_tokens = 2
text_token_count = splitter.count_tokens(text=text) - count_start_and_stop_tokens
print(text_token_count)
```

<CodeOutputBlock lang="python">

```
    2
```

</CodeOutputBlock>


```python
token_multiplier = splitter.maximum_tokens_per_chunk // text_token_count + 1

# `text_to_split` does not fit in a single chunk
text_to_split = text * token_multiplier

print(f"tokens in text to split: {splitter.count_tokens(text=text_to_split)}")
```

<CodeOutputBlock lang="python">

```
    tokens in text to split: 514
```

</CodeOutputBlock>


```python
text_chunks = splitter.split_text(text=text_to_split)

print(text_chunks[1])
```

<CodeOutputBlock lang="python">

```
    lorem
```

</CodeOutputBlock>
