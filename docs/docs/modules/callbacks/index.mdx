---
sidebar_position: 5
sidebar_class_name: hidden
---
# Callbacks

:::info
Head to [Integrations](/docs/integrations/callbacks/) for documentation on built-in callbacks integrations with 3rd-party tools.
:::

LangChain provides a callbacks system that provides a set of hooks to allow you entry into various stages of your application. This is useful for logging, tracing and monitoring.

## Concepts
Concepts include the hooks or events that provide entry to the application and the callbacks handler object which subscribes to one or more of these events.

### Events
| Event Name | Event Trigger | Associated Method |
| --- | --- | --- |
| LLM start  | When a llm starts | `on_llm_start`|
| LLM new token  | When a llm receives a new token | `on_llm_new_token`|
| LLM ends  | When a llm ends | `on_llm_end`|
| LLM errors  | When a llm errors | `on_llm_error`|
| Chat model start  | When a chat model starts | `on_chat_model_start`|
| Chain start  | When a chain starts running | `on_chain_start`|
| Chain end  | When a chain ends running | `on_chain_end`|
| Chain error  | When a chain errors | `on_chain_error`|
| Tool start  | When a tool starts running | `on_tool_start`|
| Tool end  | When a tool ends running | `on_tool_end`|
| Tool error  | When a tool errors | `on_tool_error`|
| Agent action  | When an agent takes an action | `on_agent_action`|
| Agent finish  | When an agent ends | `on_agent_finish`|
| Retriever start  | When a retriever starts | `on_retriever_start`|
| Retriever end  | When a retriever ends | `on_retriever_end`|
| Retriever error  | When a retriever errors | `on_retriever_error`|
| Text  | When arbitrary text is run | `on_text`|
| Retry  | When a retry event is run | `on_retry`|

### Callbacks Handler

Callbacks handlers are objects that implement the associated method (see table above) for each event you are interested in. Handlers register for these events using the `callbacks` argument available throughout the API. When the event is triggered, LangChain will call the appropriate method on every registered handler.


### Types

The callbacks are available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:

- **Constructor callbacks**: Defined in the constructor. In this case, the callbacks will be used for all calls made on that object, and will be scoped to that object only, e.g. if you pass a handler to the `LLMChain` constructor, it will not be used by the Model attached to that chain.
- **Request callbacks**: Defined in the `invoke` method used for issuing a request. In this case, the callbacks will be used for that specific request (and all sub-requests that it contains (e.g. defining in the `invoke` method of a LCEL chain triggers a call to all entities in the chain, using the same handler). The same approach can be used for the `ainvoke`, `batch` and `abatch` methods.):

#### When do you want to use each of these?

- Constructor callbacks are most useful for use cases such as logging, monitoring, etc., which are _not specific to a single request_, but rather to the entire chain. For example, if you want to log all the requests made to an entire chain, you would pass a handler to the constructor.
- Request callbacks are most useful for use cases such as streaming, where you want to stream the output of a single request to a specific websocket connection, or other similar use cases. For example, if you want to stream the output of a single request to a websocket, you would pass a handler to the `invoke()` method through the `config` option.

## Quick start

Let us understand how to use callbacks by examining one of the built-in handlers provided by LangChain - the `StdOutCallbackHandler`, which simply logs all events to `stdout`. This is useful for debugging, as it will log all events to the console.

**Note**: when the `verbose` flag on the object is set to true, the `StdOutCallbackHandler` will be invoked even without being explicitly passed in.

```python
from langchain_core.callbacks import StdOutCallbackHandler
from langchain_openai import OpenAI
from langchain_core.prompts import PromptTemplate

handler = StdOutCallbackHandler()
llm = OpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

# Constructor callback: we set the callback when initializing the prompt
chain = prompt.with_config({"callbacks": [handler]}) | model
chain.invoke({"number": 20})

# Constructor callback: we set the callback when initializing the model
chain = prompt | model.with_config({"callbacks": [handler]})
chain.invoke({"number": 25})

# Request callbacks: we set the request callback on the chain and see it propagated to all subrequests
chain = prompt | model
chain.invoke({"number": 40}, config = config)
```


<details> <summary>Console output</summary>


<CodeOutputBlock lang="python">

```
> Entering new PromptTemplate chain...
> Template: what is 1 + {number}?

> Finished chain.


> Entering OpenAI chain...
> Prompt: ['what is 1 + 25?']

> Finished llm chain.


> Entering new RunnableSequence chain...


> Entering new PromptTemplate chain...
> Template: what is 1 + {number}?

> Finished chain.


> Entering OpenAI chain...
> Prompt: ['what is 1 + 40?']

> Finished llm chain.

> Finished chain.
```

</CodeOutputBlock>

</details>