---
sidebar_position: 5
sidebar_class_name: hidden
---
# Callbacks

:::info
Head to [Integrations](/docs/integrations/callbacks/) for documentation on built-in callbacks integrations with 3rd-party tools.
:::

The core idea of LangChain's callbacks system is to provide a set of hooks to allow you entry into various stages of your application. This is useful for loggging, tracing and monitoring.

## Concepts
Concepts include the hooks or events that provide entry to the application and the callbacks handler object which subscribes to one or more of these events.

### Events
| Event Name | Event Trigger | Associated Method |
| --- | --- | --- |
| LLM start  | When a llm starts | `on_llm_start`|
| LLM new token  | When a llm receives a new token | `on_llm_new_token`|
| LLM ends  | When a llm ends | `on_llm_end`|
| LLM errors  | When a llm errors | `on_llm_error`|
| Chat model start  | When a chat model starts | `on_chat_model_start`|
| Chain start  | When a chain starts running | `on_chain_start`|
| Chain end  | When a chain ends runnning | `on_chain_end`|
| Chain error  | When a chain errors | `on_chain_error`|
| Tool start  | When a tool starts running | `on_tool_start`|
| Tool end  | When a tool ends running | `on_tool_end`|
| Tool error  | When a tool errors | `on_tool_error`|
| Agent action  | When an agent takes an action | `on_agent_action`|
| Agent finish  | When an agent ends | `on_agent_finish`|
| Retriever start  | When a retriever starts | `on_retriever_start`|
| Retriever end  | When a retriever ends | `on_retriever_end`|
| Retriever error  | When a retriever errors | `on_retriever_error`|
| Text  | When arbitrary text is run | `on_text`|
| Retry  | When a retry event is run | `on_retry`|

### Callbacks Handler

Callbacks handlers are objects theat implement the method for each event you are interested in. Handlers register for these events using the `callbacks` argument available throughout the API. When the event is triggered, LangChain will call the appropriated method on every registered handler.

## Quick start

LangChain provides a few built-in handlers that you can use to get started. The most basic handler is the `StdOutCallbackHandler`, which simply logs all events to `stdout`.

**Note**: when the `verbose` flag on the object is set to true, the `StdOutCallbackHandler` will be invoked even without being explicitly passed in.

```python
from langchain_core.callbacks import StdOutCallbackHandler
from langchain_openai import OpenAI
from langchain_core.prompts import PromptTemplate

handler = StdOutCallbackHandler()
llm = OpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

# Constructor callback: First, let's explicitly set the StdOutCallbackHandler when initializing our chain
# chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])
chain.invoke({"number":2, config={"callbacks": handler})

# Use verbose flag: Then, let's use the `verbose` flag to achieve the same result
chain = LLMChain(llm=llm, prompt=prompt, verbose=True)
chain.invoke({"number":2})

# Request callbacks: Finally, let's use the request `callbacks` to achieve the same result
chain = LLMChain(llm=llm, prompt=prompt)
chain.invoke({"number":2}, {"callbacks":[handler]})

```

<CodeOutputBlock lang="python">

```
> Entering new LLMChain chain...
Prompt after formatting:
1 + 2 = 

> Finished chain.


> Entering new LLMChain chain...
Prompt after formatting:
1 + 2 = 

> Finished chain.


> Entering new LLMChain chain...
Prompt after formatting:
1 + 2 = 

> Finished chain.
```

</CodeOutputBlock>

## Where to pass in callbacks

The `callbacks` are available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) in two different places:

- **Constructor callbacks**: defined in the constructor, e.g. `LLMChain(callbacks=[handler], tags=['a-tag'])`. In this case, the callbacks will be used for all calls made on that object, and will be scoped to that object only, e.g. if you pass a handler to the `LLMChain` constructor, it will not be used by the Model attached to that chain.
- **Request callbacks**: defined in the 'invoke' method used for issuing a request. In this case, the callbacks will be used for that specific request only, and all sub-requests that it contains (e.g. a call to an LLMChain triggers a call to a Model, which uses the same handler passed in the `invoke()` method).  In the `invoke()` method callbacks are passed through the config parameter.
Example with the 'invoke' method (**Note**: the same approach can be used for the `batch`, `ainvoke`, and `abatch` methods.):
```python
handler = StdOutCallbackHandler()
llm = OpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

config = {
    'callbacks' : [handler]
}

chain = prompt | chain
chain.invoke({"number":2}, config=config)
```

The `verbose` argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) as a constructor argument, e.g. `LLMChain(verbose=True)`, and it is equivalent to passing a `ConsoleCallbackHandler` to the `callbacks` argument of that object and all child objects. This is useful for debugging, as it will log all events to the console.

### When do you want to use each of these?

- Constructor callbacks are most useful for use cases such as logging, monitoring, etc., which are _not specific to a single request_, but rather to the entire chain. For example, if you want to log all the requests made to an entire chain, you would pass a handler to the constructor.
- Request callbacks are most useful for use cases such as streaming, where you want to stream the output of a single request to a specific websocket connection, or other similar use cases. For example, if you want to stream the output of a single request to a websocket, you would pass a handler to the `invoke()` method through the `config` option.

## tmp

LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.

You can subscribe to these events by using the `callbacks` argument available throughout the API. This argument is a list of handler objects, which are expected to implement one or more of the methods described below in more detail.

## Callback handlers

`CallbackHandlers` are objects that implement the `CallbackHandler` interface, which has a method for each event that can be subscribed to. The `CallbackManager` will call the appropriate method on each handler when the event is triggered.


```python
class BaseCallbackHandler:
    """Base callback handler that can be used to handle callbacks from langchain."""

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> Any:
        """Run when LLM starts running."""

    def on_chat_model_start(
        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any
    ) -> Any:
        """Run when Chat Model starts running."""

    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:
        """Run on new LLM token. Only available when streaming is enabled."""

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:
        """Run when LLM ends running."""

    def on_llm_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Run when LLM errors."""

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> Any:
        """Run when chain starts running."""

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:
        """Run when chain ends running."""

    def on_chain_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Run when chain errors."""

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> Any:
        """Run when tool starts running."""

    def on_tool_end(self, output: Any, **kwargs: Any) -> Any:
        """Run when tool ends running."""

    def on_tool_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Run when tool errors."""

    def on_text(self, text: str, **kwargs: Any) -> Any:
        """Run on arbitrary text."""

    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        """Run on agent action."""

    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:
        """Run on agent end."""
```
