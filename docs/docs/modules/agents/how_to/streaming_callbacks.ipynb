{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b69e747b-4e79-4caf-8f8b-c6e70275a31d",
   "metadata": {},
   "source": [
    "# Streaming With Callbacks (Advanced)\n",
    "\n",
    "In this notebook, we'll see how to combine agents with callbacks to achieve **token by token streaming** from the underlying tools!\n",
    "\n",
    "Our agent will use the OpenAI tools API for tool invocation, and we'll provide the agent with two tools:\n",
    "\n",
    "1. `where_cat_is_hiding`: A tool that uses an LLM to tell us where the cat is hiding\n",
    "2. `tell_me_a_joke_about`: A tool that can use an LLM to tell a joke about the given topic\n",
    "\n",
    "\n",
    "## astream_log\n",
    "\n",
    "**token by token streaming** can also be built entirely with astream_log as shown in the agent streaming notebook.\n",
    "\n",
    "## calllbacks\n",
    "\n",
    "In this example, we'll declare a callback that will print tokens to stdout based on whether they were generated from a model with matching tags.\n",
    "\n",
    "In a production setting, instead of printing the tokens to stdout, you can send them elsewhere (e.g., write them to a streaming response on your Fast API endpoint)!\n",
    "\n",
    "Please note that in this example, the callbacks are not passed to the constructor of the LLM, but instead passed as runtime parameters to `.invoke` or `.stream` methods. \n",
    "\n",
    "These callbacks become *inherited* callbacks that will be passed to all sub-dependencies when possible. When declaring tools, the tools must propagate the callbacks to the underlying LLMs for this to work (checkout the tools below to see how that works). This is generally a good practice as it also makes sure that debug traces are constructed correctly with the llm call appearing from inside the tool call.\n",
    "\n",
    "However, you can pass the callbacks to the constructor which will create a local (aka non-inherited callback), but then you don't have to propagate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f3cb97a-5af6-491f-9a82-a6ee3eaf2dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, TypeVar, Union\n",
    "from uuid import UUID\n",
    "\n",
    "from langchain import agents, hub\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.callbacks import Callbacks\n",
    "from langchain.tools import tool\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.callbacks.base import AsyncCallbackHandler\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Here is a custom handler that will print the tokens to stdout.\n",
    "# Instead of printing to stdout you can send the data elsewhere; e.g., to a streaming API response\n",
    "class TokenByTokenHandler(AsyncCallbackHandler):\n",
    "    def __init__(self, tags_of_interest: List[str]) -> None:\n",
    "        \"\"\"A custom call back handler.\n",
    "\n",
    "        Args:\n",
    "            tags_of_interest: Only LLM tokens from models with these tags will be\n",
    "                              printed.\n",
    "        \"\"\"\n",
    "        self.tags_of_interest = tags_of_interest\n",
    "\n",
    "    async def on_chat_model_start(\n",
    "            self,\n",
    "            serialized: Dict[str, Any],\n",
    "            messages: List[List[BaseMessage]],\n",
    "            *,\n",
    "            run_id: UUID,\n",
    "            parent_run_id: Optional[UUID] = None,\n",
    "            tags: Optional[List[str]] = None,\n",
    "            metadata: Optional[Dict[str, Any]] = None,\n",
    "            **kwargs: Any,\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when a chat model starts running.\"\"\"\n",
    "        overlap_tags = self.get_overlap_tags(tags)\n",
    "\n",
    "        if overlap_tags:\n",
    "            print(\",\".join(overlap_tags), end=': ', flush=True)\n",
    "\n",
    "    async def on_llm_end(\n",
    "            self,\n",
    "            response: LLMResult,\n",
    "            *,\n",
    "            run_id: UUID,\n",
    "            parent_run_id: Optional[UUID] = None,\n",
    "            tags: Optional[List[str]] = None,\n",
    "            **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "        overlap_tags = self.get_overlap_tags(tags)\n",
    "\n",
    "        if overlap_tags:\n",
    "            # Who can argue with beauty?\n",
    "            print()\n",
    "            print()\n",
    "\n",
    "    def get_overlap_tags(self, tags: Optional[List[str]]) -> List[str]:\n",
    "        \"\"\"Check for overlap with filtered tags.\"\"\"\n",
    "        if not tags:\n",
    "            return []\n",
    "        return sorted(set(tags or []) & set(self.tags_of_interest or []))\n",
    "\n",
    "    async def on_llm_new_token(\n",
    "            self,\n",
    "            token: str,\n",
    "            *,\n",
    "            chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,\n",
    "            run_id: UUID,\n",
    "            parent_run_id: Optional[UUID] = None,\n",
    "            tags: Optional[List[str]] = None,\n",
    "            **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "        overlap_tags = self.get_overlap_tags(tags)\n",
    "\n",
    "        if overlap_tags:\n",
    "            print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828cccd-889f-4c15-a9a9-58af9dfac869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks.manager import CallbackManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b0fafa-ce3b-489b-bf1d-d37b87f4819e",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "**Attention** For older versions of langchain, we must set `streaming=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa3c3761-a1cd-4118-8559-ea4d8857d394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e1a3b-2983-42d9-ac12-4a0f32cd4a24",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "We define two tools that rely on a chat model to generate output!\n",
    "\n",
    "Please note a few different things:\n",
    "\n",
    "1. We invoke the model using .stream() to force the output to stream (unfortunately for older langchain versions you should still set `streaming=True` on the model)\n",
    "2. We attach tags to the model so that we can filter on said tags in our callback handler\n",
    "3. The tools accept callbacks and propagate them to the model as a runtime argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c767f760-fe52-47e5-9c2a-622f03507aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def where_cat_is_hiding(callbacks: Callbacks) -> str:  # <--- Accept callbacks\n",
    "    \"\"\"Where is the cat hiding right now?\"\"\"\n",
    "    chunks = list(\n",
    "        model.stream(\n",
    "            \"Give one up to three word answer about where the cat might be hiding in the house right now.\",\n",
    "            {\"tags\": [\"hiding_spot\"], \"callbacks\": callbacks}, # <--- Propagate callbacks and assign a tag to this model\n",
    "        )\n",
    "    )\n",
    "    return \"\".join(chunk.content for chunk in chunks)\n",
    "\n",
    "\n",
    "@tool\n",
    "def tell_me_a_joke_about(topic: str, callbacks: Callbacks) -> str: # <--- Accept callbacks\n",
    "    \"\"\"Tell a joke about a given topic.\"\"\"\n",
    "    template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are Cat Agent 007. You are funny and know many jokes.\"),\n",
    "            (\"human\", \"Tell me a joke about {topic}\"),\n",
    "        ]\n",
    "    )\n",
    "    chain = template | model.with_config({\"tags\": [\"joke\"]})\n",
    "    chunks = list(chain.stream({\"topic\": topic}, {\"callbacks\": callbacks})) # <--- Propagate callbacks and assign a tag to this model\n",
    "    return \"\".join(chunk.content for chunk in chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba476f8-29da-4c2c-9134-186871caf7ae",
   "metadata": {},
   "source": [
    "## Initialize the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bab4488-bf4c-461f-b41e-5e60310fe0f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]\n"
     ]
    }
   ],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "print(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1762f4e1-402a-4bfb-af26-eb5b7b8f56bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = [tell_me_a_joke_about, where_cat_is_hiding]\n",
    "agent = agents.create_openai_tools_agent(\n",
    "    model.with_config({\"tags\": [\"agent\"]}), tools, prompt\n",
    ")\n",
    "executor = agents.AgentExecutor(agent=agent, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d94bd8-4a55-4527-b21a-4245a38c7c26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiding_spot: Under the bed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "handler = TokenByTokenHandler(tags_of_interest=[\"hiding_spot\", \"joke\"])\n",
    "\n",
    "result = executor.invoke(\n",
    "    {\"input\": \"where is the cat hiding?\"},\n",
    "    {\"callbacks\": [handler]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03ea798e-b62d-46cf-9bcf-81aa40903d34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiding_spot: Under the bed.\n",
      "\n",
      "joke: Why did the scarecrow bring a ladder under the bed?\n",
      "\n",
      "Because it heard there was a \"bed spring\" party going on!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "handler = TokenByTokenHandler(tags_of_interest=[\"hiding_spot\", \"joke\"])\n",
    "\n",
    "result = executor.invoke(\n",
    "    {\"input\": \"tell me a joke about the location where the cat is hiding\"},\n",
    "    {\"callbacks\": [handler]},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
