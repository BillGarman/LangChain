{
 "cells": [
  {
   "cell_type": "raw",
   "id": "473081cc",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ee4216",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "Streaming is an important UX consideration for LLM apps, and agents are no exception. Streaming with agents is made more complicated by the fact that it's not just tokens of the final answer that you will want to stream, but you may also want to stream back the intermediate steps an agent takes.\n",
    "\n",
    "Our agent will use the OpenAI tools API for tool invocation, and we'll provide the agent with two tools:\n",
    "\n",
    "1. `where_cat_is_hiding`: A tool that uses an LLM to tell us where the cat is hiding\n",
    "2. `get_items`: A tool that uses an LLM to determine which items are in a given place\n",
    "\n",
    "In this notebook, we'll see how to use `.stream` to stream action / observation pairs, and then we'll see how to use `.astream_log` to stream LLM output token by token, including from within the underlying tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40aae3d-b872-4e0f-ad54-8df6150fa863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import agents, hub\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools import tool\n",
    "from langchain_core.callbacks import Callbacks\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59502ed8-2f9f-4758-a0d5-90a0392ed33d",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "**Attention** \n",
    "\n",
    "* For older versions of langchain, we must set `streaming=True` on the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66e36d43-2c12-4cda-b591-383eb61b4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9c5e5-34d4-4208-9f78-7f9a1ff3029b",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "We define two tools that rely on a chat model to generate output!\n",
    "\n",
    "**Attention**\n",
    "\n",
    "1. We invoke the model using `astream()` to force the output to stream (unfortunately for older langchain versions you should still set `streaming=True` on the model). Do not use `.ainvoke` if you need token by token streaming.\n",
    "2. We attach tags to the model so that we can filter on said tags when using astream_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "742ed148-d54c-4398-bd25-e8ca8a2c8d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def where_cat_is_hiding(callbacks: Callbacks) -> str:  # <--- Accept callbacks\n",
    "    \"\"\"Where is the cat hiding right now?\"\"\"\n",
    "    # Attach name, tags and callbacks.\n",
    "    # Name\n",
    "    configured_model = model.with_config(\n",
    "        {\"run_name\": \"where_cat_at\", \"tags\": [\"tool_llm\"], \"callbacks\": callbacks}\n",
    "    )\n",
    "    chunks = [\n",
    "        chunk\n",
    "        async for chunk in configured_model.astream(\n",
    "            \"Think of a few places in a house where a cat likes to hangout. Then select ONE and only ONE of the places at random and write just its name.\",\n",
    "        )\n",
    "    ]\n",
    "    return \"\".join(chunk.content for chunk in chunks)\n",
    "\n",
    "\n",
    "@tool\n",
    "async def get_items(place: str, callbacks: Callbacks) -> str:  # <--- Accept callbacks\n",
    "    \"\"\"Use this tool to look up which items are in the given place.\"\"\"\n",
    "    template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Can you tell me what kind of items i might find in the following place: '{place}'. \"\n",
    "                \"List at least 3 such items separating them by a comma. And include a brief description of each item..\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    chain = template | model.with_config(\n",
    "        {\n",
    "            \"run_name\": \"Get Items LLM\",\n",
    "            \"tags\": [\"tool_llm\"],\n",
    "            \"callbacks\": callbacks,\n",
    "        }  # <-- Propagate callbacks\n",
    "    )\n",
    "    chunks = [chunk async for chunk in chain.astream({\"place\": place})]\n",
    "    return \"\".join(chunk.content for chunk in chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1257a508-c791-4d81-82d2-df021c560bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Window sill'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await where_cat_is_hiding.ainvoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea408ee-5260-418c-b769-5ba20e2999e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On a table, you might find a few common items such as:\\n\\n1. A book: A book is a written or printed work consisting of pages glued or sewn together along one side and bound in covers. It could be a novel, a textbook, or any other type of reading material.\\n\\n2. A coffee mug: A coffee mug is a cylindrical-shaped cup typically used for drinking hot beverages like coffee or tea. It usually has a handle for easy gripping and can be made of various materials such as ceramic, glass, or stainless steel.\\n\\n3. A vase with flowers: A vase is a decorative container, often made of glass or ceramic, used to hold flowers or other ornamental plants. It adds a touch of beauty and freshness to the surroundings, and the flowers inside can vary depending on personal preference or occasion.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await get_items.ainvoke({\"place\": \"on a a table\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c08cd5-34eb-41a7-b524-7c3d1d274a67",
   "metadata": {},
   "source": [
    "## Initialize the agent\n",
    "\n",
    "Here, we'll initialize an OpenAI tools agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adecca7a-9864-496d-a3a9-906b56ecd03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "# print(prompt.messages) -- to see the prompt\n",
    "tools = [get_items, where_cat_is_hiding]\n",
    "agent = agents.create_openai_tools_agent(\n",
    "    model.with_config({\"tags\": [\"agent_llm\"]}), tools, prompt\n",
    ")\n",
    "agent_executor = agents.AgentExecutor(agent=agent, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9a9eb",
   "metadata": {},
   "source": [
    "## Stream Intermediate Steps\n",
    "\n",
    "We'll use `.stream` method of the AgentExecutor to stream the agent's intermediate steps.\n",
    "\n",
    "The output from `.stream` alternates between (action, observation) pairs, finally concluding with the answer if the agent achieved its objective. \n",
    "\n",
    "It'll look like this:\n",
    "\n",
    "1. actions output\n",
    "2. observations output\n",
    "3. actions output\n",
    "4. observations output\n",
    "\n",
    "**... (continue until goal is reached) ...**\n",
    "\n",
    "Then, if the final goal is reached, the agent will output the **final answer**.\n",
    "\n",
    "\n",
    "The contents of these outputs are summarized here:\n",
    "\n",
    "| Output             | Contents                                                                                          |\n",
    "|----------------------|------------------------------------------------------------------------------------------------------|\n",
    "| **Actions**   |  <ul> <li> `actions` `AgentAction` or a subclass </li><li> `messages` chat messages corresponding to action invocation </li></ul> |\n",
    "| **Observations** | <ul> <li> `steps` History of what the agent did so far, including the current action and its observation </li><li> `messages` chat message with function invocation results (aka observations) </li></ul>|\n",
    "| **Final answer** | <ul> <li> `output` `AgentFinish`  </li><li> `messages` chat messages with the final output </li></ul>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eab4d4a0-55ed-407a-baf0-9f0eaf8c3518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "{'actions': [...], 'messages': [...]}\n",
      "------\n",
      "{'messages': [...], 'steps': [...]}\n",
      "------\n",
      "{'actions': [...], 'messages': [...]}\n",
      "------\n",
      "{'messages': [...], 'steps': [...]}\n",
      "------\n",
      "{'messages': [...],\n",
      " 'output': 'The items located where the cat is hiding on the window sill are:\\n'\n",
      "           '\\n'\n",
      "           '1. Potted plants\\n'\n",
      "           '2. Decorative figurines\\n'\n",
      "           '3. Sun catchers'}\n"
     ]
    }
   ],
   "source": [
    "# Note: We use `pprint` to print only to depth 1, it makes it easier to see the output from a high level, before digging in.\n",
    "import pprint\n",
    "\n",
    "chunks = []\n",
    "\n",
    "async for chunk in agent_executor.astream(\n",
    "    {\"input\": \"what's items are located where the cat is hiding?\"}\n",
    "):\n",
    "    chunks.append(chunk)\n",
    "    print(\"------\")\n",
    "    pprint.pprint(chunk, depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a930c7-7c6f-4602-b265-d38018f067be",
   "metadata": {},
   "source": [
    "### Using Messages\n",
    "\n",
    "You can access the underlying `messages` from the outputs. Using messages can be nice when working with chat applications - because everything is a message!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d5a3112-b2d4-488a-ac76-aa40dcec9cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OpenAIToolAgentAction(tool='where_cat_is_hiding', tool_input={}, log='\\nInvoking: `where_cat_is_hiding` with `{}`\\n\\n\\n', message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_HIvqZ1luv8TE7flNIWHKaHsZ', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})], tool_call_id='call_HIvqZ1luv8TE7flNIWHKaHsZ')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0][\"actions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f5eead3-f6f0-40b7-82c7-3b485c634e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_HIvqZ1luv8TE7flNIWHKaHsZ', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})]\n",
      "[FunctionMessage(content='Window sill', name='where_cat_is_hiding')]\n",
      "[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_8b8TvhnncfM0L5YnH6FCLqXL', 'function': {'arguments': '{\\n  \"place\": \"Window sill\"\\n}', 'name': 'get_items'}, 'type': 'function'}]})]\n",
      "[FunctionMessage(content='In a window sill, you might find:\\n\\n1. Potted plants: A window sill is a perfect spot for small potted plants, such as succulents or herbs. These plants bring a touch of nature indoors and add a pop of greenery to the window area.\\n\\n2. Decorative figurines: Many people like to display small decorative figurines on their window sills. These can be anything from ceramic animals to miniature sculptures, adding a personal touch and enhancing the aesthetic appeal of the window.\\n\\n3. Sun catchers: Sun catchers are often hung or placed on window sills to catch and reflect sunlight, creating beautiful patterns and colors in the room. They are typically made of glass or crystal and can add a whimsical or elegant touch to the window area.', name='get_items')]\n",
      "[AIMessage(content='The items located where the cat is hiding on the window sill are:\\n\\n1. Potted plants\\n2. Decorative figurines\\n3. Sun catchers')]\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1397f859-8595-488e-9857-c4e090a136d3",
   "metadata": {},
   "source": [
    "In addition, they contain full logging information (`actions` and `steps`) which may be easier to process for rendering purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd291a7",
   "metadata": {},
   "source": [
    "### Using AgentAction/Observation\n",
    "\n",
    "The outputs also contain richer structured information inside of `actions` and `steps`, which could be useful in some situations, but can also be harder to parse.\n",
    "\n",
    "**Attention** `AgentFinish` is not available as part of the `streaming` method. If this is something you'd like to be added, please start a discussion on github and explain the reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "603bff1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Tool: `where_cat_is_hiding` with input `{}`\n",
      "---\n",
      "Tool Result: `Window sill`\n",
      "---\n",
      "Calling Tool: `get_items` with input `{'place': 'Window sill'}`\n",
      "---\n",
      "Tool Result: `In a window sill, you might find:\n",
      "\n",
      "1. Potted plants: A window sill is a perfect spot for small potted plants, such as succulents or herbs. These plants bring a touch of nature indoors and add a pop of greenery to the window area.\n",
      "\n",
      "2. Decorative figurines: Many people like to display small decorative figurines on their window sills. These can be anything from ceramic animals to miniature sculptures, adding a personal touch and enhancing the aesthetic appeal of the window.\n",
      "\n",
      "3. Sun catchers: Sun catchers are often hung or placed on window sills to catch and reflect sunlight, creating beautiful patterns and colors in the room. They are typically made of glass or crystal and can add a whimsical or elegant touch to the window area.`\n",
      "---\n",
      "Final Output: The items located where the cat is hiding on the window sill are:\n",
      "\n",
      "1. Potted plants\n",
      "2. Decorative figurines\n",
      "3. Sun catchers\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "async for chunk in agent_executor.astream(\n",
    "    {\"input\": \"what's items are located where the cat is hiding?\"}\n",
    "):\n",
    "    # Agent Action\n",
    "    if \"actions\" in chunk:\n",
    "        for action in chunk[\"actions\"]:\n",
    "            print(f\"Calling Tool: `{action.tool}` with input `{action.tool_input}`\")\n",
    "    # Observation\n",
    "    elif \"steps\" in chunk:\n",
    "        for step in chunk[\"steps\"]:\n",
    "            print(f\"Tool Result: `{step.observation}`\")\n",
    "    # Final result\n",
    "    elif \"output\" in chunk:\n",
    "        print(f'Final Output: {chunk[\"output\"]}')\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be877d0c-df5f-4c6d-ba99-c9007598b79b",
   "metadata": {},
   "source": [
    "## Streaming Tokens & More\n",
    "\n",
    "For some applications, you may want to stream individual LLM tokens, surface information about tool execution, or output custom messages before / after tool executions.\n",
    "\n",
    "We will **only** stream tokens from LLMs used within tools by an agent and from no other LLMs (just to show that we can) 😮! Feel free to adapt this example to the needs of your application.\n",
    " \n",
    "There are different ways in which you might be able to achieve token streaming:\n",
    "\n",
    "1. `astream_events`: **beta** API, introduced in new langchain versions. This is the **recommended** approach.\n",
    "2. [astream_log](https://python.langchain.com/docs/expression_language/interface#async-stream-intermediate-steps) API: Produces a granular log of all events that occur during execution. The log format is based on the [JSONPatch](https://jsonpatch.com/) standard. It's granular, but requirs some effort to parse.\n",
    "3. `callbacks`: This can be useful if you're on older versions of LangChain and cannot upgrade. This is **NOT** recommended, as for most applications you'll need to set up a queue and send the callbacks to another worker (i.e., there's hidden complexity!). `astream_events` does this under the hood!\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "* Make sure that you set the LLM to `streaming=True`\n",
    "* Use async throughout (we will try to lift that restriction a bit, but for now if something isn't working use async!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46c59cac-25fa-4f42-8cf2-9bcaed6d92c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "Starting tool: where_cat_is_hiding with inputs: {}\n",
      "\n",
      "\n",
      "|Window| sill||\n",
      "\n",
      "Ended tool: where_cat_is_hiding\n",
      "--\n",
      "Starting tool: get_items with inputs: {'place': 'Window sill'}\n",
      "\n",
      "\n",
      "|In| a| window| sill|,| you| might| find|:\n",
      "\n",
      "|1|.| P|otted| plants|:| A| window| sill| is| a| perfect| spot| for| small| p|otted| plants|,| such| as| succ|ul|ents| or| herbs|.| These| plants| bring| a| touch| of| nature| indoors| and| add| a| pop| of| green|ery| to| the| window| area|.\n",
      "\n",
      "|2|.| Decor|ative| figur|ines|:| Many| people| like| to| display| small| decorative| figur|ines| on| their| window| s|ills|.| These| can| be| anything| from| ceramic| animals| to| miniature| sculptures|,| adding| a| personal| touch| and| enhancing| the| aesthetic| appeal| of| the| window|.\n",
      "\n",
      "|3|.| Sun| catch|ers|:| Sun| catch|ers| are| often| hung| or| placed| on| window| s|ills| to| catch| and| reflect| sunlight|,| creating| beautiful| patterns| and| colors| in| the| room|.| They| are| typically| made| of| glass| or| crystal| and| can| add| a| whims|ical| or| elegant| touch| to| the| window| area|.||\n",
      "\n",
      "Ended tool: get_items\n"
     ]
    }
   ],
   "source": [
    "async for event in agent_executor.astream_events(\n",
    "    {\"input\": \"where is the cat hiding? what items are in that location?\"}\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\" and \"tool_llm\" in event[\"tags\"]:\n",
    "        print(event[\"data\"][\"chunk\"].content, end=\"|\")\n",
    "    elif (\n",
    "        kind in {\"on_chat_model_start\", \"on_chat_model_end\"}\n",
    "        and \"tool_llm\" in event[\"tags\"]\n",
    "    ):\n",
    "        print()\n",
    "        print()\n",
    "    elif kind == \"on_tool_start\":\n",
    "        print(\"--\")\n",
    "        print(\n",
    "            f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\"\n",
    "        )\n",
    "    elif kind == \"on_tool_end\":\n",
    "        print(f\"Ended tool: {event['name']}\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d3251-3ab1-4e0e-8c90-cc6d1af1a985",
   "metadata": {},
   "source": [
    "### Using `astream_log` (Advanced)\n",
    "\n",
    "[astream_log](https://python.langchain.com/docs/expression_language/interface#async-stream-intermediate-steps) API: Produces a granular log of all events that occur during execution. The log format is based on the [JSONPatch](https://jsonpatch.com/) standard. It's granular so it has information, but it also requirs a bunch of effort to parse. (Please note that it is currently missing some information like \"inputs\" into the runnables.)\n",
    "\n",
    "We've included sample parsing code in this example. The parsing code can be simplified or made more complex. It can also rely more on JSONPatch standard to build up the actual JSON from the underlying patches. This is entirely up to you!\n",
    "\n",
    "Please keep in mind that `astream_events` does somthing similar under the hood for you, so if all you want is events, just use `astream_events` and enjoy life ☀️!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b748dbf-f444-4b50-8c2d-9a8958c618d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_log_patches = [\n",
    "    record_log_patch\n",
    "    async for record_log_patch in agent_executor.astream_log(\n",
    "        {\"input\": \"what's items are located where the cat is hiding?\"},\n",
    "        include_types=[\"tool\", \"llm\"],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d312d2-3699-4601-9d84-cce8782d2380",
   "metadata": {},
   "source": [
    "Below we're showing a few sample JSON patch operations. These json patch operations contain extremely granular information about all events that occurred during agent streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daeefa40-2a5a-478c-929a-2d1262a946ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunLogPatch({'op': 'add',\n",
       "  'path': '/streamed_output/-',\n",
       "  'value': {'actions': [OpenAIToolAgentAction(tool='where_cat_is_hiding', tool_input={}, log='\\nInvoking: `where_cat_is_hiding` with `{}`\\n\\n\\n', message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_CQhtjfttt3c3fXLxikbqKU4h', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})], tool_call_id='call_CQhtjfttt3c3fXLxikbqKU4h')],\n",
       "            'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_CQhtjfttt3c3fXLxikbqKU4h', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})]}},\n",
       " {'op': 'replace',\n",
       "  'path': '/final_output',\n",
       "  'value': {'actions': [OpenAIToolAgentAction(tool='where_cat_is_hiding', tool_input={}, log='\\nInvoking: `where_cat_is_hiding` with `{}`\\n\\n\\n', message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_CQhtjfttt3c3fXLxikbqKU4h', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})], tool_call_id='call_CQhtjfttt3c3fXLxikbqKU4h')],\n",
       "            'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_CQhtjfttt3c3fXLxikbqKU4h', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})]}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_log_patches[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5451b239-48d3-4962-8315-fb2e0cd3476d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunLogPatch({'op': 'add',\n",
       "  'path': '/logs/where_cat_at/streamed_output_str/-',\n",
       "  'value': 'Window'},\n",
       " {'op': 'add',\n",
       "  'path': '/logs/where_cat_at/streamed_output/-',\n",
       "  'value': AIMessageChunk(content='Window')})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_log_patches[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "481b1fd7-2d08-4f19-ab54-5af2b8f65348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, AsyncIterator, Dict, Literal, Optional, Union\n",
    "\n",
    "from langchain_core.tracers import RunLogPatch\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class StartEvent(TypedDict):\n",
    "    \"\"\"Represents a start event.\"\"\"\n",
    "\n",
    "    event_type: Literal[\"start\"]\n",
    "    start_time: float\n",
    "    type: Optional[str]  # e.g., llm, tool\n",
    "    name: Optional[str]  # the name of the llm or tool if assigned\n",
    "    tags: Optional[Dict[str, Any]]\n",
    "    metadata: Optional[Dict[str, Any]]\n",
    "\n",
    "\n",
    "class EndEvent(TypedDict):\n",
    "    \"\"\"End event.\"\"\"\n",
    "\n",
    "    event_type: Literal[\"end\"]\n",
    "    start_time: float\n",
    "    type: Optional[str]\n",
    "    name: Optional[str]\n",
    "    tags: Optional[Dict[str, Any]]\n",
    "    metadata: Optional[Dict[str, Any]]\n",
    "\n",
    "\n",
    "class StreamEvent(TypedDict):\n",
    "    \"\"\"Streaming event.\"\"\"\n",
    "\n",
    "    event_type: Literal[\"stream\"]\n",
    "    stream_type: Literal[\"str\", \"original\"]\n",
    "    type: Optional[str]\n",
    "    name: Optional[str]\n",
    "    tags: Optional[Dict[str, Any]]\n",
    "    metadata: Optional[Dict[str, Any]]\n",
    "\n",
    "\n",
    "Event = Union[StartEvent, EndEvent, StreamEvent]\n",
    "\n",
    "\n",
    "async def as_event_stream(\n",
    "    run_log_patches: AsyncIterator[RunLogPatch]\n",
    ") -> AsyncIterator[Event]:\n",
    "    \"\"\"Process log patches into a list of AIMessageChunks.\"\"\"\n",
    "    # Info keeps track of information like tags, metadata, type, name etc.\n",
    "    info: Dict[str, Any] = {}\n",
    "    async for run_log_patch in run_log_patches:\n",
    "        for op in run_log_patch.ops:\n",
    "            if op[\"op\"] != \"add\":\n",
    "                continue\n",
    "\n",
    "            path = op[\"path\"]\n",
    "\n",
    "            if not path.startswith(\"/logs/\"):\n",
    "                continue\n",
    "\n",
    "            path_in_logs = path[len(\"/logs/\") :]\n",
    "\n",
    "            components = path_in_logs.split(\"/\")\n",
    "\n",
    "            if len(components) == 1:\n",
    "                # It's a start event.\n",
    "                name = components[0]\n",
    "                value = op[\"value\"]\n",
    "                info[name] = {\n",
    "                    \"start_time\": value[\"start_time\"],\n",
    "                    \"type\": value.get(\"type\"),\n",
    "                    \"name\": value.get(\"name\"),\n",
    "                    \"tags\": value.get(\"tags\"),\n",
    "                    \"metadata\": value.get(\"metadata\"),\n",
    "                }\n",
    "\n",
    "                yield {\n",
    "                    \"event_type\": \"start\",\n",
    "                    **info[name],  # TODO(Eugene): We should make a copy here\n",
    "                }\n",
    "                continue\n",
    "            elif len(components) == 2:\n",
    "                name, kind = components\n",
    "                value = op[\"value\"]\n",
    "                if kind == \"final_output\":\n",
    "                    info[\"value\"] = value\n",
    "                    continue\n",
    "                elif kind == \"end_time\":\n",
    "                    yield {\n",
    "                        \"event_type\": \"end\",\n",
    "                        **info[name],  # TODO(Eugene): We should make a copy here\n",
    "                    }\n",
    "                    continue\n",
    "                else:\n",
    "                    raise ValueError(op)\n",
    "            elif len(components) == 3:\n",
    "                name, kind, remainder = components\n",
    "                if remainder != \"-\":\n",
    "                    raise AssertionError(components)\n",
    "                if kind == \"streamed_output_str\":\n",
    "                    value = op[\"value\"]\n",
    "                    yield {\n",
    "                        \"event_type\": \"stream\",\n",
    "                        \"stream_type\": \"str\",\n",
    "                        \"value\": value,\n",
    "                        **info[name],  # TODO(Eugene): We should make a copy here\n",
    "                    }\n",
    "                    continue\n",
    "                elif kind == \"streamed_output\":\n",
    "                    value = op[\"value\"]\n",
    "                    yield {\n",
    "                        \"event_type\": \"stream\",\n",
    "                        \"stream_type\": \"original\",\n",
    "                        \"value\": value,\n",
    "                        **info[name],  # TODO(Eugene): We should make a copy here\n",
    "                    }\n",
    "                    continue\n",
    "                else:\n",
    "                    raise NotImplementedError(\n",
    "                        f\"Parsing for op: `{op}` not implemented.\"\n",
    "                    )\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Parsing for op: `{op}` not implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784fb518-08d3-4687-b6f7-b6071ddb6df1",
   "metadata": {},
   "source": [
    "Let's materialize all events to make it convenient to work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de034014-9452-4186-8cc4-6aaa01692ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event_type': 'start',\n",
       "  'start_time': '2024-01-19T19:33:23.255+00:00',\n",
       "  'type': 'llm',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': ['seq:step:3', 'agent_llm'],\n",
       "  'metadata': {}},\n",
       " {'event_type': 'stream',\n",
       "  'stream_type': 'str',\n",
       "  'value': '',\n",
       "  'start_time': '2024-01-19T19:33:23.255+00:00',\n",
       "  'type': 'llm',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': ['seq:step:3', 'agent_llm'],\n",
       "  'metadata': {}},\n",
       " {'event_type': 'stream',\n",
       "  'stream_type': 'original',\n",
       "  'value': AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_aUJ9gaedIycr6JmOrWXT5IHP', 'function': {'arguments': '', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]}),\n",
       "  'start_time': '2024-01-19T19:33:23.255+00:00',\n",
       "  'type': 'llm',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': ['seq:step:3', 'agent_llm'],\n",
       "  'metadata': {}}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = [\n",
    "    event\n",
    "    async for event in as_event_stream(\n",
    "        agent_executor.astream_log(\n",
    "            {\"input\": \"what items are located where the cat is hiding?\"},\n",
    "            include_types=[\"tool\", \"llm\"],\n",
    "        )\n",
    "    )\n",
    "]\n",
    "events[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd953c8-767b-4c26-956f-f42b677849bb",
   "metadata": {},
   "source": [
    "Let's use the parsed astream_log information to print stream some information about what the agent and the tools are doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3b4a583-2141-43bb-a1f1-c8bf77d64084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start >> ChatOpenAI (llm), tags: agent_llm, seq:step:3\n",
      "\n",
      "End >> ChatOpenAI [llm]\n",
      "\n",
      "Start >> where_cat_is_hiding (tool), tags: []\n",
      "Start >> where_cat_at (llm), tags: tool_llm\n",
      "|Window| sill||\n",
      "End >> where_cat_at [llm]\n",
      "\n",
      "\n",
      "End >> where_cat_is_hiding [tool]\n",
      "\n",
      "Start >> ChatOpenAI (llm), tags: agent_llm, seq:step:3\n",
      "\n",
      "End >> ChatOpenAI [llm]\n",
      "\n",
      "Start >> get_items (tool), tags: []\n",
      "Start >> Get Items LLM (llm), tags: seq:step:2, tool_llm\n",
      "|In| a| window| sill|,| you| might| find|:\n",
      "\n",
      "|1|.| P|otted| plants|:| A| window| sill| is| a| perfect| spot| for| small| p|otted| plants|,| such| as| succ|ul|ents| or| herbs|.| These| plants| bring| a| touch| of| nature| indoors| and| add| a| pop| of| green|ery| to| the| window| area|.\n",
      "\n",
      "|2|.| Decor|ative| figur|ines|:| Many| people| like| to| display| small| decorative| figur|ines| on| their| window| s|ills|.| These| can| be| anything| from| ceramic| animals| to| miniature| sculptures|,| adding| a| personal| touch| and| enhancing| the| aesthetic| appeal| of| the| window|.\n",
      "\n",
      "|3|.| Sun| catch|ers|:| Sun| catch|ers| are| often| hung| or| placed| on| window| s|ills| to| catch| and| reflect| sunlight|,| creating| beautiful| patterns| and| colors| in| the| room|.| They| are| typically| made| of| glass| or| crystal| and| can| add| a| whims|ical| or| elegant| touch| to| the| window| area|.||\n",
      "End >> Get Items LLM [llm]\n",
      "\n",
      "\n",
      "End >> get_items [tool]\n",
      "\n",
      "Start >> ChatOpenAI (llm), tags: agent_llm, seq:step:3\n",
      "\n",
      "End >> ChatOpenAI [llm]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessageChunk\n",
    "\n",
    "async for event in as_event_stream(\n",
    "    agent_executor.astream_log(\n",
    "        {\"input\": \"what items are located where the cat is hiding?\"},\n",
    "        include_types=[\"tool\", \"llm\"],\n",
    "    )\n",
    "):\n",
    "    function_message = None\n",
    "    if event[\"event_type\"] == \"start\":\n",
    "        tags = \", \".join(sorted(event[\"tags\"]))\n",
    "        print(f\"Start >> {event['name']} ({event['type']}), tags: {tags or []}\")\n",
    "\n",
    "    if event[\"event_type\"] == \"stream\" and \"tool_llm\" in event[\"tags\"]:\n",
    "        value = event[\"value\"]\n",
    "\n",
    "        if event[\"stream_type\"] != \"original\":\n",
    "            continue\n",
    "\n",
    "        if not value:\n",
    "            continue\n",
    "        if isinstance(value, AIMessageChunk):\n",
    "            # print(event['time'])\n",
    "            print(value.content, end=\"|\")\n",
    "        else:\n",
    "            raise NotImplementedError(type(value))\n",
    "\n",
    "    if event[\"event_type\"] == \"end\":\n",
    "        print()\n",
    "        print(f\"End >> {event['name']} [{event['type']}]\")\n",
    "        print()\n",
    "        function_message = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee86df27-a035-4a1c-a1c9-a109152b35e5",
   "metadata": {},
   "source": [
    "### Using Callbacks (Legacy)\n",
    "\n",
    "This can be useful if you're still on older version of LangChain and cannot upgrade. \n",
    "\n",
    "However, this is **NOT** a recommended approach, as for most applications you'll need to send the callbacks to another worker that can stream them to the client (i.e., there's hidden complexity to actually make it work!). \n",
    "\n",
    "`astream_events` does this for you under the hood!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f0413d8-f246-4a99-a35f-43cb6959faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, TypeVar, Union\n",
    "from uuid import UUID\n",
    "\n",
    "from langchain_core.callbacks.base import AsyncCallbackHandler\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult\n",
    "\n",
    "\n",
    "# Here is a custom handler that will print the tokens to stdout.\n",
    "# Instead of printing to stdout you can send the data elsewhere; e.g., to a streaming API response\n",
    "class TokenByTokenHandler(AsyncCallbackHandler):\n",
    "    def __init__(self, tags_of_interest: List[str]) -> None:\n",
    "        \"\"\"A custom call back handler.\n",
    "\n",
    "        Args:\n",
    "            tags_of_interest: Only LLM tokens from models with these tags will be\n",
    "                              printed.\n",
    "        \"\"\"\n",
    "        self.tags_of_interest = tags_of_interest\n",
    "\n",
    "    async def on_chat_model_start(\n",
    "        self,\n",
    "        serialized: Dict[str, Any],\n",
    "        messages: List[List[BaseMessage]],\n",
    "        *,\n",
    "        run_id: UUID,\n",
    "        parent_run_id: Optional[UUID] = None,\n",
    "        tags: Optional[List[str]] = None,\n",
    "        metadata: Optional[Dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when a chat model starts running.\"\"\"\n",
    "        overlap_tags = self.get_overlap_tags(tags)\n",
    "\n",
    "        if overlap_tags:\n",
    "            print(\",\".join(overlap_tags), end=\": \", flush=True)\n",
    "\n",
    "    async def on_llm_end(\n",
    "        self,\n",
    "        response: LLMResult,\n",
    "        *,\n",
    "        run_id: UUID,\n",
    "        parent_run_id: Optional[UUID] = None,\n",
    "        tags: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "        overlap_tags = self.get_overlap_tags(tags)\n",
    "\n",
    "        if overlap_tags:\n",
    "            # Who can argue with beauty?\n",
    "            print()\n",
    "            print()\n",
    "\n",
    "    def get_overlap_tags(self, tags: Optional[List[str]]) -> List[str]:\n",
    "        \"\"\"Check for overlap with filtered tags.\"\"\"\n",
    "        if not tags:\n",
    "            return []\n",
    "        return sorted(set(tags or []) & set(self.tags_of_interest or []))\n",
    "\n",
    "    async def on_llm_new_token(\n",
    "        self,\n",
    "        token: str,\n",
    "        *,\n",
    "        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,\n",
    "        run_id: UUID,\n",
    "        parent_run_id: Optional[UUID] = None,\n",
    "        tags: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "        overlap_tags = self.get_overlap_tags(tags)\n",
    "\n",
    "        if overlap_tags:\n",
    "            print(token, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90ba2640-8d15-4f4f-a86e-44156967f316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_llm: |Window| sill||\n",
      "\n",
      "tool_llm: |In| a| window| sill|,| you| might| find|:\n",
      "\n",
      "|1|.| P|otted| plants|:| A| window| sill| is| a| perfect| spot| for| small| p|otted| plants|,| such| as| succ|ul|ents| or| herbs|.| These| plants| bring| a| touch| of| nature| indoors| and| add| a| pop| of| green|ery| to| the| window| area|.\n",
      "\n",
      "|2|.| Decor|ative| figur|ines|:| Many| people| like| to| display| small| decorative| figur|ines| on| their| window| s|ills|.| These| can| be| anything| from| ceramic| animals| to| miniature| sculptures|,| adding| a| personal| touch| and| enhancing| the| aesthetic| appeal| of| the| window|.\n",
      "\n",
      "|3|.| Sun| catch|ers|:| Sun| catch|ers| are| often| hung| or| placed| on| window| s|ills| to| catch| and| reflect| sunlight|,| creating| beautiful| patterns| and| colors| in| the| room|.| They| are| typically| made| of| glass| or| crystal| and| can| add| a| whims|ical| or| elegant| touch| to| the| window| area|.||\n",
      "\n"
     ]
    }
   ],
   "source": [
    "handler = TokenByTokenHandler(tags_of_interest=[\"tool_llm\"])\n",
    "\n",
    "result = await agent_executor.ainvoke(\n",
    "    {\"input\": \"where is the cat hiding and what items can be found there?\"},\n",
    "    {\"callbacks\": [handler]},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
