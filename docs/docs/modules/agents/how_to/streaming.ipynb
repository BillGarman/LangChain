{
 "cells": [
  {
   "cell_type": "raw",
   "id": "473081cc",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ee4216",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "Streaming is an important UX consideration for LLM apps, and agents are no exception. Streaming with agents is made more complicated by the fact that it's not just tokens of the final answer that you will want to stream, but you may also want to stream back the intermediate steps an agent takes.\n",
    "\n",
    "In this notebook, we'll cover the `stream/astream` and `astream_events` for streaming.\n",
    "\n",
    "Our agent will use a tools API for tool invocation with the tools:\n",
    "\n",
    "1. `where_cat_is_hiding`:  Returns a location where the cat is hiding\n",
    "2. `get_items`: Lists items that can be found in a particular place\n",
    "\n",
    "These tools will allow us to explore streaming in a more interesting situation where the agent will have to use both tools to answer some questions (e.g., to answer the question `what items are located where the cat is hiding?`).\n",
    "\n",
    "We've designed `get_items` to use an LLM as part of its implementation to show how to achieve **token by token** streaming even from code nested within tools.\n",
    "\n",
    "Ready?🏎️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d40aae3d-b872-4e0f-ad54-8df6150fa863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools import tool\n",
    "from langchain_core.callbacks import Callbacks\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59502ed8-2f9f-4758-a0d5-90a0392ed33d",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "**Attention** \n",
    "\n",
    "* For older versions of langchain, we must set `streaming=True` on the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e36d43-2c12-4cda-b591-383eb61b4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9c5e5-34d4-4208-9f78-7f9a1ff3029b",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "We define two tools that rely on a chat model to generate output!\n",
    "\n",
    "**Attention**\n",
    "\n",
    "1. We invoke the model using `astream()` to force the output to stream (unfortunately for older langchain versions you should still set `streaming=True` on the model). Do not use `.ainvoke` if you need token by token streaming.\n",
    "2. We attach `names` and `tags` to the models inside the tools. This will allow us to later filter based on either name or tags when using `astream_events`.\n",
    "3. Pay attention to the callback propagation! We have to propagate callbacks if we want our callback handlers (including the one that powers `astream_events`) to be passed to components. This is what allows the chat models to send token by token updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fc761f5-7619-4d4a-afbe-232f936d2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742ed148-d54c-4398-bd25-e8ca8a2c8d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def where_cat_is_hiding() -> str:\n",
    "    \"\"\"Where is the cat hiding right now?\"\"\"\n",
    "    return random.choice(\n",
    "        [\"under the bed\", \"on the shelf\", \"in the corner\", \"in the bathroom\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# And a tool that uses an LLM under the hood\n",
    "@tool\n",
    "async def get_items(place: str, callbacks: Callbacks) -> str:  # <--- Accept callbacks\n",
    "    \"\"\"Use this tool to look up which items are in the given place.\"\"\"\n",
    "    template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Can you tell me what kind of items i might find in the following place: '{place}'. \"\n",
    "                \"List at least 3 such items separating them by a comma. And include a brief description of each item..\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    chain = template | model.with_config(\n",
    "        {\n",
    "            \"run_name\": \"Get Items LLM\",\n",
    "            \"tags\": [\"tool_llm\"],\n",
    "            \"callbacks\": callbacks,\n",
    "        }  # <-- Propagate callbacks\n",
    "    )\n",
    "    chunks = [chunk async for chunk in chain.astream({\"place\": place})]\n",
    "    return \"\".join(chunk.content for chunk in chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1257a508-c791-4d81-82d2-df021c560bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'under the bed'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await where_cat_is_hiding.ainvoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eea408ee-5260-418c-b769-5ba20e2999e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On a table, you might find a few common items such as:\\n\\n1. A book: A book is a written or printed work consisting of pages glued or sewn together along one side and bound in covers. It could be a novel, a textbook, or any other type of reading material.\\n\\n2. A coffee mug: A coffee mug is a cylindrical-shaped cup typically used for drinking hot beverages like coffee or tea. It usually has a handle for easy gripping and can be made of various materials such as ceramic, glass, or stainless steel.\\n\\n3. A vase with flowers: A vase is a decorative container, often made of glass or ceramic, used to hold flowers or other ornamental plants. It adds a touch of beauty and freshness to the surroundings, and the flowers inside can vary depending on personal preference or occasion.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await get_items.ainvoke({\"place\": \"on a a table\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c08cd5-34eb-41a7-b524-7c3d1d274a67",
   "metadata": {},
   "source": [
    "## Initialize the agent\n",
    "\n",
    "Here, we'll initialize an OpenAI tools agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adecca7a-9864-496d-a3a9-906b56ecd03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "# print(prompt.messages) -- to see the prompt\n",
    "tools = [get_items, where_cat_is_hiding]\n",
    "agent = create_openai_tools_agent(\n",
    "    model.with_config({\"tags\": [\"agent_llm\"]}), tools, prompt\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools).with_config(\n",
    "    {\"run_name\": \"Agent\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9a9eb",
   "metadata": {},
   "source": [
    "## Stream Intermediate Steps\n",
    "\n",
    "We'll use `.stream` method of the AgentExecutor to stream the agent's intermediate steps.\n",
    "\n",
    "The output from `.stream` alternates between (action, observation) pairs, finally concluding with the answer if the agent achieved its objective. \n",
    "\n",
    "It'll look like this:\n",
    "\n",
    "1. actions output\n",
    "2. observations output\n",
    "3. actions output\n",
    "4. observations output\n",
    "\n",
    "**... (continue until goal is reached) ...**\n",
    "\n",
    "Then, if the final goal is reached, the agent will output the **final answer**.\n",
    "\n",
    "\n",
    "The contents of these outputs are summarized here:\n",
    "\n",
    "| Output             | Contents                                                                                          |\n",
    "|----------------------|------------------------------------------------------------------------------------------------------|\n",
    "| **Actions**   |  `actions` `AgentAction` or a subclass, `messages` chat messages corresponding to action invocation |\n",
    "| **Observations** |  `steps` History of what the agent did so far, including the current action and its observation, `messages` chat message with function invocation results (aka observations)|\n",
    "| **Final answer** | `output` `AgentFinish`, `messages` chat messages with the final output|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eab4d4a0-55ed-407a-baf0-9f0eaf8c3518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "{'actions': [...], 'messages': [...]}\n",
      "------\n",
      "{'messages': [...], 'steps': [...]}\n",
      "------\n",
      "{'actions': [...], 'messages': [...]}\n",
      "------\n",
      "{'messages': [...], 'steps': [...]}\n",
      "------\n",
      "{'messages': [...],\n",
      " 'output': 'The items located where the cat is hiding in the bathroom are:\\n'\n",
      "           '\\n'\n",
      "           '1. Toilet paper\\n'\n",
      "           '2. Toothbrush\\n'\n",
      "           '3. Soap'}\n"
     ]
    }
   ],
   "source": [
    "# Note: We use `pprint` to print only to depth 1, it makes it easier to see the output from a high level, before digging in.\n",
    "import pprint\n",
    "\n",
    "chunks = []\n",
    "\n",
    "async for chunk in agent_executor.astream(\n",
    "    {\"input\": \"what's items are located where the cat is hiding?\"}\n",
    "):\n",
    "    chunks.append(chunk)\n",
    "    print(\"------\")\n",
    "    pprint.pprint(chunk, depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a930c7-7c6f-4602-b265-d38018f067be",
   "metadata": {},
   "source": [
    "### Using Messages\n",
    "\n",
    "You can access the underlying `messages` from the outputs. Using messages can be nice when working with chat applications - because everything is a message!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d5a3112-b2d4-488a-ac76-aa40dcec9cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OpenAIToolAgentAction(tool='where_cat_is_hiding', tool_input={}, log='\\nInvoking: `where_cat_is_hiding` with `{}`\\n\\n\\n', message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_onVxOwNvzVYzzvk4nK6GjipD', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})], tool_call_id='call_onVxOwNvzVYzzvk4nK6GjipD')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0][\"actions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f5eead3-f6f0-40b7-82c7-3b485c634e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_onVxOwNvzVYzzvk4nK6GjipD', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})]\n",
      "[FunctionMessage(content='in the bathroom', name='where_cat_is_hiding')]\n",
      "[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_J9qR0NxVBuEECA4O9RC6KFEv', 'function': {'arguments': '{\\n  \"place\": \"bathroom\"\\n}', 'name': 'get_items'}, 'type': 'function'}]})]\n",
      "[FunctionMessage(content='In a bathroom, you might find the following items: \\n\\n1. Toilet paper: A roll of soft paper used for personal hygiene after using the toilet. It is typically placed on a holder or a wall-mounted dispenser.\\n\\n2. Toothbrush: A small handheld brush with bristles used for cleaning teeth. It usually has a handle and a head with bristles of varying lengths to reach different areas of the mouth.\\n\\n3. Soap: A cleansing agent used for washing hands and body. It comes in various forms such as bars, liquid, or foam and is typically scented or unscented. Soap helps remove dirt, bacteria, and other impurities from the skin.', name='get_items')]\n",
      "[AIMessage(content='The items located where the cat is hiding in the bathroom are:\\n\\n1. Toilet paper\\n2. Toothbrush\\n3. Soap')]\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1397f859-8595-488e-9857-c4e090a136d3",
   "metadata": {},
   "source": [
    "In addition, they contain full logging information (`actions` and `steps`) which may be easier to process for rendering purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd291a7",
   "metadata": {},
   "source": [
    "### Using AgentAction/Observation\n",
    "\n",
    "The outputs also contain richer structured information inside of `actions` and `steps`, which could be useful in some situations, but can also be harder to parse.\n",
    "\n",
    "**Attention** `AgentFinish` is not available as part of the `streaming` method. If this is something you'd like to be added, please start a discussion on github and explain why its needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "603bff1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Tool: `where_cat_is_hiding` with input `{}`\n",
      "---\n",
      "Tool Result: `in the bathroom`\n",
      "---\n",
      "Calling Tool: `get_items` with input `{'place': 'bathroom'}`\n",
      "---\n",
      "Tool Result: `In a bathroom, you might find the following items: \n",
      "\n",
      "1. Toilet paper: A soft, absorbent paper used for personal hygiene after using the toilet. It is typically found on a roll and is essential for maintaining cleanliness and comfort in the bathroom.\n",
      "\n",
      "2. Toothbrush: A small, handheld tool with bristles used for cleaning teeth. It is commonly used with toothpaste to remove plaque and maintain oral hygiene. Toothbrushes come in various sizes and designs, with bristles that are gentle on the gums.\n",
      "\n",
      "3. Soap: A cleansing agent used for washing hands and body. Soap comes in different forms such as bars, liquid, or foam. It helps to remove dirt, bacteria, and oils from the skin, leaving it clean and refreshed.`\n",
      "---\n",
      "Final Output: The items that are located where the cat is hiding in the bathroom are:\n",
      "\n",
      "1. Toilet paper\n",
      "2. Toothbrush\n",
      "3. Soap\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "async for chunk in agent_executor.astream(\n",
    "    {\"input\": \"what's items are located where the cat is hiding?\"}\n",
    "):\n",
    "    # Agent Action\n",
    "    if \"actions\" in chunk:\n",
    "        for action in chunk[\"actions\"]:\n",
    "            print(f\"Calling Tool: `{action.tool}` with input `{action.tool_input}`\")\n",
    "    # Observation\n",
    "    elif \"steps\" in chunk:\n",
    "        for step in chunk[\"steps\"]:\n",
    "            print(f\"Tool Result: `{step.observation}`\")\n",
    "    # Final result\n",
    "    elif \"output\" in chunk:\n",
    "        print(f'Final Output: {chunk[\"output\"]}')\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be877d0c-df5f-4c6d-ba99-c9007598b79b",
   "metadata": {},
   "source": [
    "## Custom Streaming\n",
    "\n",
    "The default implementation of `stream` may not be appropriate for some applications; e.g., if you want to stream individual tokens, or surfacing intermediate\n",
    "steps occuring **within** tools. \n",
    "\n",
    "Let's take a look at two different ways to customize what the agent is streaming.\n",
    "\n",
    "1. `astream_events`: **beta** API, introduced in new langchain versions. This is the **recommended** approach.\n",
    "2. `callbacks`: This can be useful if you're on older versions of LangChain and cannot upgrade. This is **NOT** recommended, as for most applications you'll need to set up a queue and send the callbacks to another worker (i.e., there's hidden complexity!). `astream_events` does this under the hood!\n",
    "\n",
    "**ATTENTION**\n",
    "\n",
    "* If you want to see *token by token* streaming, set the LLM to `streaming=True`\n",
    "* Use `async` code throughout (whether it's callbacks or tools etc) -- we will try to lift this restriction in the future\\\n",
    "\n",
    "**NOTE** You can also use the [astream_log](https://python.langchain.com/docs/expression_language/interface#async-stream-intermediate-steps) API. This API produces a granular log of all events that occur during execution. The log format is based on the [JSONPatch](https://jsonpatch.com/) standard. It's granular, but requires effort to parse. For this reason, we created the `astream_events` API instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ed6ab-9cec-4b66-ab8b-513e2af55888",
   "metadata": {},
   "source": [
    "### Using `astream_events` (Beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46c59cac-25fa-4f42-8cf2-9bcaed6d92c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting agent: Agent with input: {'input': 'where is the cat hiding? what items are in that location?'}\n",
      "--\n",
      "Starting tool: where_cat_is_hiding with inputs: {}\n",
      "Done tool: where_cat_is_hiding\n",
      "Tool output was: in the corner\n",
      "--\n",
      "--\n",
      "Starting tool: get_items with inputs: {'place': 'corner'}\n",
      "\n",
      "\n",
      "In| a| corner|,| you| might| find|:\n",
      "\n",
      "|1|.| Lamp|:| A| small| table| lamp| that| provides| soft| and| warm| lighting|.| It| is| usually| placed| on| a| corner| table| or| shelf|,| adding| both| functionality| and| ambiance| to| the| space|.\n",
      "\n",
      "|2|.| Book|shelf|:| A| tall| and| narrow| book|shelf| that| fits| perfectly| in| a| corner|.| It| is| designed| to| hold| books|,| magazines|,| or| decorative| items|,| providing| storage| and| displaying| opportunities| while| optimizing| the| use| of| space|.\n",
      "\n",
      "|3|.| Plant|:| A| p|otted| plant| placed| in| a| corner| can| bring| life| and| freshness| to| the| area|.| It| could| be| a| small| indoor| plant| or| a| larger| p|otted| tree|,| adding| a| touch| of| nature| and| enhancing| the| overall| aesthetic| of| the| room|.|\n",
      "\n",
      "Done tool: get_items\n",
      "Tool output was: In a corner, you might find:\n",
      "\n",
      "1. Lamp: A small table lamp that provides soft and warm lighting. It is usually placed on a corner table or shelf, adding both functionality and ambiance to the space.\n",
      "\n",
      "2. Bookshelf: A tall and narrow bookshelf that fits perfectly in a corner. It is designed to hold books, magazines, or decorative items, providing storage and displaying opportunities while optimizing the use of space.\n",
      "\n",
      "3. Plant: A potted plant placed in a corner can bring life and freshness to the area. It could be a small indoor plant or a larger potted tree, adding a touch of nature and enhancing the overall aesthetic of the room.\n",
      "--\n",
      "The| cat| is| hiding| in| the| corner|.| In| that| location|,| you| might| find| a| lamp|,| a| book|shelf|,| and| a| plant|.|\n",
      "--\n",
      "Done agent: Agent with output: The cat is hiding in the corner. In that location, you might find a lamp, a bookshelf, and a plant.\n"
     ]
    }
   ],
   "source": [
    "async for event in agent_executor.astream_events(\n",
    "    {\"input\": \"where is the cat hiding? what items are in that location?\"},\n",
    "    version=\"v1\",\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        # Empty content in the context of OpenAI means\n",
    "        # that the model is asking for a tool to be invoked.\n",
    "        if content:\n",
    "            print(content, end=\"|\")\n",
    "    elif (\n",
    "        kind in {\"on_chat_model_start\", \"on_chat_model_end\"}\n",
    "        and \"tool_llm\" in event[\"tags\"]\n",
    "    ):\n",
    "        print()\n",
    "        print()\n",
    "    elif kind == \"on_tool_start\":\n",
    "        print(\"--\")\n",
    "        print(\n",
    "            f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\"\n",
    "        )\n",
    "    elif kind == \"on_tool_end\":\n",
    "        print(f\"Done tool: {event['name']}\")\n",
    "        print(f\"Tool output was: {event['data'].get('output')}\")\n",
    "        print(\"--\")\n",
    "    elif kind == \"on_chain_start\":\n",
    "        if event[\"name\"] == \"Agent\":\n",
    "            print(\n",
    "                f\"Starting agent: {event['name']} with input: {event['data'].get('input')}\"\n",
    "            )\n",
    "    elif kind == \"on_chain_end\":\n",
    "        if event[\"name\"] == \"Agent\":\n",
    "            print()\n",
    "            print(\"--\")\n",
    "            print(\n",
    "                f\"Done agent: {event['name']} with output: {event['data'].get('output')['output']}\"\n",
    "            )\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee86df27-a035-4a1c-a1c9-a109152b35e5",
   "metadata": {},
   "source": [
    "### Using Callbacks (Legacy)\n",
    "\n",
    "Another approach to streaming is using callbacks. This may be useful if you're still on an older version of LangChain and cannot upgrade.\n",
    "\n",
    "Generall, this is **NOT** a recommended approach because:\n",
    "\n",
    "1. for most applications, you'll need to create two workers, write the callbacks to a queue and have another worker reading from the queue (i.e., there's hidden complexity to make this work).\n",
    "2. **end** events may be missing some metadata (e.g., like run name). So if you need the additional metadata, you should inherit from `BaseTracer` instead of `AsyncCallbackHandler` to pick up the relevant information from the runs (aka traces), or else implement the aggregation logic yourself based on the `run_id`.\n",
    "3. There is inconsistent behavior with the callbacks (e.g., how inputs and outputs are encoded) depending on the callback type that you'll need to workaround.\n",
    "\n",
    "For illustration purposes, we implement a callback below that shows how to get *token by token* streaming. Feel free to implement other callbacks based on your application needs.\n",
    "\n",
    "But `astream_events` does all of this you under the hood, so you don't have to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f0413d8-f246-4a99-a35f-43cb6959faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, TypeVar, Union\n",
    "from uuid import UUID\n",
    "\n",
    "from langchain_core.callbacks.base import AsyncCallbackHandler\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult\n",
    "\n",
    "\n",
    "# Here is a custom handler that will print the tokens to stdout.\n",
    "# Instead of printing to stdout you can send the data elsewhere; e.g., to a streaming API response\n",
    "class TokenByTokenHandler(AsyncCallbackHandler):\n",
    "    def __init__(self, tags_of_interest: List[str]) -> None:\n",
    "        \"\"\"A custom call back handler.\n",
    "\n",
    "        Args:\n",
    "            tags_of_interest: Only LLM tokens from models with these tags will be\n",
    "                              printed.\n",
    "        \"\"\"\n",
    "        self.tags_of_interest = tags_of_interest\n",
    "\n",
    "    async def on_chat_model_start(\n",
    "        self,\n",
    "        serialized: Dict[str, Any],\n",
    "        messages: List[List[BaseMessage]],\n",
    "        *,\n",
    "        run_id: UUID,\n",
    "        parent_run_id: Optional[UUID] = None,\n",
    "        tags: Optional[List[str]] = None,\n",
    "        metadata: Optional[Dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when a chat model starts running.\"\"\"\n",
    "        overlap_tags = self.get_overlap_tags(tags)\n",
    "\n",
    "        if overlap_tags:\n",
    "            print(\",\".join(overlap_tags), end=\": \", flush=True)\n",
    "\n",
    "    async def on_llm_end(\n",
    "        self,\n",
    "        response: LLMResult,\n",
    "        *,\n",
    "        run_id: UUID,\n",
    "        parent_run_id: Optional[UUID] = None,\n",
    "        tags: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "        overlap_tags = self.get_overlap_tags(tags)\n",
    "\n",
    "        if overlap_tags:\n",
    "            # Who can argue with beauty?\n",
    "            print()\n",
    "            print()\n",
    "\n",
    "    def get_overlap_tags(self, tags: Optional[List[str]]) -> List[str]:\n",
    "        \"\"\"Check for overlap with filtered tags.\"\"\"\n",
    "        if not tags:\n",
    "            return []\n",
    "        return sorted(set(tags or []) & set(self.tags_of_interest or []))\n",
    "\n",
    "    async def on_llm_new_token(\n",
    "        self,\n",
    "        token: str,\n",
    "        *,\n",
    "        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,\n",
    "        run_id: UUID,\n",
    "        parent_run_id: Optional[UUID] = None,\n",
    "        tags: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "        overlap_tags = self.get_overlap_tags(tags)\n",
    "\n",
    "        if token and overlap_tags:\n",
    "            print(token, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90ba2640-8d15-4f4f-a86e-44156967f316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_llm: \n",
      "\n",
      "agent_llm: \n",
      "\n",
      "tool_llm: In| a| bathroom|,| you| might| find| the| following| items|:| \n",
      "\n",
      "|1|.| Toilet| paper|:| A| soft|,| absorb|ent| paper| used| for| personal| hygiene| after| using| the| toilet|.| It| is| typically| found| on| a| roll| and| is| essential| for| maintaining| cleanliness| and| comfort| in| the| bathroom|.\n",
      "\n",
      "|2|.| Tooth|brush|:| A| small|,| handheld| tool| with| br|istles| used| for| cleaning| teeth|.| It| is| commonly| used| with| tooth|paste| to| remove| plaque| and| maintain| oral| hygiene|.| Tooth|brush|es| come| in| various| sizes| and| designs|,| and| are| usually| stored| near| the| sink| or| in| a| tooth|brush| holder|.\n",
      "\n",
      "|3|.| Soap|:| A| cleansing| agent| used| for| washing| hands| and| body|.| Soap| comes| in| different| forms| such| as| bars|,| liquid|,| or| foam|,| and| is| typically| used| with| water| to| create| l|ather|.| It| helps| remove| dirt|,| bacteria|,| and| other| imp|urities|,| leaving| the| skin| feeling| clean| and| refreshed|.| Soap| is| often| placed| near| the| sink| or| in| a| soap| dish|.|\n",
      "\n",
      "agent_llm: The| cat| is| hiding| in| the| bathroom|.| In| the| bathroom|,| you| can| find| items| such| as| toilet| paper|,| a| tooth|brush|,| and| soap|.|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "handler = TokenByTokenHandler(tags_of_interest=[\"tool_llm\", \"agent_llm\"])\n",
    "\n",
    "result = await agent_executor.ainvoke(\n",
    "    {\"input\": \"where is the cat hiding and what items can be found there?\"},\n",
    "    {\"callbacks\": [handler]},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
