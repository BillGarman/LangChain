{
 "cells": [
  {
   "cell_type": "raw",
   "id": "473081cc",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ee4216",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "Streaming is an important UX consideration for LLM apps, and agents are no exception. Streaming with agents is made more complicated by the fact that it's not just tokens of the final answer that you will want to stream, but you may also want to stream back the intermediate steps an agent takes.\n",
    "\n",
    "Our agent will use the OpenAI tools API for tool invocation, and we'll provide the agent with two tools:\n",
    "\n",
    "1. `where_cat_is_hiding`: A tool that uses an LLM to tell us where the cat is hiding\n",
    "2. `get_items`: A tool that uses an LLM to determine which items are in a given place\n",
    "\n",
    "In this notebook, we'll see how to use `.stream` to stream action / observation pairs, and then we'll see how to use `.astream_log` to stream LLM output token by token, including from within the underlying tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d40aae3d-b872-4e0f-ad54-8df6150fa863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, TypeVar, Union\n",
    "from uuid import UUID\n",
    "\n",
    "from langchain import agents, hub\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools import tool\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.callbacks import Callbacks\n",
    "from langchain_core.callbacks.base import AsyncCallbackHandler\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59502ed8-2f9f-4758-a0d5-90a0392ed33d",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "**Attention** For older versions of langchain, we must set `streaming=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66e36d43-2c12-4cda-b591-383eb61b4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0.0, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9c5e5-34d4-4208-9f78-7f9a1ff3029b",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "We define two tools that rely on a chat model to generate output!\n",
    "\n",
    "Please note a few different things:\n",
    "\n",
    "1. We invoke the model using .stream() to force the output to stream (unfortunately for older langchain versions you should still set `streaming=True` on the model)\n",
    "2. We attach tags to the model so that we can filter on said tags when using astream_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "742ed148-d54c-4398-bd25-e8ca8a2c8d2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ')' does not match opening parenthesis '{' (1547706171.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    configured_model = model.with_config{{'tags': ['hiding_spot'], 'run_name': \"where_cat_at\", \"callbacks\": callbacks})\u001b[0m\n\u001b[0m                                                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ')' does not match opening parenthesis '{'\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "async def where_cat_is_hiding(callbacks: Callbacks) -> str:  # <--- Accept callbacks\n",
    "    \"\"\"Where is the cat hiding right now?\"\"\"\n",
    "    # Attach name, tags and callbacks.\n",
    "    # Name\n",
    "    configured_model = model.with_config{{'tags': ['hiding_spot'], 'run_name': \"where_cat_at\", \"callbacks\": callbacks})\n",
    "    chunks = [\n",
    "        chunk\n",
    "        async for chunk in configured_model.astream(\n",
    "            \"Make up a place in the house where the cat might be hiding in the house right now. Include the name of the place only.\",\n",
    "        )\n",
    "    ]\n",
    "    return \"\".join(chunk.content for chunk in chunks)\n",
    "\n",
    "\n",
    "@tool\n",
    "async def get_items(place: str, callbacks: Callbacks) -> str:  # <--- Accept callbacks\n",
    "    \"\"\"Use this tool to look up which items are in the given place.\"\"\"\n",
    "    template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Can you tell me what kind of items i might find in the following place: '{place}'. \"\n",
    "                \"List at least 3 such items separating them by a comma. And include a brief description of each item..\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    chain = template | model.with_config(\n",
    "        {\"tags\": [\"get_items\"], \"run_name\": \"Get Items LLM\",  \"callbacks\": callbacks} # <-- Propagate callbacks\n",
    "    )\n",
    "    chunks = [\n",
    "        chunk\n",
    "        async for chunk in chain.astream({\"place\": place})\n",
    "    ]\n",
    "    return \"\".join(chunk.content for chunk in chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7561299-9d01-4dac-88ce-0e71312ef7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Fuzzy Fortress'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await where_cat_is_hiding.ainvoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea408ee-5260-418c-b769-5ba20e2999e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On a table, you might find a few common items such as:\\n\\n1. A book: A book is a written or printed work consisting of pages glued or sewn together along one side and bound in covers. It could be a novel, a textbook, or any other type of reading material.\\n\\n2. A coffee mug: A coffee mug is a cylindrical-shaped cup typically used for drinking hot beverages like coffee or tea. It usually has a handle for easy gripping and can be made of various materials such as ceramic, glass, or stainless steel.\\n\\n3. A vase with flowers: A vase is a decorative container, often made of glass or ceramic, used to hold flowers or other ornamental plants. It adds a touch of beauty and freshness to the surroundings, and the flowers inside can vary depending on personal preference or occasion.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await get_items.ainvoke({\"place\": \"on a a table\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c28f0056-bd70-4bb4-bfbc-fae239c37f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In 'The Mysterious Laundry Basket', you might find the following items:\\n\\n1. A tattered diary: This diary appears to be old and worn, with its pages yellowed and edges frayed. It contains cryptic writings, sketches, and mysterious symbols, hinting at hidden secrets and forgotten stories.\\n\\n2. A silver locket: This delicate locket is intricately designed with ornate patterns and engravings. It is locked shut, and its contents remain a mystery. The locket holds an air of nostalgia and whispers of forgotten memories.\\n\\n3. A faded map: This map seems to have been through many hands and countless adventures. Its edges are worn, and some areas are faded, making it difficult to decipher. It leads to an unknown destination, promising untold treasures or perhaps a hidden realm waiting to be discovered.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await get_items.ainvoke({\"place\": \"The Mysterious Laundry Basket\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c08cd5-34eb-41a7-b524-7c3d1d274a67",
   "metadata": {},
   "source": [
    "## Initialize the agent\n",
    "\n",
    "Here, we'll initialize an OpenAI tools agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adecca7a-9864-496d-a3a9-906b56ecd03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "# print(prompt.messages) -- to see the prompt\n",
    "tools = [get_items, where_cat_is_hiding]\n",
    "agent = agents.create_openai_tools_agent(\n",
    "    model.with_config({\"tags\": [\"agent_llm\"]}), tools, prompt\n",
    ")\n",
    "agent_executor = agents.AgentExecutor(agent=agent, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9a9eb",
   "metadata": {},
   "source": [
    "## Stream Intermediate Steps\n",
    "\n",
    "We'll use `.stream` method of the AgentExecutor to stream the agent's intermediate steps.\n",
    "\n",
    "The output from `.stream` alternates between (action, observation) pairs, finally concluding with the answer if the agent achieved its objective. \n",
    "\n",
    "It'll look like this:\n",
    "\n",
    "1. actions output\n",
    "2. observations output\n",
    "3. actions output\n",
    "4. observations output\n",
    "\n",
    "**... (continue until goal is reached) ...**\n",
    "\n",
    "Then, if the final goal is reached, the agent will output the **final answer**.\n",
    "\n",
    "\n",
    "The contents of these outputs are summarized here:\n",
    "\n",
    "| Output             | Contents                                                                                          |\n",
    "|----------------------|------------------------------------------------------------------------------------------------------|\n",
    "| **Actions**   |  <ul> <li> `actions` `AgentAction` or a subclass </li><li> `messages` chat messages corresponding to action invocation </li></ul> |\n",
    "| **Observations** | <ul> <li> `steps` History of what the agent did so far, including the current action and its observation </li><li> `messages` chat message with function invocation results (aka observations) </li></ul>|\n",
    "| **Final answer** | <ul> <li> `output` `AgentFinish`  </li><li> `messages` chat messages with the final output </li></ul>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eab4d4a0-55ed-407a-baf0-9f0eaf8c3518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "{'actions': [...], 'messages': [...]}\n",
      "------\n",
      "{'messages': [...], 'steps': [...]}\n",
      "------\n",
      "{'actions': [...], 'messages': [...]}\n",
      "------\n",
      "{'messages': [...], 'steps': [...]}\n",
      "------\n",
      "{'messages': [...],\n",
      " 'output': 'The items located where the cat is hiding in \"The Fuzzy Fortress\" '\n",
      "           'are:\\n'\n",
      "           '\\n'\n",
      "           '1. Fluffy Slippers\\n'\n",
      "           '2. Fuzzy Blankets\\n'\n",
      "           '3. Furry Pillows'}\n"
     ]
    }
   ],
   "source": [
    "# Note: We use `pprint` to print only to depth 1, it makes it easier to see the output from a high level, before digging in.\n",
    "import pprint\n",
    "\n",
    "chunks = []\n",
    "\n",
    "async for chunk in agent_executor.astream(\n",
    "    {\"input\": \"what's items are located where the cat is hiding?\"}\n",
    "):\n",
    "    chunks.append(chunk)\n",
    "    print(\"------\")\n",
    "    pprint.pprint(chunk, depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a930c7-7c6f-4602-b265-d38018f067be",
   "metadata": {},
   "source": [
    "### Using Messages\n",
    "\n",
    "You can access the underlying `messages` from the outputs. Using messages can be nice when working with chat applications - because everything is a message!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d5a3112-b2d4-488a-ac76-aa40dcec9cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OpenAIToolAgentAction(tool='where_cat_is_hiding', tool_input={}, log='\\nInvoking: `where_cat_is_hiding` with `{}`\\n\\n\\n', message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_5OKVxVMSBDWzHsXACHozZqUo', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})], tool_call_id='call_5OKVxVMSBDWzHsXACHozZqUo')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0][\"actions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f5eead3-f6f0-40b7-82c7-3b485c634e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_5OKVxVMSBDWzHsXACHozZqUo', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})]\n",
      "[FunctionMessage(content='The Fuzzy Fortress', name='where_cat_is_hiding')]\n",
      "[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_sxYLixBFuB9mfIH3CiQlF2q5', 'function': {'arguments': '{\\n  \"place\": \"The Fuzzy Fortress\"\\n}', 'name': 'get_items'}, 'type': 'function'}]})]\n",
      "[FunctionMessage(content=\"In 'The Fuzzy Fortress', you might find the following items:\\n\\n1. Fluffy Slippers: These cozy slippers are made of soft, plush material, providing ultimate comfort for your feet. They come in various vibrant colors and have adorable fuzzy animal faces on the front.\\n\\n2. Fuzzy Blankets: These luxurious blankets are crafted from high-quality fleece, ensuring warmth and snuggliness. They are available in different sizes and patterns, featuring cute fuzzy animals or colorful abstract designs.\\n\\n3. Furry Pillows: These fluffy pillows are perfect for adding a touch of comfort and style to any space. They are covered in ultra-soft faux fur, making them irresistibly huggable. The pillows come in various shapes and sizes, from round to square, and are available in a range of vibrant or neutral colors.\", name='get_items')]\n",
      "[AIMessage(content='The items located where the cat is hiding in \"The Fuzzy Fortress\" are:\\n\\n1. Fluffy Slippers\\n2. Fuzzy Blankets\\n3. Furry Pillows')]\n",
      "[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_Z1HU1iFLNpJFMXIL2lucK9vh', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})]\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1397f859-8595-488e-9857-c4e090a136d3",
   "metadata": {},
   "source": [
    "In addition, they contain full logging information (`actions` and `steps`) which may be easier to process for rendering purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd291a7",
   "metadata": {},
   "source": [
    "### Using AgentAction/Observation\n",
    "\n",
    "The outputs also contain richer structured information inside of `actions` and `steps`, which could be useful in some situations, but can also be harder to parse.\n",
    "\n",
    "**Attention** `AgentFinish` is not available as part of the `streaming` method. If this is something you'd like to be added, please start a discussion on github and explain the reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "603bff1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Tool: `where_cat_is_hiding` with input `{}`\n",
      "---\n",
      "Tool Result: `The Cozy Cavern`\n",
      "---\n",
      "Calling Tool: `get_items` with input `{'place': 'Cozy Cavern'}`\n",
      "---\n",
      "Tool Result: `In the \"Cozy Cavern,\" you might find:\n",
      "\n",
      "1. Plush velvet cushions: Soft and luxurious, these cushions are perfect for sinking into and getting comfortable. They come in various colors, providing a cozy and inviting atmosphere in the cavern.\n",
      "2. Rustic fireplace: A charming fireplace made of stone or brick, with a crackling fire warming the space. It adds both warmth and a rustic ambiance to the cavern, making it an ideal spot for relaxation and gathering.\n",
      "3. Twinkling fairy lights: Delicate and enchanting, these fairy lights are strung across the cavern's ceiling, creating a magical and whimsical atmosphere. Their soft glow adds a touch of warmth and beauty to the space, making it feel even cozier.`\n",
      "---\n",
      "Final Output: The items located where the cat is hiding in the Cozy Cavern are:\n",
      "\n",
      "1. Plush velvet cushions\n",
      "2. Rustic fireplace\n",
      "3. Twinkling fairy lights\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "async for chunk in agent_executor.astream(\n",
    "    {\"input\": \"what's items are located where the cat is hiding?\"}\n",
    "):\n",
    "    # Agent Action\n",
    "    if \"actions\" in chunk:\n",
    "        for action in chunk[\"actions\"]:\n",
    "            print(f\"Calling Tool: `{action.tool}` with input `{action.tool_input}`\")\n",
    "    # Observation\n",
    "    elif \"steps\" in chunk:\n",
    "        for step in chunk[\"steps\"]:\n",
    "            print(f\"Tool Result: `{step.observation}`\")\n",
    "    # Final result\n",
    "    elif \"output\" in chunk:\n",
    "        print(f'Final Output: {chunk[\"output\"]}')\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be877d0c-df5f-4c6d-ba99-c9007598b79b",
   "metadata": {},
   "source": [
    "## Streaming Tokens & More\n",
    "\n",
    "For some applications, you may want to stream individual LLM tokens, surface information about tool execution, or output custom messages before / after tool executions.\n",
    "\n",
    "There are different ways in which you might be able to achieve token streaming:\n",
    "\n",
    "1. `astream_events`: **beta** API, introduced in new langchain versions. This is the **recommended** approach.\n",
    "2. [astream_log](https://python.langchain.com/docs/expression_language/interface#async-stream-intermediate-steps) API: Produces a granular log of all events that occur during execution. The log format is based on the [JSONPatch](https://jsonpatch.com/) standard. It's granular, but requirs some effort to parse.\n",
    "3. `callbacks`: This can be useful if you're on older versions of LangChain and cannot upgrade. This is **NOT** recommended, as for most applications you'll need to set up a queue and send the callbacks to another worker (i.e., there's hidden complexity!). `astream_events` does this under the hood!\n",
    "\n",
    "**ATTENTION** \n",
    "* Make sure that you set the LLM to `streaming=True`\n",
    "* Use async throughout (we will try to lift that restriction a bit, but for now if something isn't working use async!)\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d10d12-b436-4a32-b644-490c3af5615a",
   "metadata": {},
   "source": [
    "### Event Streaming\n",
    "\n",
    "**NEW** This is a new API only works with recent versions of langchain-core!\n",
    "\n",
    "In this notebook, we'll see how to use `astream_events` to stream **token by token** from LLM calls used within the tools invoked by the agent. \n",
    "\n",
    "We will **only** stream tokens from LLMs used within tools and from no other LLMs (just to show that we can)! \n",
    "\n",
    "Feel free to adapt this example to the needs of your application.\n",
    "\n",
    "Our agent will use the OpenAI tools API for tool invocation, and we'll provide the agent with two tools:\n",
    "\n",
    "1. `where_cat_is_hiding`: A tool that uses an LLM to tell us where the cat is hiding\n",
    "2. `tell_me_a_joke_about`: A tool that can use an LLM to tell a joke about the given topic\n",
    "\n",
    "⚠️ Beta ⚠️\n",
    "\n",
    "Event Streaming is a **beta** API, and may change a bit based on feedback.\n",
    "\n",
    "Keep in mind the following constraints (repeated in tools section):\n",
    "\n",
    "* streaming only works properly if using `async`\n",
    "* propagate callbacks if definning custom functions / runnables\n",
    "* If creating a tool that uses an LLM, make sure to use `.astream()` on the LLM rather than `.ainvoke` to ask the LLM to stream tokens.\n",
    "\n",
    "#### Evens Reference\n",
    "\n",
    "\n",
    "Here is a reference table that shows some events that might be emitted by the various Runnable objects.\n",
    "Definitions for some of the Runnable are included after the table.\n",
    "\n",
    "⚠️ When streaming the inputs for the runnable will not be available until the input stream has been entirely consumed This means that the inputs will be available at for the corresponding `end` hook rather than `start` event.\n",
    "\n",
    "\n",
    "| event                | name             | chunk                           | input                                         | output                                          |\n",
    "|----------------------|------------------|---------------------------------|-----------------------------------------------|-------------------------------------------------|\n",
    "| on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
    "| on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
    "| on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | {\"generations\": [...], \"llm_output\": None, ...} |\n",
    "| on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
    "| on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
    "| on_llm_end           | [model name]     |                                 | 'Hello human!'                                |\n",
    "| on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
    "| on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
    "| on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
    "| on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
    "| on_tool_stream       | some_tool        | {\"x\": 1, \"y\": \"2\"}              |                                               |                                                 |\n",
    "| on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
    "| on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
    "| on_retriever_chunk   | [retriever name] | {documents: [...]}              |                                               |                                                 |\n",
    "| on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | {documents: [...]}                              |\n",
    "| on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
    "| on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
    "\n",
    "\n",
    "Here are declarations associated with the events shown above:\n",
    "\n",
    "`format_docs`:\n",
    "\n",
    "```python\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    '''Format the docs.'''\n",
    "    return \", \".join([doc.page_content for doc in docs])\n",
    "\n",
    "format_docs = RunnableLambda(format_docs)\n",
    "```\n",
    "\n",
    "`some_tool`:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def some_tool(x: int, y: str) -> dict:\n",
    "    '''Some_tool.'''\n",
    "    return {\"x\": x, \"y\": y}\n",
    "```\n",
    "\n",
    "`prompt`:\n",
    "\n",
    "```python\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
    ").with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b748dbf-f444-4b50-8c2d-9a8958c618d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_log_patches = [\n",
    "    record_log_patch\n",
    "    async for record_log_patch in agent_executor.astream_log(\n",
    "        {\"input\": \"what's items are located where the cat is hiding?\"},\n",
    "        include_types=[\"tool\", \"llm\"],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d312d2-3699-4601-9d84-cce8782d2380",
   "metadata": {},
   "source": [
    "Below we're showing a few sample JSON patch operations. These json patch operations contain extremely granular information about all events that occurred during agent streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daeefa40-2a5a-478c-929a-2d1262a946ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunLogPatch({'op': 'add',\n",
       "  'path': '/streamed_output/-',\n",
       "  'value': {'actions': [OpenAIToolAgentAction(tool='where_cat_is_hiding', tool_input={}, log='\\nInvoking: `where_cat_is_hiding` with `{}`\\n\\n\\n', message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_7j1dxpnUzJ1OUQrGaJPqtZes', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})], tool_call_id='call_7j1dxpnUzJ1OUQrGaJPqtZes')],\n",
       "            'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_7j1dxpnUzJ1OUQrGaJPqtZes', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})]}},\n",
       " {'op': 'replace',\n",
       "  'path': '/final_output',\n",
       "  'value': {'actions': [OpenAIToolAgentAction(tool='where_cat_is_hiding', tool_input={}, log='\\nInvoking: `where_cat_is_hiding` with `{}`\\n\\n\\n', message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_7j1dxpnUzJ1OUQrGaJPqtZes', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})], tool_call_id='call_7j1dxpnUzJ1OUQrGaJPqtZes')],\n",
       "            'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_7j1dxpnUzJ1OUQrGaJPqtZes', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})]}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_log_patches[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5451b239-48d3-4962-8315-fb2e0cd3476d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunLogPatch({'op': 'add',\n",
       "  'path': '/logs/ChatOpenAI:2/streamed_output_str/-',\n",
       "  'value': 'The'},\n",
       " {'op': 'add',\n",
       "  'path': '/logs/ChatOpenAI:2/streamed_output/-',\n",
       "  'value': AIMessageChunk(content='The')})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_log_patches[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fa4dc5-3f62-4714-a594-ee271b638b82",
   "metadata": {},
   "source": [
    "### Parsing astream_log\n",
    "\n",
    "As of 2024-01-12, `LangChain` currently does provide utility code to work with astream_log, but will be introduced in the near future.\n",
    "\n",
    "In the menatime, to make it easier to use `astream_log`, we've included sample parsing code is included below.\n",
    "\n",
    "If you're reading this in a month in the future, please check LangChain documentation about astream_log to see if stable helpers have been added to the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "481b1fd7-2d08-4f19-ab54-5af2b8f65348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, AsyncIterator, Dict, Literal, Optional\n",
    "\n",
    "from langchain_core.tracers import RunLogPatch\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class StartEvent(TypedDict):\n",
    "    \"\"\"Represents a start event.\"\"\"\n",
    "\n",
    "    event_type: Literal[\"start\"]\n",
    "    start_time: float\n",
    "    type: Optional[str]  # e.g., llm, tool\n",
    "    name: Optional[str]  # the name of the llm or tool if assigned\n",
    "    tags: Optional[Dict[str, Any]]\n",
    "    metadata: Optional[Dict[str, Any]]\n",
    "\n",
    "\n",
    "class EndEvent(TypedDict):\n",
    "    \"\"\"End event.\"\"\"\n",
    "\n",
    "    event_type: Literal[\"end\"]\n",
    "    start_time: float\n",
    "    type: Optional[str]\n",
    "    name: Optional[str]\n",
    "    tags: Optional[Dict[str, Any]]\n",
    "    metadata: Optional[Dict[str, Any]]\n",
    "\n",
    "\n",
    "class StreamEvent(TypedDict):\n",
    "    \"\"\"Streaming event.\"\"\"\n",
    "\n",
    "    event_type: Literal[\"stream\"]\n",
    "    stream_type: Literal[\"str\", \"original\"]\n",
    "    type: Optional[str]\n",
    "    name: Optional[str]\n",
    "    tags: Optional[Dict[str, Any]]\n",
    "    metadata: Optional[Dict[str, Any]]\n",
    "\n",
    "\n",
    "Event = Union[StartEvent, EndEvent, StreamEvent]\n",
    "\n",
    "\n",
    "async def as_event_stream(\n",
    "    run_log_patches: AsyncIterator[RunLogPatch]\n",
    ") -> AsyncIterator[Event]:\n",
    "    \"\"\"Process log patches into a list of AIMessageChunks.\"\"\"\n",
    "    # Info keeps track of information like tags, metadata, type, name etc.\n",
    "    info: Dict[str, Any] = {}\n",
    "    async for run_log_patch in run_log_patches:\n",
    "        for op in run_log_patch.ops:\n",
    "            if op[\"op\"] != \"add\":\n",
    "                continue\n",
    "\n",
    "            path = op[\"path\"]\n",
    "\n",
    "            if not path.startswith(\"/logs/\"):\n",
    "                continue\n",
    "\n",
    "            path_in_logs = path[len(\"/logs/\") :]\n",
    "\n",
    "            components = path_in_logs.split(\"/\")\n",
    "\n",
    "            if len(components) == 1:\n",
    "                # It's a start event.\n",
    "                name = components[0]\n",
    "                value = op[\"value\"]\n",
    "                info[name] = {\n",
    "                    \"start_time\": value[\"start_time\"],\n",
    "                    \"type\": value.get(\"type\"),\n",
    "                    \"name\": value.get(\"name\"),\n",
    "                    \"tags\": value.get(\"tags\"),\n",
    "                    \"metadata\": value.get(\"metadata\"),\n",
    "                }\n",
    "\n",
    "                yield {\n",
    "                    \"event_type\": \"start\",\n",
    "                    **info[name],  # TODO(Eugene): We should make a copy here\n",
    "                }\n",
    "                continue\n",
    "            elif len(components) == 2:\n",
    "                name, kind = components\n",
    "                value = op[\"value\"]\n",
    "                if kind == \"final_output\":\n",
    "                    info[\"value\"] = value\n",
    "                    continue\n",
    "                elif kind == \"end_time\":\n",
    "                    yield {\n",
    "                        \"event_type\": \"end\",\n",
    "                        **info[name],  # TODO(Eugene): We should make a copy here\n",
    "                    }\n",
    "                    continue\n",
    "                else:\n",
    "                    raise ValueError(op)\n",
    "            elif len(components) == 3:\n",
    "                name, kind, remainder = components\n",
    "                if remainder != \"-\":\n",
    "                    raise AssertionError(components)\n",
    "                if kind == \"streamed_output_str\":\n",
    "                    value = op[\"value\"]\n",
    "                    yield {\n",
    "                        \"event_type\": \"stream\",\n",
    "                        \"stream_type\": \"str\",\n",
    "                        \"value\": value,\n",
    "                        **info[name],  # TODO(Eugene): We should make a copy here\n",
    "                    }\n",
    "                    continue\n",
    "                elif kind == \"streamed_output\":\n",
    "                    value = op[\"value\"]\n",
    "                    yield {\n",
    "                        \"event_type\": \"stream\",\n",
    "                        \"stream_type\": \"original\",\n",
    "                        \"value\": value,\n",
    "                        **info[name],  # TODO(Eugene): We should make a copy here\n",
    "                    }\n",
    "                    continue\n",
    "                else:\n",
    "                    raise NotImplementedError(\n",
    "                        f\"Parsing for op: `{op}` not implemented.\"\n",
    "                    )\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Parsing for op: `{op}` not implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de034014-9452-4186-8cc4-6aaa01692ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event_type': 'start',\n",
       "  'start_time': '2024-01-12T16:21:21.480+00:00',\n",
       "  'type': 'llm',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': ['seq:step:3', 'agent_llm'],\n",
       "  'metadata': {}},\n",
       " {'event_type': 'stream',\n",
       "  'stream_type': 'str',\n",
       "  'value': '',\n",
       "  'start_time': '2024-01-12T16:21:21.480+00:00',\n",
       "  'type': 'llm',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': ['seq:step:3', 'agent_llm'],\n",
       "  'metadata': {}},\n",
       " {'event_type': 'stream',\n",
       "  'stream_type': 'original',\n",
       "  'value': AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_iwOW9rEwf6OtINFRCtw9hrd3', 'function': {'arguments': '', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]}),\n",
       "  'start_time': '2024-01-12T16:21:21.480+00:00',\n",
       "  'type': 'llm',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': ['seq:step:3', 'agent_llm'],\n",
       "  'metadata': {}}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's materialize all events to make it convenient to work with them.\n",
    "events = [\n",
    "    event\n",
    "    async for event in as_event_stream(\n",
    "        agent_executor.astream_log(\n",
    "            {\"input\": \"what items are located where the cat is hiding?\"},\n",
    "            include_types=[\"tool\", \"llm\"],\n",
    "        )\n",
    "    )\n",
    "]\n",
    "events[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd953c8-767b-4c26-956f-f42b677849bb",
   "metadata": {},
   "source": [
    "Let's use the parsed astream_log information to print stream some information about what the agent and the tools are doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3b4a583-2141-43bb-a1f1-c8bf77d64084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start >> ChatOpenAI (llm), tags: agent_llm, seq:step:3\n",
      "\n",
      "End >> ChatOpenAI [llm]\n",
      "\n",
      "Start >> where_cat_is_hiding (tool), tags: []\n",
      "Start >> ChatOpenAI (llm), tags: hiding_spot\n",
      "The Cozy Cubby\n",
      "End >> ChatOpenAI [llm]\n",
      "\n",
      "\n",
      "End >> where_cat_is_hiding [tool]\n",
      "\n",
      "Start >> ChatOpenAI (llm), tags: agent_llm, seq:step:3\n",
      "\n",
      "End >> ChatOpenAI [llm]\n",
      "\n",
      "Start >> get_items (tool), tags: []\n",
      "Start >> Get Items LLM (llm), tags: get_items, seq:step:2\n",
      "At 'The Cozy Cubby', you might find the following items:\n",
      "\n",
      "1. Plush Throw Blankets: Soft and luxurious, these blankets are perfect for snuggling up on the couch or adding a cozy touch to your bed. They come in various colors and patterns, providing warmth and comfort during chilly evenings.\n",
      "\n",
      "2. Aromatherapy Candles: These candles are designed to create a soothing ambiance and fill the air with delightful scents. Made from natural ingredients, they offer a calming and relaxing atmosphere, perfect for unwinding after a long day.\n",
      "\n",
      "3. Vintage Teacups: Delicate and charming, these vintage teacups add a touch of elegance to your tea time. With intricate designs and beautiful patterns, they are perfect for enjoying a cup of tea or as decorative pieces for your kitchen or dining area.\n",
      "End >> Get Items LLM [llm]\n",
      "\n",
      "\n",
      "End >> get_items [tool]\n",
      "\n",
      "Start >> ChatOpenAI (llm), tags: agent_llm, seq:step:3\n",
      "The items located where the cat is hiding at \"The Cozy Cubby\" are:\n",
      "\n",
      "1. Plush Throw Blankets\n",
      "2. Aromatherapy Candles\n",
      "3. Vintage Teacups\n",
      "End >> ChatOpenAI [llm]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessageChunk\n",
    "\n",
    "async for event in as_event_stream(\n",
    "    agent_executor.astream_log(\n",
    "        {\"input\": \"what items are located where the cat is hiding?\"},\n",
    "        include_types=[\"tool\", \"llm\"],\n",
    "    )\n",
    "):\n",
    "    function_message = None\n",
    "    if event[\"event_type\"] == \"start\":\n",
    "        tags = \", \".join(sorted(event[\"tags\"]))\n",
    "        print(f\"Start >> {event['name']} ({event['type']}), tags: {tags or []}\")\n",
    "\n",
    "    if event[\"event_type\"] == \"stream\":\n",
    "        value = event[\"value\"]\n",
    "\n",
    "        if event[\"stream_type\"] != \"original\":\n",
    "            continue\n",
    "\n",
    "        if not value:\n",
    "            continue\n",
    "        if isinstance(value, AIMessageChunk):\n",
    "            # print(event['time'])\n",
    "            print(value.content, end=\"\")\n",
    "        else:\n",
    "            raise NotImplementedError(type(value))\n",
    "\n",
    "    if event[\"event_type\"] == \"end\":\n",
    "        print()\n",
    "        print(f\"End >> {event['name']} [{event['type']}]\")\n",
    "        print()\n",
    "        function_message = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
