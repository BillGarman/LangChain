{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4adf4238-b6e5-43ca-ba5e-7fd2884dfaf1",
   "metadata": {},
   "source": [
    "# Custom Chat Model\n",
    "\n",
    "Let's see how to create a custom LangChain `ChatModel` implementation.\n",
    "\n",
    "This allows you to wrap an LLM that may not be currently supported by `LangChain` for your own use and to maybe even contribute it back to LangChain!\n",
    "\n",
    "Wrapping the LLM with the `ChatModel` interface makes it easy to swap your `LLM` into existing `LangChain` programs with minimal code modifications!\n",
    "\n",
    "In addition, by wrapping the LLM with a `ChatModel` implementation, the implementation is automatically endowed \n",
    "with the standard `LangChain Runnable` interface and benefits from certain \n",
    "optimizations out of the box (e.g., batch) as well as async support (e.g., `ainvoke`, `abatch`).\n",
    "\n",
    "There are 2 interface that you can inherit from to provide your implementation:\n",
    "\n",
    "1. `SimpleChatModel` -- Meant for prototyping. This iswhen not all features are required (e.g., not need for function calling).\n",
    "2. `BaseChatModel` -- Best suited for a full implementation that supports all features (e.g., streaming, function calling)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "120f1b46-d6ee-4e7a-84ba-afe5f68baa80",
   "metadata": {},
   "source": [
    "## Inputs and outputs\n",
    "\n",
    "Before we start with implementing a chat model. Let's take a look at the inputs and outputs of chat models.\n",
    "\n",
    "### Messages\n",
    "\n",
    "Chat models take messages as inputs and return chat messages as outputs. \n",
    "\n",
    "Here are the messages:\n",
    "\n",
    "- `SystemMessage`: Used for priming AI behavior, usually passed in as the first of a sequence of input messages.\n",
    "- `HumanMessage`: Represents a message from a person interacting with the chat model.\n",
    "- `AIMessage`: Represents a message from the chat model. This can be either text or a request to invoke a tool.\n",
    "- `FunctionMessage` / `ToolMessage`: Message for passing the results of tool invocation back to the model.\n",
    "\n",
    "::: {.callout-note}\n",
    "`ToolMessage` and `FunctionMessage` closely follow OpenAIs `function` and `tool` arguments.\n",
    "\n",
    "This is a rapidly developing field and as more models add function calling capabilities, expect that there will be additions to this schema.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5046e6a-8b09-4a99-b6e6-7a605aac5738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage, SystemMessage, AIMessage, HumanMessage, FunctionMessage, ToolMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53033447-8260-4f53-bd6f-b2f744e04e75",
   "metadata": {},
   "source": [
    "### Streaming Variant\n",
    "\n",
    "All the chat messages have a streaming variant that contains `Chunk` in the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4656e9d-bfa1-4703-8f79-762fe6421294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessageChunk, AIMessageChunk, HumanMessageChunk, FunctionMessageChunk, ToolMessageChunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ebf3f4-c760-4898-b921-fdb469453d4a",
   "metadata": {},
   "source": [
    "These chunks are used when streaming output from chat models, and they all define an additive property!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c15c299-6f8a-49cf-a072-09924fd44396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='Hello World!')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AIMessageChunk(content=\"Hello\") + AIMessageChunk(content=\" World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e952d64-6d38-4a2b-b996-8812c204a12c",
   "metadata": {},
   "source": [
    "## Simple Chat Model\n",
    "\n",
    "Inherting from `SimpleChatModel` is great for prototyping!\n",
    "\n",
    "It won't allow you to implement all features that you might want out of a chat model, but it's quick to implement, and if you need more you can transition to `BaseChatModel` shown below.\n",
    "\n",
    "Let's implement a chat model that echoes back the last `n` characters of the prompt!\n",
    "\n",
    "You need to implement the following:\n",
    "\n",
    "* The method `_call` - Use to generate a chat result from a prompt.\n",
    "* The property `_llm_type` - Used to uniquely identify the type of the model. Used for logging.\n",
    "\n",
    "In addition, you have the option to specify the following:\n",
    "\n",
    "* The property `_identifying_params` - Represent model parameterization for logging purposes.\n",
    "\n",
    "Optional:\n",
    "\n",
    "* `_stream` - Use to implement streaming.\n",
    "* `_agenerate` - Use to implement a native async method\n",
    "* `_astream` - Use to implement async version of `_stream`\n",
    "\n",
    ":::{.callout-caution}\n",
    "\n",
    "If you're implementing streaming and want streaming to work in async, you must provide an async implementation of streaming (`_astream`).\n",
    "\n",
    "If you want to replicate the logic in the sync variant of `_stream` you can use the trick below by running it in a separate executor.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b318e80c-ca39-4743-bbb9-0d6bd30afeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Dict\n",
    "\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models import SimpleChatModel\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "\n",
    "class CustomChatModel(SimpleChatModel):\n",
    "    \"\"\"A custom chat model that echoes the first `n` characters of the input.\n",
    "    \n",
    "    When contributing an implementation to LangChain, carefully document\n",
    "    the model including the initialization parameters, include\n",
    "    an example of how to initialize the model and include any relevant\n",
    "    links to the underlying models documentation or API.\n",
    "    \n",
    "    Example:\n",
    "        \n",
    "        .. code-block:: python\n",
    "        \n",
    "            model = CustomChatModel(n=2)\n",
    "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
    "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
    "                                 [HumanMessage(content=\"world\")]])\n",
    "    \"\"\"\n",
    "    n: int\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "\n",
    "    # This is the core logic. It accepts a prompt composed of a list of messages\n",
    "    # and returns a response which is a string.\n",
    "    def _call(\n",
    "            self,\n",
    "            messages: List[BaseMessage],\n",
    "            stop: Optional[List[str]] = None,\n",
    "            run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "            **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Implementation of the chat model logic.\n",
    "\n",
    "        Args:\n",
    "            messages: the prompt composed of a list of messages.\n",
    "            stop: a list of strings on which the model should stop generating.\n",
    "                  If generation stops due to a stop token, the stop token itself\n",
    "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
    "                  across models right now, but it's a good practice to follow since\n",
    "                  it makes it much easier to parse the output of the model\n",
    "                  downstream and understand why generation stopped.\n",
    "            run_manager: A run manager that contains callbacks for the LLM.\n",
    "                on_chat_start and on_llm_end callbacks are automatically called\n",
    "                by wrapping code, so you don't need to invoke them when \n",
    "                using SimpleChatModel.\n",
    "                Please refer to the callbacks section in the documentation for\n",
    "                more details about callbacks.\n",
    "        \"\"\"\n",
    "        return messages[-1].content[:self.n]\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model.\n",
    "\n",
    "        This property must return a string. It's used for logging purposes, and\n",
    "        will be accessible from callbacks. It should identify the type\n",
    "        of the model uniquely.\n",
    "        \"\"\"\n",
    "        return \"echoing_chat_model\"\n",
    "\n",
    "    # **Optional** override the identifying parameters to include additional\n",
    "    # information about the model parameterization for logging purposes\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\"n\": self.n}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714dede0",
   "metadata": {},
   "source": [
    "### Let's test it 🧪\n",
    "\n",
    "The chat model will implement the standard `Runnable` interface of LangChain which many of the LangChain abstractions support!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e5ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomChatModel(n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d92f328f-8094-4d6e-8db1-a929c05368a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='hello w')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content='hello world!')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245f2ecf-aaa7-457a-8fed-828faac1dea4",
   "metadata": {},
   "source": [
    "In addition, it supoprts the standard type conversions associated with chat models to make it more convenient to work with it! \n",
    "\n",
    "An input of a string, gets interpreted as a `Human Message`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ab5064-ba04-4ea1-9eb4-8703fa8f1474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello W')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e7b9d-d373-46ee-b118-53e8fed47f70",
   "metadata": {},
   "source": [
    "And look async works as well 😘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b832be1-19cc-4164-8f0d-80f686696256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello W')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await model.ainvoke(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e6c75-33d4-446d-86be-d490bdfec18b",
   "metadata": {},
   "source": [
    "By adding a LangChain interface to your LLM, you get a bunch of optimizations out of the box!\n",
    "\n",
    "Batch by default executes in a threadpool which means that operations that block on IO (e.g., an API call to another service), will automatically be run in parallel!\n",
    "\n",
    ":::{.callout-note}\n",
    "The default `batch` implementation will not help to speed up CPU / GPU bound operations, unless those operations are delegated to lower level code that releases the GIL.\n",
    "\n",
    "If this doesn't mean much to you, then just ignore it. 😵‍💫\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cd49199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hello w'), AIMessage(content='goodbye')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.batch(['hello world', 'goodbye moon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a16e5-4ce8-4ea8-bbb7-e0fcc3777391",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "The `astream` interface will work as well, but it won't actually stream since we haven't provided a streaming implementation!\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e898fa8-bd5d-4470-8a6a-6001f60997f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='hello w'\n"
     ]
    }
   ],
   "source": [
    "async for chunk in model.astream('hello world'):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfebea1",
   "metadata": {},
   "source": [
    "## BaseChatModel\n",
    "\n",
    "You may want to add additional features like function calling or running the model in JSON mode or streaming.\n",
    "\n",
    "To do so inherit from `BaseChatModel` which is a lower level class and implement the methods:\n",
    "\n",
    "* `_generate` - Use to generate a chat result from a prompt\n",
    "\n",
    "Optional:\n",
    "\n",
    "* `_stream` - Use to implement streaming\n",
    "* `_agenerate` - Use to implement a native async method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d309a719-087e-4e39-a291-6aaa16da7a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, AsyncIterator, List, Optional, Dict, Iterator\n",
    "\n",
    "from langchain_core.callbacks.manager import AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun\n",
    "from langchain_core.language_models import SimpleChatModel, BaseChatModel\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.outputs import ChatResult, ChatGeneration, ChatGenerationChunk\n",
    "from langchain_core.messages import AIMessageChunk\n",
    "from langchain_core.runnables import run_in_executor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomChatModelAdvanced(BaseChatModel):\n",
    "    \"\"\"A custom chat model that echoes the first `n` characters of the input.\n",
    "\n",
    "    When contributing an implementation to LangChain, carefully document\n",
    "    the model including the initialization parameters, include\n",
    "    an example of how to initialize the model and include any relevant\n",
    "    links to the underlying models documentation or API.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            model = CustomChatModel(n=2)\n",
    "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
    "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
    "                                 [HumanMessage(content=\"world\")]])\n",
    "    \"\"\"\n",
    "    n: int\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "\n",
    "\n",
    "    def _generate(\n",
    "            self,\n",
    "            messages: List[BaseMessage],\n",
    "            stop: Optional[List[str]] = None,\n",
    "            run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "            **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"Override the _generate method to implement the chat model logic.\n",
    "\n",
    "        This can be a call to an API, a call to a local model, or any other\n",
    "        implementation that generates a response to the input prompt.\n",
    "\n",
    "        Args:\n",
    "            messages: the prompt composed of a list of messages.\n",
    "            stop: a list of strings on which the model should stop generating.\n",
    "                  If generation stops due to a stop token, the stop token itself\n",
    "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
    "                  across models right now, but it's a good practice to follow since\n",
    "                  it makes it much easier to parse the output of the model\n",
    "                  downstream and understand why generation stopped.\n",
    "            run_manager: A run manager with callbacks for the LLM.\n",
    "        \"\"\"\n",
    "        last_message = messages[-1]\n",
    "        tokens = last_message.content[:self.n]    \n",
    "        message = AIMessage(content=tokens)\n",
    "        generation = ChatGeneration(message=message)\n",
    "        return ChatResult(generations=[generation])\n",
    "\n",
    "    def _stream(\n",
    "            self,\n",
    "            messages: List[BaseMessage],\n",
    "            stop: Optional[List[str]] = None,\n",
    "            run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "            **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        \"\"\"Stream the output of the model.\n",
    "\n",
    "        This method should be implemented if the model can generate output\n",
    "        in a streaming fashion. If the model does not support streaming,\n",
    "        do not implement it. In that case streaming requests will be automatically\n",
    "        handled by the _generate method.\n",
    "\n",
    "        Args:\n",
    "            messages: the prompt composed of a list of messages.\n",
    "            stop: a list of strings on which the model should stop generating.\n",
    "                  If generation stops due to a stop token, the stop token itself\n",
    "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
    "                  across models right now, but it's a good practice to follow since\n",
    "                  it makes it much easier to parse the output of the model\n",
    "                  downstream and understand why generation stopped.\n",
    "            run_manager: A run manager with callbacks for the LLM.\n",
    "        \"\"\"\n",
    "        last_message = messages[-1]\n",
    "        tokens = last_message.content[:self.n]\n",
    "\n",
    "        for token in tokens:\n",
    "            chunk = ChatGenerationChunk(message=AIMessageChunk(content=token))\n",
    "\n",
    "            if run_manager:\n",
    "                run_manager.on_llm_new_token(token, chunk=chunk)\n",
    "\n",
    "            yield chunk\n",
    "\n",
    "    async def _astream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> AsyncIterator[ChatGenerationChunk]:\n",
    "        \"\"\"An async variant of astream.\n",
    "\n",
    "        If not provided, the default behavior is to delegate to the _generate method.\n",
    "\n",
    "        The implementation below instead will delegate to `_stream` and will\n",
    "        kick it off in a separate thread.\n",
    "\n",
    "        If you're able to natively support async, then by all means do so!\n",
    "        \"\"\"\n",
    "        result = await run_in_executor(\n",
    "            None,\n",
    "            self._stream,\n",
    "            messages,\n",
    "            stop=stop,\n",
    "            run_manager=run_manager.get_sync() if run_manager else None,\n",
    "            **kwargs,\n",
    "        )\n",
    "        for chunk in result:\n",
    "            yield chunk\n",
    "\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model.\"\"\"\n",
    "        return \"echoing-chat-model-advanced\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\"n\": self.n}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9af284-f2d3-44e2-ac6a-09b73d89ada3",
   "metadata": {},
   "source": [
    "### Let's test it 🧪\n",
    "\n",
    "The chat model will implement the standard `Runnable` interface of LangChain which many of the LangChain abstractions support!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34bf2d48-556a-48be-aee7-496fb02332f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomChatModelAdvanced(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27689f30-dcd2-466b-ba9d-f60b7d434110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Meo')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content='hello!'), AIMessage(content=\"Hi there human!\"), HumanMessage(content=\"Meow!\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "406436df-31bf-466b-9c3d-39db9d6b6407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='hel')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a72ffa46-6004-41ef-bbe4-56fa17a029e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hel'), AIMessage(content='goo')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.batch(['hello', 'goodbye'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3633be2c-2ea0-42f9-a72f-3b5240690b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c|a|t|"
     ]
    }
   ],
   "source": [
    "for chunk in model.stream('cat'):\n",
    "    print(chunk.content, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a7c42-aec4-4116-adf3-93133d409827",
   "metadata": {},
   "source": [
    "Please see the implementation of `_astream` in the model! If you do not implement it, then no output will stream.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7d73995-eeab-48c6-a7d8-32c98ba29fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c|a|t|"
     ]
    }
   ],
   "source": [
    "async for chunk in model.astream('cat'):\n",
    "    print(chunk.content, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80dc55b-d159-4527-9191-407a7c6d6042",
   "metadata": {},
   "source": [
    "Let's try to use the astream events API which will also help double check that all the callbacks were implemented!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17840eba-8ff4-4e73-8e4f-85f16eb1c9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chat_model_start', 'run_id': 'fe9141d7-78a0-4a5e-a267-44194f3a1d39', 'name': 'CustomChatModelAdvanced', 'tags': [], 'metadata': {}, 'data': {'input': 'cat'}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': 'fe9141d7-78a0-4a5e-a267-44194f3a1d39', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='c')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': 'fe9141d7-78a0-4a5e-a267-44194f3a1d39', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='a')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': 'fe9141d7-78a0-4a5e-a267-44194f3a1d39', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='t')}}\n",
      "{'event': 'on_chat_model_end', 'name': 'CustomChatModelAdvanced', 'run_id': 'fe9141d7-78a0-4a5e-a267-44194f3a1d39', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='cat')}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "async for event in model.astream_events('cat', version='v1'):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee559b-b1da-4851-8c97-420ab394aff9",
   "metadata": {},
   "source": [
    "## Contributing\n",
    "\n",
    "We would very much appreciate contributions for the chat model.\n",
    "\n",
    "Here's a checklist to help you validate your implementation of the chat model.\n",
    "\n",
    "### Checklist\n",
    "\n",
    "An overview of things to verify to make sure that the implementation is done correctly.\n",
    "\n",
    "Documentation:\n",
    "- [ ] The model contains doc-strings for all initialization arguments, as these will be surfaced in the [APIReference](https://api.python.langchain.com/en/stable/langchain_api_reference.html).\n",
    "- [ ] The class doc-string for the model contains a link to the model API if the model is powered by a service.\n",
    "\n",
    "Tests:\n",
    "- [ ] Add unit or integration tests to the overridden methods. Verify that `invoke`, `ainvoke`, `batch`, `stream` work if you've over-ridden the corresponding code.\n",
    "\n",
    "Streaming (if you're implementing it):\n",
    "- [ ] Provided an async implementation via `_astream`\n",
    "- [ ] Make sure to invoke the `on_llm_new_token` callback\n",
    "- [ ] `on_llm_new_token` is invoked BEFORE yielding the chunk\n",
    "\n",
    "Stop Token Behavior:\n",
    "- [ ] Stop token should be respected\n",
    "- [ ] Stop token should be INCLUDED as part of the response\n",
    "\n",
    "Secret API Keys:\n",
    "- [ ] If your model connects to an API it will likely accept API keys as part of its initialization. Use Pydantic's `SecretStr` type for secrets, so they don't get accidentally printed out when folks print the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
