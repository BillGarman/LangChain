{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca415494-62b6-4254-8ffd-953426bc776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "sidebar_position: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae8d4ed-9150-45da-b494-7717ab0a2960",
   "metadata": {},
   "source": [
    "# Function calling\n",
    "\n",
    "Certain chat models like [OpenAI](https://platform.openai.com/docs/guides/function-calling), [Gemini](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling), etc. have a function-calling API that lets you describe functions and their arguments, and have the model return a JSON object with a function to invoke and the inputs to that function. Function-calling is extremely useful for building [tool-using chains and agents](/docs/use_cases/tool_use/), and for getting structured outputs from models more generally.\n",
    "\n",
    "LangChain comes with a number of utilities to make function-calling easy. Namely, it comes with:\n",
    "\n",
    "* simple syntax for binding functions to models\n",
    "* converters for formatting various types of objects to the expected function schemas\n",
    "* output parsers for extracting the function invocations from API responses\n",
    "* chains for getting structured outputs from a model, built on top of function calling\n",
    "\n",
    "We'll focus here on the first two points. To see how output parsing works check out the [OpenAI Tools output parsers](/docs/modules/model_io/output_parsers/types/openai_tools) and to see the structured output chains check out the [Structured output guide](/docs/guides/structured_output)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9bfa44-2d46-421c-bbd1-a078145de018",
   "metadata": {},
   "source": [
    "Before getting started make sure you have `langchain-core` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d1dc0c-6170-4977-809f-365099f628ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-core langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c44609-be17-49aa-89b4-03a9c775d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d3c3b-aecf-4e5f-b3fb-f378f157c2b7",
   "metadata": {},
   "source": [
    "## Binding functions\n",
    "\n",
    "A number of models implement helper methods that will take care of formatting and binding different function-like objects to the model. Let's take a look at how we might take the following Pydantic function schema and get different models to invoke it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95008145-6eff-44a0-8ca2-2c465c716390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Note that the docstring here are crucial, as they will be passed along \n",
    "# to the model along with the class name.\n",
    "class Multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4de12-d41a-4f3a-9ffb-eb48e300de97",
   "metadata": {},
   "source": [
    "import Tabs from '@theme/Tabs';\n",
    "import TabItem from '@theme/TabItem';\n",
    "import CodeBlock from \"@theme/CodeBlock\";\n",
    "\n",
    "<Tabs>\n",
    "  <TabItem value=\"openai\" label=\"OpenAI\" default>\n",
    "\n",
    "Set up dependencies and API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d560a747-f01e-4ee3-839b-1134b45bbae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf95a2-658f-4506-8032-02cb3200b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9016c3-05d7-41a2-933a-cd2276b9fe72",
   "metadata": {},
   "source": [
    "We can use the `ChatOpenAI.bind_tools()` method to handle converting `Multiply` to an OpenAI function and binding it to the model (i.e., passing it in each time the model is invoked). \n",
    "\n",
    ":::{.callout-note} \n",
    "There is a legacy `ChatOpenAI.bind_functions()` method that uses OpenAI's now-deprecated `functions` API parameter instead of their\n",
    "updated `tools` parameter. Using the new `tools` parameter allows us to take advantage of the parallel function-calling capabilities of the latest models.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "140fa1ca-06f6-41d3-9e26-c87070b51ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Q8ZQ97Qrj5zalugSkYMGV1Uo', 'function': {'arguments': '{\"a\":3,\"b\":12}', 'name': 'Multiply'}, 'type': 'function'}]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([Multiply])\n",
    "llm_with_tools.invoke(\"what's 3 * 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e20c2a-073e-47a9-817b-6c429e3a8b2e",
   "metadata": {},
   "source": [
    "We can add a tool parser to extract the tool calls from the generated message to JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d94edf6a-e731-46d4-974f-218b1e92f55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'Multiply', 'args': {'a': 3, 'b': 12}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "\n",
    "tool_chain = llm_with_tools | JsonOutputToolsParser()\n",
    "tool_chain.invoke(\"what's 3 * 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caedad55-83bd-477e-a433-5c62230ed917",
   "metadata": {},
   "source": [
    "Or back to the original Pydantic class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "455198d9-cf33-429b-90a0-d23d50f2f8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Multiply(a=3, b=12)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "\n",
    "tool_chain = llm_with_tools | PydanticToolsParser(tools=[Multiply])\n",
    "tool_chain.invoke(\"what's 3 * 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452af02-f638-4787-86ef-e7b7e4d4a345",
   "metadata": {},
   "source": [
    "If we wanted to force that a tool is used (and that it is used only once), we can set the `tool_choice` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7db165dd-5ddd-45a5-9efe-d296b2e6766d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_f3DApOzb60iYjTfOhVFhDRMI', 'function': {'arguments': '{\"a\":5,\"b\":10}', 'name': 'Multiply'}, 'type': 'function'}]})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_multiply = llm.bind_tools([Multiply], tool_choice=\"Multiply\")\n",
    "llm_with_multiply.invoke(\"make up some numbers if you really want but I'm not forcing you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6427dc0-7a2b-471e-9975-d75462da0be9",
   "metadata": {},
   "source": [
    "For more see the [ChatOpenAI API reference](https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI.bind_tools)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f06dbd-869d-49da-b63b-ce95dfcc0512",
   "metadata": {},
   "source": [
    "  </TabItem>\n",
    "  <TabItem value=\"google_vertexai\" label=\"Google Vertex AI\">\n",
    "\n",
    "Install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122b498-2886-42c4-b76f-c73145b2a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb9324a-e2a4-4883-a3ed-360b20daf962",
   "metadata": {},
   "source": [
    "Uncomment to set up Google Cloud auth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a4268c-a2e0-4153-9a40-9103f2b55cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81077a0-30b3-4d88-92ff-86acca77eeca",
   "metadata": {},
   "source": [
    "We can pass Pydantic classes in as function schemas by binding the `functions` argument to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0cf4b1-2374-4081-a68a-5f983d237f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "llm = ChatVertexAI(model=\"gemini-pro\", temperature=0)\n",
    "llm_with_functions = llm.bind(functions=[Multiply])\n",
    "llm_with_functions.invoke(\"what's 3 * 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce9b0ea-8a09-4c26-a3d8-4f44d0dbf268",
   "metadata": {},
   "source": [
    "  </TabItem>\n",
    "  <TabItem value=\"fireworks\" label=\"Fireworks\">\n",
    "\n",
    "Install dependencies and set API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc4e55-b6a2-4836-a523-b74b9a419f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-fireworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c7062-ab65-4328-8bcb-c1a5933b9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858612c5-342b-4331-bb49-03b3569d3cdc",
   "metadata": {},
   "source": [
    "We can use the `ChatFireworks.bind_tools()` method to handle converting `Multiply` to a valid function schema and binding it to the model (i.e., passing it in each time the model is invoked). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d38b84e-655d-4a67-aed8-c855ee439558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Three multiplied by twelve is 36.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_fireworks import ChatFireworks\n",
    "\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/firefunction-v1\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([Multiply])\n",
    "llm_with_tools.invoke(\"what's 3 * 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff91427-6f69-47d0-bb26-27ab12b53aaa",
   "metadata": {},
   "source": [
    "If our model isn't using the tool, as is the case here, we can force tool usage by specifying `tool_choice=\"any\"` or by specifying the name of the specific tool we want used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd513f05-4e32-452e-be22-46432c4be2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_qIP2bJugb67LGvc6Zhwkvfqc', 'type': 'function', 'function': {'name': 'Multiply', 'arguments': '{\"a\": 3, \"b\": 12}'}}]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools = llm.bind_tools([Multiply], tool_choice=\"Multiply\")\n",
    "llm_with_tools.invoke(\"what's 3 * 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b610df-03a1-4034-ab28-77b6a9692a0d",
   "metadata": {},
   "source": [
    "We can add a tool parser to extract the tool calls from the generated message to JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5857702b-e757-47b5-a70e-192ef17bfc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'Multiply', 'args': {'a': 3, 'b': 12}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "\n",
    "tool_chain = llm_with_tools | JsonOutputToolsParser()\n",
    "tool_chain.invoke(\"what's 3 * 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52853b-85f3-4ef8-a82c-13a082d6349b",
   "metadata": {},
   "source": [
    "Or back to the original Pydantic class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "478c63b5-ec01-4178-8fc8-5a5db781953c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Multiply(a=3, b=12)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "\n",
    "tool_chain = llm_with_tools | PydanticToolsParser(tools=[Multiply])\n",
    "tool_chain.invoke(\"what's 3 * 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd00142-38d1-46de-a692-867998321886",
   "metadata": {},
   "source": [
    "  </TabItem>\n",
    "  <TabItem value=\"mistral\" label=\"Mistral\">\n",
    "\n",
    "Install dependencies and set API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6cc50d-7607-47b4-9d0c-78d00ad07e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f13e8-da08-4c79-b4d9-31e4058ee772",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec855af-428e-4947-9d65-6eaf4b898045",
   "metadata": {},
   "source": [
    "We can use the `ChatMistralAI.bind_tools()` method to handle converting `Multiply` to a valid function schema and binding it to the model (i.e., passing it in each time the model is invoked). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f5cb0ce-ec84-460e-83ff-1d680bfc4bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'null', 'type': <ToolType.function: 'function'>, 'function': {'name': 'Multiply', 'arguments': '{\"a\": 3, \"b\": 12}'}}]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([Multiply])\n",
    "llm_with_tools.invoke(\"what's 3 * 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde3aad-14fe-4412-9f7b-670f000e0b48",
   "metadata": {},
   "source": [
    "We can add a tool parser to extract the tool calls from the generated message to JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2da396dd-30e5-4da3-bf7c-7aeac23f8c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'Multiply', 'args': {'a': 3, 'b': 12}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "\n",
    "tool_chain = llm_with_tools | JsonOutputToolsParser()\n",
    "tool_chain.invoke(\"what's 3 * 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513907f3-2c56-4fb7-953f-00777d9a326d",
   "metadata": {},
   "source": [
    "Or back to the original Pydantic class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9085faee-2575-4adc-b475-5118a84d043a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Multiply(a=3, b=12)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "\n",
    "tool_chain = llm_with_tools | PydanticToolsParser(tools=[Multiply])\n",
    "tool_chain.invoke(\"what's 3 * 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ff31ca-9bdd-4a20-abc1-3303e2a7d580",
   "metadata": {},
   "source": [
    "We can force tool usage by specifying `tool_choice=\"any\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5233487-3531-4a83-86ab-31a9226b97cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'null', 'type': <ToolType.function: 'function'>, 'function': {'name': 'Multiply', 'arguments': '{\"a\": 5, \"b\": 7}'}}]})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools = llm.bind_tools([Multiply], tool_choice=\"any\")\n",
    "llm_with_tools.invoke(\"I don't even want you to use the tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d17340-5920-465f-85d6-3742e0da8f41",
   "metadata": {},
   "source": [
    "  </TabItem>\n",
    "  <TabItem value=\"mistral\" label=\"Mistral\">\n",
    "\n",
    "Since TogetherAI is a drop-in replacement for OpenAI, we can just use the OpenAI integration.\n",
    "\n",
    "Install dependencies and set API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a22e679-812d-45ef-b5df-7ddf499772e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547eb3db-e8ce-41b1-a44d-2a3253053532",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29190e8b-9c59-4b28-a6b9-2ba1c6ad4318",
   "metadata": {},
   "source": [
    "We can use the `ChatOpenAI.bind_tools()` method to handle converting `Multiply` to a valid function schema and binding it to the model (i.e., passing it in each time the model is invoked). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc71d172-5071-4503-80ce-d7c4852cadf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_4tc61dp0478zafqe33hfriee', 'function': {'arguments': '{\"a\":3,\"b\":12}', 'name': 'Multiply'}, 'type': 'function'}]})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    api_key=os.environ[\"TOGETHER_API_KEY\"],\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    ")\n",
    "llm_with_tools = llm.bind_tools([Multiply])\n",
    "llm_with_tools.invoke(\"what's 3 * 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98820f29-cbef-49cb-8385-7cbeb73311cc",
   "metadata": {},
   "source": [
    "We can add a tool parser to extract the tool calls from the generated message to JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cf313ab-49bb-418b-b241-06b17e0a6a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'Multiply', 'args': {'a': 3, 'b': 12}}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "\n",
    "tool_chain = llm_with_tools | JsonOutputToolsParser()\n",
    "tool_chain.invoke(\"what's 3 * 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7df8786-bd7d-4722-91cc-9b2345bdd9d0",
   "metadata": {},
   "source": [
    "Or back to the original Pydantic class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a2299dc-1f95-4886-ad46-a0888349373f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Multiply(a=3, b=12)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "\n",
    "tool_chain = llm_with_tools | PydanticToolsParser(tools=[Multiply])\n",
    "tool_chain.invoke(\"what's 3 * 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c1bfa-9602-422b-a39a-a3baddbb7a0f",
   "metadata": {},
   "source": [
    "If we wanted to force that a tool is used (and that it is used only once), we can set the `tool_choice` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dd8f071-8b48-4668-8cf7-9196ccce5a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_6k6d0gr3jhqil2kqf7sgeusl', 'function': {'arguments': '{\"a\":5,\"b\":7}', 'name': 'Multiply'}, 'type': 'function'}]})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_multiply = llm.bind_tools([Multiply], tool_choice=\"Multiply\")\n",
    "llm_with_multiply.invoke(\"make up some numbers if you really want but I'm not forcing you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b2bf3c-3d73-4378-8d24-3821f69189c3",
   "metadata": {},
   "source": [
    "  </TabItem>\n",
    "</Tabs>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a177c64b-7c99-495c-b362-5ed3b40aa26a",
   "metadata": {},
   "source": [
    "## Defining functions\n",
    "\n",
    "We'll focus on the [OpenAI function format](https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools) here since as of this writing that is the main model provider that supports function calling. LangChain has a built-in converter that can turn Python functions, Pydantic classes, and LangChain Tools into the OpenAI function format:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd290bd-7621-466b-a73e-fc8480f879ec",
   "metadata": {},
   "source": [
    "### Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ebab5c-0e9f-4b49-86ee-9290ced2fe96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\": \"function\",\n",
      "  \"function\": {\n",
      "    \"name\": \"multiply\",\n",
      "    \"description\": \"Multiply two integers together.\",\n",
      "    \"parameters\": {\n",
      "      \"type\": \"object\",\n",
      "      \"properties\": {\n",
      "        \"a\": {\n",
      "          \"type\": \"integer\",\n",
      "          \"description\": \"First integer\"\n",
      "        },\n",
      "        \"b\": {\n",
      "          \"type\": \"integer\",\n",
      "          \"description\": \"Second integer\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"a\",\n",
      "        \"b\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers together.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "print(json.dumps(convert_to_openai_tool(multiply), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf22577-38ab-48f1-ba0b-371aaba1bacc",
   "metadata": {},
   "source": [
    "### Pydantic class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecc8ffd4-aed3-4f47-892d-1896cc1ca4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\": \"function\",\n",
      "  \"function\": {\n",
      "    \"name\": \"multiply\",\n",
      "    \"description\": \"Multiply two integers together.\",\n",
      "    \"parameters\": {\n",
      "      \"type\": \"object\",\n",
      "      \"properties\": {\n",
      "        \"a\": {\n",
      "          \"description\": \"First integer\",\n",
      "          \"type\": \"integer\"\n",
      "        },\n",
      "        \"b\": {\n",
      "          \"description\": \"Second integer\",\n",
      "          \"type\": \"integer\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"a\",\n",
      "        \"b\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "print(json.dumps(convert_to_openai_tool(multiply), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d5a88-50ed-4ae4-85cf-8b895617496f",
   "metadata": {},
   "source": [
    "### LangChain Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "696c7dd6-660c-4797-909f-bf878b3acf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\": \"function\",\n",
      "  \"function\": {\n",
      "    \"name\": \"multiply\",\n",
      "    \"description\": \"Multiply two integers together.\",\n",
      "    \"parameters\": {\n",
      "      \"type\": \"object\",\n",
      "      \"properties\": {\n",
      "        \"a\": {\n",
      "          \"description\": \"First integer\",\n",
      "          \"type\": \"integer\"\n",
      "        },\n",
      "        \"b\": {\n",
      "          \"description\": \"Second integer\",\n",
      "          \"type\": \"integer\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"a\",\n",
      "        \"b\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Type\n",
    "\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "\n",
    "class MultiplySchema(BaseModel):\n",
    "    \"\"\"Multiply tool schema.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "class Multiply(BaseTool):\n",
    "    args_schema: Type[BaseModel] = MultiplySchema\n",
    "    name: str = \"multiply\"\n",
    "    description: str = \"Multiply two integers together.\"\n",
    "\n",
    "    def _run(self, a: int, b: int, **kwargs: Any) -> Any:\n",
    "        return a * b\n",
    "\n",
    "\n",
    "# Note: we're passing in a Multiply object not the class itself.\n",
    "print(json.dumps(convert_to_openai_tool(Multiply()), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bda177-202f-4811-bb74-f3fa7094a15b",
   "metadata": {},
   "source": [
    "## Binding functions\n",
    "\n",
    "Now that we've defined a function, we'll want to pass it in to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5aa93a7-6859-43e8-be85-619d975b908c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_JvOu9oUwMrQHiDekZTbpNCHY', 'function': {'arguments': '{\\n  \"a\": 5,\\n  \"b\": 3\\n}', 'name': 'multiply'}, 'type': 'function'}]})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm.invoke(\"what's 5 times three\", tools=[convert_to_openai_tool(multiply)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e7365-32d0-46a3-b8f2-caf27d5d9262",
   "metadata": {},
   "source": [
    "And if we wanted this function to be passed in every time we call the tool, we could bind it to the tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87165d64-31a7-4332-965e-18fa939fda50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_cwRoTnD1ux1SnWXLrTj2KlWH', 'function': {'arguments': '{\\n  \"a\": 5,\\n  \"b\": 3\\n}', 'name': 'multiply'}, 'type': 'function'}]})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tool = llm.bind(tools=[convert_to_openai_tool(multiply)])\n",
    "llm_with_tool.invoke(\"what's 5 times three\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b4d000-3828-4e32-9226-55119f47ee67",
   "metadata": {},
   "source": [
    "We can also enforce that a tool is called using the [tool_choice](https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools) parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2daa354c-cc85-4a60-a9b2-b681ec22ca33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_sWjLyioSZAtYMQRLMTzncz1v', 'function': {'arguments': '{\\n  \"a\": 5,\\n  \"b\": 4\\n}', 'name': 'multiply'}, 'type': 'function'}]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tool = llm.bind(\n",
    "    tools=[convert_to_openai_tool(multiply)],\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"multiply\"}},\n",
    ")\n",
    "llm_with_tool.invoke(\n",
    "    \"don't answer my question. no do answer my question. no don't. what's five times four\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce013d11-49ea-4de9-8bbc-bc9ae203002c",
   "metadata": {},
   "source": [
    "The [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI) class even comes with a `bind_tools` helper function that handles converting function-like objects to the OpenAI format and binding them for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "842c9914-ac28-428f-9fcc-556177e8e715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_LCdBa4cbhMJPRdtkhDzpRh7x', 'function': {'arguments': '{\\n  \"a\": 5,\\n  \"b\": 3\\n}', 'name': 'multiply'}, 'type': 'function'}]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tool = llm.bind_tools([multiply], tool_choice=\"multiply\")\n",
    "llm_with_tool.invoke(\"what's 5 times three\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e22d8-9f33-4845-9364-0d276df35ff5",
   "metadata": {},
   "source": [
    "## Legacy args `functions` and `function_call`\n",
    "\n",
    "Until Fall of 2023 the OpenAI API expected arguments `functions` and `funtion_call` instead of `tools` and `tool_choice`, and they had a slightly different format than `tools` and `tool_choice`. LangChain maintains utilities for using the old API if you need to use that as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a317f71e-177e-404b-b09c-8fb365a4d8a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'multiply',\n",
       " 'description': 'Multiply two integers together.',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'a': {'description': 'First integer', 'type': 'integer'},\n",
       "   'b': {'description': 'Second integer', 'type': 'integer'}},\n",
       "  'required': ['a', 'b']}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "convert_to_openai_function(multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd124259-75e2-4704-9f57-824d3e463bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"a\": 3,\\n  \"b\": 1000000\\n}', 'name': 'multiply'}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_functions = llm.bind(\n",
    "    functions=[convert_to_openai_function(multiply)], function_call={\"name\": \"multiply\"}\n",
    ")\n",
    "llm_with_functions.invoke(\"what's 3 times a million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9a90af9-1c81-4ace-b155-1589f7308a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"a\": 3,\\n  \"b\": 1000000\\n}', 'name': 'multiply'}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_functions = llm.bind_functions([multiply], function_call=\"multiply\")\n",
    "llm_with_functions.invoke(\"what's 3 times a million\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779808d-d75c-4d76-890d-ba8c6c571514",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "* **Output parsing**: See [OpenAI Tools output parsers](/docs/modules/model_io/output_parsers/types/openai_tools) and [OpenAI Functions output parsers](/docs/modules/model_io/output_parsers/types/openai_functions) to learn about extracting the function calling API responses into various formats.\n",
    "* **Tool use**: See how to construct chains and agents that actually call the invoked tools in [these guides](/docs/use_cases/tool_use/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-venv-2",
   "language": "python",
   "name": "poetry-venv-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
