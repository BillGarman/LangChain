{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "922a7a98-7d73-4a1a-8860-76a33451d1be",
   "metadata": {},
   "source": [
    "# GraphDB QA Chain\n",
    "\n",
    "This notebook shows how to use LLMs to provide natural language querying (NLQ to SPARQL, also called text2sparql) for [GraphDB](https://graphdb.ontotext.com/). Ontotext GraphDB is a graph database and knowledge discovery tool compliant with [RDF](https://www.w3.org/RDF/) and [SPARQL](https://www.w3.org/TR/sparql11-query/).\n",
    "\n",
    "## GraphDB LLM Functionalities\n",
    "\n",
    "GraphDB supports some LLM integration functionalities as described in https://github.com/w3c/sparql-dev/issues/193:\n",
    "\n",
    "[gpt-queries](https://graphdb.ontotext.com/documentation/10.5/gpt-queries.html)\n",
    "- magic predicates to ask an LLM for text, list or table using data from your knowledge graph (KG)\n",
    "- query explanation\n",
    "- result explanation, summarization, rephrasing, translation\n",
    "\n",
    "[retrieval-graphdb-connector](https://graphdb.ontotext.com/documentation/10.5/retrieval-graphdb-connector.html)\n",
    "- Indexing of KG entities in a vector database\n",
    "- Supports any text embedding algorithm and vector database\n",
    "- Uses the same powerful connector (indexing) language that GraphDB uses for Elastic, Solr, Lucene\n",
    "- Automatic synchronization of changes in RDF data to the KG entity index\n",
    "- Supports nested objects (no UI support in GraphDB versions <= 10.5 as of yet)\n",
    "- Serializes KG entities to text like this (e.g. for a Wines dataset):\n",
    "\n",
    "        Franvino:\n",
    "        - is a RedWine.\n",
    "        - made from grape Merlo.\n",
    "        - made from grape Cabernet Franc.\n",
    "        - has sugar dry.\n",
    "        - has year 2012.\n",
    "\n",
    "[talk-to-graph](https://graphdb.ontotext.com/documentation/10.5/talk-to-graph.html)\n",
    "- A simple chatbot using a defined KG entity index\n",
    "\n",
    "## Querying the GraphDB Database\n",
    "\n",
    "For this tutorial, we won't use the GraphDB LLM integration, but SPARQL generation from NLQ. We'll use the Star Wars API (SWAPI) ontology and dataset that you can examine [here](https://drive.google.com/file/d/1wQ2K4uZp4eq3wlJ6_F_TxkOolaiczdYp/view?usp=drive_link).\n",
    "\n",
    "You will need to have a running GraphDB instance. This tutorial shows how to run the database locally using the [GraphDB Docker image](https://hub.docker.com/r/ontotext/graphdb). It provides a docker compose set-up, which populates GraphDB with the Star Wars dataset. All nessessary files including this notebook can be downloaded from GDrive. \n",
    "\n",
    "### Set-up\n",
    "\n",
    "- Install [Docker](https://docs.docker.com/get-docker/). This tutorial is created using Docker version `24.0.7` which bundles [Docker Compose](https://docs.docker.com/compose/). For earlier Docker versions you may need to install Docker Compose separately.\n",
    "- Download all files from [GDrive](https://drive.google.com/drive/folders/18dN7WQxfGu26Z9C9HUU5jBwDuPnVTLbl) in a local folder on your machine.\n",
    "- Start GraphDB with the following script executed from this folder\n",
    "```\n",
    "docker build --tag graphdb .\n",
    "docker compose up -d graphdb\n",
    "```\n",
    "You need to wait a couple of seconds for the database to start on `http://localhost:7200/`. The Star Wars dataset `starwars-data.trig` is automatically loaded into the `langchain` repository. The local SPARQL endpoint `http://localhost:7200/repositories/langchain` can be used to run queries against. You can also open the GraphDB Workbench from your favourite web browser `http://localhost:7200/sparql` where you can make queries interactively.\n",
    "- Working environment\n",
    "\n",
    "If you use `conda`, create and activate a new conda env (e.g. `conda create -n graph_graphdb_qa python=3.9.18`).\n",
    "Install the following libraries:\n",
    "\n",
    "```\n",
    "pip install jupyter==1.0.0\n",
    "pip install openai==0.28.0\n",
    "pip install rdflib==6.3.2\n",
    "pip install langchain-openai==0.0.2.post1\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "Run Jupyter with\n",
    "```\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51b397c-2fdc-4b99-9fed-1ab2b6ef7547",
   "metadata": {},
   "source": [
    "### Specifying the Ontology\n",
    "\n",
    "In order for the LLM to be able to generate SPARQL, it needs to know the knowledge graph schema (the ontology). It can be provided using one of two parameters on the `GraphDBGraph` class:\n",
    "- `query_ontology`: a `CONSTRUCT` query that is executed on the SPARQL endpoint and returns the KG schema statements. We recommend that you store the ontology in its own named graph, which will make it easier to get only the relevant statements (as the example below). `DESCRIBE` queries are not supported, because `DESCRIBE` returns the Symmetric Concise Bounded Description (SCBD), i.e. also the incoming class links. In case of large graphs with a million of instances, this is not efficient. Check https://github.com/eclipse-rdf4j/rdf4j/issues/4857\n",
    "- `local_file`: a local RDF ontology file. Supported file formats are `.ttl`, `.trig`, `.xml`, `.n3`, `.nt`, `.nq` and `.jsonld`.\n",
    "\n",
    "In either case, the ontology dump should:\n",
    "- Include enough information about classes, properties, property attachment to classes (using rdfs:domain, schema:domainIncludes or OWL restrictions), and taxonomies (important individuals).\n",
    "- Not include overly verbose and irrelevant definitions and examples that do not help SPARQL construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc8792e0-acfb-4310-b5fa-8f649e448870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import GraphDBGraph\n",
    "\n",
    "# feeding the schema using a user construct query\n",
    "\n",
    "graph = GraphDBGraph(\n",
    "    query_endpoint=\"http://localhost:7200/repositories/langchain\",\n",
    "    query_ontology=\"CONSTRUCT {?s ?p ?o} FROM <https://swapi.co/ontology/> WHERE {?s ?p ?o}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a08b8d8c-af01-4401-8069-5f2cd022a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeding the schema using a local RDF file\n",
    "\n",
    "graph = GraphDBGraph(\n",
    "    query_endpoint=\"http://localhost:7200/repositories/langchain\",\n",
    "    local_file=\"/path/to/langchain_graphdb_tutorial/starwars-ontology.nt\",  # change the path here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583b26ce-fb0d-4e9c-b5cd-9ec0e3be8922",
   "metadata": {},
   "source": [
    "Either way, the ontology (schema) is fed to the LLM in `.ttl` since `.ttl` with appropriate prefixes is most compact and easiest for the LLM to remember.\n",
    "\n",
    "The Star Wars ontology is a bit unusual in that it includes a lot of specific triples about classes, e.g. that the species :Aleena live on <planet/38>, <aleena/47>, they are a subclass of :Reptile, have certain typical characteristics (height, lifespan, skinColor), and specific individuals (characters) are representatives of that class:\n",
    "\n",
    "\n",
    "```\n",
    "@prefix : <https://swapi.co/vocabulary/> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "\n",
    ":Aleena a owl:Class,\n",
    "        :Species ;\n",
    "    rdfs:label \"Aleena\" ;\n",
    "    rdfs:isDefinedBy <https://swapi.co/ontology/> ;\n",
    "    rdfs:subClassOf :Reptile,\n",
    "        :Sentient ;\n",
    "    :averageHeight 80.0 ;\n",
    "    :averageLifespan \"79\" ;\n",
    "    :character <https://swapi.co/resource/aleena/47> ;\n",
    "    :film <https://swapi.co/resource/film/4> ;\n",
    "    :language \"Aleena\" ;\n",
    "    :planet <https://swapi.co/resource/planet/38> ;\n",
    "    :skinColor \"blue\",\n",
    "        \"gray\" .\n",
    "\n",
    ":Award a owl:Class ;\n",
    "    rdfs:isDefinedBy <https://swapi.co/ontology/> .\n",
    "\n",
    ":AwardRecognition a owl:Class ;\n",
    "    rdfs:isDefinedBy <https://swapi.co/ontology/> .\n",
    "\n",
    ":Besalisk a owl:Class,\n",
    "        :Species ;\n",
    "    rdfs:label \"Besalisk\" ;\n",
    "    rdfs:isDefinedBy <https://swapi.co/ontology/> ;\n",
    "    rdfs:subClassOf :Amphibian,\n",
    "        :Sentient ;\n",
    "    :averageHeight 178.0 ;\n",
    "    :averageLifespan \"75\" ;\n",
    "    :character <https://swapi.co/resource/besalisk/71> ;\n",
    "    :eyeColor \"yellow\" ;\n",
    "    :film <https://swapi.co/resource/film/5> ;\n",
    "    :language \"besalisk\" ;\n",
    "    :planet <https://swapi.co/resource/planet/55> ;\n",
    "    :skinColor \"brown\" .\n",
    "\n",
    "    ...\n",
    "\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6277d911-b0f6-4aeb-9aa5-96416b668468",
   "metadata": {},
   "source": [
    "In order to keep this tutorial simple, we use un-secured GraphDB. If GraphDB is secured, you should set the environment variables 'GRAPHDB_USERNAME' and 'GRAPHDB_PASSWORD' before the initialization of `GraphDBGraph`.\n",
    "\n",
    "```python\n",
    "os.environ[\"GRAPHDB_USERNAME\"] = \"graphdb-user\"\n",
    "os.environ[\"GRAPHDB_PASSWORD\"] = \"graphdb-password\"\n",
    "\n",
    "graph = GraphDBGraph(\n",
    "    query_endpoint=...,\n",
    "    query_ontology=...\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d8a00-c98f-43b8-9e84-77b244f7bb24",
   "metadata": {},
   "source": [
    "### Question Answering against the StarWars Dataset\n",
    "\n",
    "We can now use the `GraphDBQAChain` to ask some questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab63d88-511d-4049-9bf0-ca8748f1fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chains import GraphDBQAChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# We'll be using an OpenAI model which requires an OpenAI API Key.\n",
    "# However, other models are available as well:\n",
    "# https://python.langchain.com/docs/integrations/chat/\n",
    "\n",
    "# Set the environment variable `OPENAI_API_KEY` to your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-***\"\n",
    "\n",
    "# Any available OpenAI model can be used here.\n",
    "# We use 'gpt-4-1106-preview' because of the bigger context window.\n",
    "# The 'gpt-4-1106-preview' model_name will deprecate in the future and will change to 'gpt-4-turbo' or similar,\n",
    "# so be sure to consult with the OpenAI API https://platform.openai.com/docs/models for the correct naming.\n",
    "\n",
    "chain = GraphDBQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0, model_name=\"gpt-4-1106-preview\"),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1730bcb4-a720-4fa5-8fb7-a6d38f849478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphDBQAChain chain...\u001b[0m\n",
      "Generated SPARQL:\n",
      "\u001b[32;1m\u001b[1;3mPREFIX ns1: <https://swapi.co/vocabulary/>\n",
      "\n",
      "SELECT ?averageHeight\n",
      "WHERE {\n",
      "    ns1:Ewok ns1:averageHeight ?averageHeight .\n",
      "}\u001b[0m\n",
      "Query results:\n",
      "\u001b[32;1m\u001b[1;3m[(rdflib.term.Literal('100.0', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#decimal')),)]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the average height of the Ewok?',\n",
       " 'result': 'The average height of an Ewok is 100.0 centimeters.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is the average height of the Ewok?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1f3d23e-38ab-49c3-b040-5edf787f268d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphDBQAChain chain...\u001b[0m\n",
      "Generated SPARQL:\n",
      "\u001b[32;1m\u001b[1;3mPREFIX ns1: <https://swapi.co/vocabulary/>\n",
      "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "\n",
      "SELECT ?description\n",
      "WHERE {\n",
      "    ns1:Wookiee rdfs:label \"Wookiee\" .\n",
      "    ns1:Wookiee ns1:desc ?description .\n",
      "}\u001b[0m\n",
      "Query results:\n",
      "\u001b[32;1m\u001b[1;3m[(rdflib.term.Literal('Wookiees (/ˈwʊkiːz/) are a fictional species of intelligent bipeds from the planet Kashyyyk in the Star Wars universe. They are taller, stronger, and hairier than humans and most (if not all) other humanoid species. The most notable Wookiee is Chewbacca, the copilot of Han Solo, who first appeared in the 1977 film Star Wars Episode IV: A New Hope.'),)]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Can you describe the Wookiees?',\n",
       " 'result': 'Wookiees are a fictional species from the Star Wars universe known for being intelligent bipeds originating from the planet Kashyyyk. They are characterized by their towering height, superior strength, and hairier appearance compared to humans and most other humanoid species. The most famous Wookiee is undoubtedly Chewbacca, who is recognized as the co-pilot of Han Solo. Chewbacca made his first appearance in the 1977 film \"Star Wars Episode IV: A New Hope.\"'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Can you describe the Wookiees?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93bd0b43-b2b7-44fd-a3c1-064368d05e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphDBQAChain chain...\u001b[0m\n",
      "Generated SPARQL:\n",
      "\u001b[32;1m\u001b[1;3mPREFIX ns1: <https://swapi.co/vocabulary/>\n",
      "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "\n",
      "SELECT ?climate\n",
      "WHERE {\n",
      "    ?planet rdfs:label \"Tatooine\" .\n",
      "    ?planet ns1:climate ?climate .\n",
      "}\u001b[0m\n",
      "Query results:\n",
      "\u001b[32;1m\u001b[1;3m[(rdflib.term.Literal('arid'),)]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the climate on Tatooine?',\n",
       " 'result': 'The climate on Tatooine is arid.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is the climate on Tatooine?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11511345-8436-4634-92c6-36f2c0dd44db",
   "metadata": {},
   "source": [
    "### Chain Modifiers\n",
    "\n",
    "The GraphDB QA chain allows prompt refinement for further improvement of your QA chain and enhancing the overall user experience of your app.\n",
    "\n",
    "\n",
    "#### \"SPARQL Generation\" Prompt\n",
    "\n",
    "The prompt is used for the SPARQL query generation based on the user question and the KG schema.\n",
    "\n",
    "- `sparql_select_prompt`\n",
    "\n",
    "    Default value - ``sparql_select_prompt=GRAPHDB_GENERATION_SELECT_PROMPT`` :  \n",
    "\n",
    "  ```\n",
    "    GRAPHDB_GENERATION_SELECT_TEMPLATE = \"\"\"Task: Generate a SPARQL SELECT statement for querying a graph database.\n",
    "    For instance, to find all email addresses of John Doe, the following query would be suitable:\n",
    "    \n",
    "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "    SELECT ?email\n",
    "    WHERE {{\n",
    "        ?person foaf:name \"John Doe\" .\n",
    "        ?person foaf:mbox ?email .\n",
    "    }}\n",
    "    \n",
    "    Instructions:\n",
    "    Use only the node types and properties provided in the schema.\n",
    "    Do not use any node types and properties that are not explicitly provided.\n",
    "    Include all necessary prefixes.\n",
    "    Schema in turtle format:\n",
    "    {schema}\n",
    "    Note: Be as concise as possible.\n",
    "    Do not include any explanations or apologies in your responses.\n",
    "    Do not include '```sparql'.\n",
    "    Do not respond to any questions that ask for anything else than for you to construct a SPARQL query.\n",
    "    Do not include any text except the SPARQL query generated.\n",
    "    \n",
    "    The question is:\n",
    "    {prompt}\"\"\"\n",
    "    \n",
    "    GRAPHDB_GENERATION_SELECT_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"schema\", \"prompt\"],\n",
    "        template=GRAPHDB_GENERATION_SELECT_TEMPLATE,\n",
    "    )\n",
    "  ```\n",
    "\n",
    "#### \"SPARQL Fix\" Prompt\n",
    "\n",
    "Sometimes, the LLM may generate a SPARQL query with syntactic errors. The chain will try to amend this by prompting the LLM to correct it a certain number of times.\n",
    "\n",
    "- `sparql_fix_select_prompt`\n",
    "\n",
    "    Default value - ``sparql_fix_select_prompt=GRAPHDB_FIX_SELECT_PROMPT`` :\n",
    "  ```\n",
    "    GRAPHDB_FIX_SELECT_TEMPLATE = \"\"\"Task: This query returns a syntactic error: \n",
    "    {parse_exception}\n",
    "    Give me an improved query that works without any explanations or apologies. Do not change the logic of the query.\n",
    "    The query is: \n",
    "    {generated_sparql}\"\"\"\n",
    "    \n",
    "    GRAPHDB_FIX_SELECT_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"parse_exception\", \"generated_sparql\"],\n",
    "        template=GRAPHDB_FIX_SELECT_TEMPLATE,\n",
    "    )\n",
    "  ```\n",
    "\n",
    "- `max_regeneration_attempts`\n",
    "  \n",
    "    Default value - ``max_regeneration_attempts=5``\n",
    "\n",
    "#### \"Answering\" Prompt\n",
    "\n",
    "The prompt is used for answering the question based on the results returned from the database and the initial user question. By default, the LLM is instructed to only use the information from the returned result(s). If the result set is empty, the LLM should inform that it can't answer the question.\n",
    "\n",
    "- `qa_prompt`\n",
    "  \n",
    "  Default value - ``qa_prompt=GRAPHDB_QA_PROMPT`` :\n",
    "  \n",
    "  ```\n",
    "    GRAPHDB_QA_TEMPLATE = \"\"\"Task: Generate a natural language response from the results of a SPARQL query.\n",
    "    You are an assistant that creates well-written and human understandable answers.\n",
    "    The information part contains the information provided, which you can use to construct an answer.\n",
    "    The information provided is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\n",
    "    Make your response sound like the information is coming from an AI assistant, but don't add any information.\n",
    "    Don't use internal knowledge to answer the question, just say you don't know if no information is available.\n",
    "    Information:\n",
    "    {context}\n",
    "    \n",
    "    Question: {prompt}\n",
    "    Helpful Answer:\"\"\"\n",
    "    GRAPHDB_QA_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"context\", \"prompt\"], template=GRAPHDB_QA_TEMPLATE\n",
    "    )\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11479165-9e6b-45b8-bc9c-bf30a3812008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of setting the `qa_prompt` to make the LLM answer correctly using common/general knowledge, if the SPARQL query returns empty set.\n",
    "\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=\"Task: Generate a natural language response from the results of a SPARQL query. \"\n",
    "    \"If the query returns an empty list, you can use common knowledge. \"\n",
    "    \"Indicate if you've used common knowledge or information from the database. \"\n",
    "    \"Information: {context} \"\n",
    "    \"Question: {prompt}\",\n",
    "    input_variables=[\"context\", \"prompt\"],\n",
    ")\n",
    "\n",
    "chain_common_knowledge = GraphDBQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0, model_name=\"gpt-4-1106-preview\"),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    qa_prompt=qa_prompt,\n",
    "    max_regeneration_attempts=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be3db4e7-7d69-4956-8dc7-5cf4100e67df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphDBQAChain chain...\u001b[0m\n",
      "Generated SPARQL:\n",
      "\u001b[32;1m\u001b[1;3mPREFIX ns1: <https://swapi.co/vocabulary/>\n",
      "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "\n",
      "SELECT ?climate\n",
      "WHERE {\n",
      "    ?person ns1:name \"Luke Skywalker\" .\n",
      "    ?person ns1:homeworld ?planet .\n",
      "    ?planet ns1:climate ?climate .\n",
      "}\u001b[0m\n",
      "Query results:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': \"What is the climate on Luke Skywalker's home planet?\",\n",
       " 'result': \"Luke Skywalker's home planet is Tatooine. Tatooine is known for its harsh, desert climate, characterized by extremely high temperatures, dry conditions, and a lack of surface water. The planet has two suns, which contribute to its arid environment. This information is based on common knowledge from the Star Wars universe, as the SPARQL query results were empty.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_common_knowledge.invoke(\"What is the climate on Luke Skywalker's home planet?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef8c073-003d-44ab-8a7b-cf45c50f6370",
   "metadata": {},
   "source": [
    "Once you're finished playing with QA with GraphDB, you can shut down the Docker environment by running\n",
    "``\n",
    "docker compose down -v --remove-orphans\n",
    "``\n",
    "from the directory with the Docker compose file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
