{
 "cells": [
  {
   "cell_type": "raw",
   "id": "df29b30a-fd27-4e08-8269-870df5631f9e",
   "metadata": {},
   "source": [
    "---\n",
    "title: Extraction\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed451cd3-0ad0-4cfc-9e96-d3dc126a3861",
   "metadata": {},
   "source": [
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/use_cases/extraction.ipynb)\n",
    "\n",
    "## Use case\n",
    "\n",
    "LLMs can be used to extract information from text. \n",
    "\n",
    "There are 3 broad approaches for information extraction using LLMs:\n",
    "\n",
    "- **Schema Based**: There are LLMs can generate output according to a provided schema. These LLMs are often used to invoke **tools**, so this mode is commonly called **\"function-calling\"** or **\"tool-calling\"**. For example, see [OpenAI's function and tool calling](https://platform.openai.com/docs/guides/function-calling)\n",
    "\n",
    "- **JSON Mode**: LLMs are forced to output valid JSON. Similar to **Schema Based** approach, except the schema is provided as part of the prompt (i.e., in text). Generally, expected to perform worse than a **Schema Based** approach. For example, see [OpenAI's JSON mode](https://platform.openai.com/docs/guides/text-generation/json-mode).\n",
    "\n",
    "- **Prompt Based**: LLMs that can follow instructions well can be instructed to generate text in a desired format. This text can then be pased using existing [Output Parsers](/docs/modules/model_io/output_parsers/) or [custom parsers](/docs/modules/model_io/output_parsers/custom) into structured format (e.g., JSON)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c552a55-44f5-4e79-8c4e-8584a5607d04",
   "metadata": {},
   "source": [
    "## Option 1: Schema Based Generation (aka tool-calling)\n",
    "\n",
    "### Quickstart\n",
    "\n",
    "Use `create_structured_output_runnable` to create a LangChain `Runnable` that supports information extraction using **tool calling**.\n",
    "\n",
    "The desired output schema can be expressed either via a `Pydantic` model or a Python dict representing OpenAI's schema for tool calling (very similar to [JsonSchema](https://json-schema.org/)).\n",
    "\n",
    "`create_structured_output_runnable` supports both `openai-tools` and `openai-functions` formats. \n",
    "\n",
    ":::{.callout-tip}\n",
    "Model providers like [Fireworks](https://fireworks.ai/) use the same format specification as OpenAI. So the formats `openai-tools` and `openai-functions` can be used with other model providers as well if they support them.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5ec7a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2907515574.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install langchain langchain-openai\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-openai \n",
    "\n",
    "# Set env var OPENAI_API_KEY or load from a .env file:\n",
    "# import dotenv\n",
    "# dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e320f5f-36a0-46de-8d57-8240bfe4aeb0",
   "metadata": {},
   "source": [
    "### Specifying Schemas\n",
    "\n",
    "There are two best practices when defining schema for a model:\n",
    "\n",
    "1. **Document** the attributes and model: This information is sent to the model and is used to improve the quality of information extraction.\n",
    "2. Build in an **\"escape-hatch\"** into the schema that allows the model to correctly **REFUSE** to extract information if no relevant information can be extracted.\n",
    "\n",
    "Let's define an example schema to extract some information about a person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd19329f-ce9b-43b6-a087-19cfddc592bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\" # <-- Doc-string for the entity Person\n",
    "    # Below we define two **optional** fields called `name` and `hair_color`.\n",
    "    # The model is allowed to provide a `null` if no relevant information is present in the text.\n",
    "    # In addition, the fields are documented using a `description`.\n",
    "    # Feel free to make the description more specific (e.g., to include an example of hair color etc.)\n",
    "    name: str = Field(..., description=\"The name of the person\")\n",
    "    hair_color: str = Field(..., description=\"The color of the peron's eyes if known\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21f3efd-3fec-4167-9774-d88719dc3c37",
   "metadata": {},
   "source": [
    ":::{.callout-tip} Document the schema!\n",
    "\n",
    "Schema documentation is sent to the LLM to help it extract information correctly!\n",
    "\n",
    "* Document all attributes using a **description**.\n",
    "* Document the model itself using a doc-string under the class.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f778ed-e693-45e0-a267-d92c72c42336",
   "metadata": {},
   "source": [
    "Test out the extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec420ae-8096-4be4-a98f-f99c8e2ec30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain.chains import create_structured_output_runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4-0125-preview\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30669c47-3117-4272-a31c-856c641545da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'runnable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBobby Smith is 5 feet tall and has blond hair.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrunnable\u001b[49m\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: text})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'runnable' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"Bobby Smith is 5 feet tall and has blond hair.\"\n",
    "runnable.invoke({'text': text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff372f-9bc0-4621-9388-cebadd8d115a",
   "metadata": {},
   "source": [
    "#### Escape Hatches\n",
    "\n",
    "**\"Escape hatches\"** allow the model to extract no information if there's no information to be extracted.\n",
    "\n",
    "If you're working on a problem where you need to extract information from across a collection of documents, providing these escape hatches will become extremely important\n",
    "\n",
    "It's extremely important to allow the model to decline to extract information if the information is not present in the content, especially if extracting information from many document\n",
    "\n",
    "\n",
    "When doing extraction, it's often helpful to mark the fields of the model as **optional** to allow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd60a0ba-6a45-4ff0-b937-ef62749ea036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Bobby Smith', hair_color='blond')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Bobby Smith is 5 feet tall and has blond hair.\"\n",
    "runnable.invoke({'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "707a1a06-cac5-4472-9439-92db79c4e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6f0f1d3-09ce-4a22-abf6-60554ee71625",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [('system', 'You are an expert extraction algorithm. '\n",
    "    'Only extract relevant information from the text. '\n",
    "    'Do not extract information that does not match the description'),\n",
    "     ('human', '{text}')]\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56245301-b2ba-4031-b5b0-6b8f921c7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = create_structured_output_runnable(Person, llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e490bb0e-5fb2-46ee-8d36-fbae723f26e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name=None, hair_color=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Earth has only 1 moon.\"\n",
    "runnable.invoke({'text': text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd21ff-27a8-4890-bb18-fc852cafb18a",
   "metadata": {},
   "source": [
    "### Specifying schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a74f3e-92aa-4ac7-96f2-ea89b8740ba8",
   "metadata": {},
   "source": [
    "A convenient way to express desired output schemas is via Pydantic. The above example specified the desired output schema via `Person`, a Pydantic model. Such schemas can be easily combined together to generate richer output formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c8fe71-0ae4-466a-b32f-001c59b62bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: Sequence[Person]\n",
    "\n",
    "\n",
    "runnable = create_structured_output_runnable(People, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa9e43-9202-4b2d-a767-e596296b3a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "People(people=[Person(person_name='Alex', person_height=5, person_hair_color='blond', dog_breed=None, dog_name=None), Person(person_name='Claudia', person_height=6, person_hair_color='brunette', dog_breed='beagle', dog_name='Harry')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"\"\"Alex is 5 feet tall and has blond hair.\n",
    "Claudia is 1 feet taller Alex and jumps higher than him.\n",
    "Claudia is a brunette and has a beagle named Harry.\"\"\"\n",
    "\n",
    "runnable.invoke(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e316ea-b74a-4512-a9ab-c5d01ff583fe",
   "metadata": {},
   "source": [
    "Note that `dog_breed` and `dog_name` are optional attributes, such that here they are extracted for Claudia and not for Alex.\n",
    "\n",
    "One can also specify the desired output format with a Python dict representing valid JsonSchema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e017ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"height\": {\"type\": \"integer\"},\n",
    "        \"hair_color\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"name\", \"height\"],\n",
    "}\n",
    "\n",
    "runnable = create_structured_output_runnable(schema, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb525991-643d-4d47-9111-a3d4364c03d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Alex', 'height': 60}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"Alex is 5 feet tall. I don't know his hair color.\"\n",
    "runnable.invoke(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d3f0d2-c9d4-4ab8-9a5a-1ddda62db6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Alex', 'height': 60, 'hair_color': 'blond'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"Alex is 5 feet tall. He is blond.\"\n",
    "runnable.invoke(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3b958",
   "metadata": {},
   "source": [
    "#### Extra information\n",
    "\n",
    "Runnables constructed via `create_structured_output_runnable` generally are capable of semantic extraction, such that they can populate information that is not explicitly enumerated in the schema.\n",
    "\n",
    "Suppose we want unspecified additional information about dogs. \n",
    "\n",
    "We can use add a placeholder for unstructured extraction, `dog_extra_info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ed3b5e6-a7f3-453e-be61-d94fc665c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"\"\"Alex is 5 feet tall and has blond hair.\n",
    "Claudia is 1 feet taller Alex and jumps higher than him.\n",
    "Claudia is a brunette and has a beagle named Harry.\n",
    "Harry likes to play with other dogs and can always be found\n",
    "playing with Milo, a border collie that lives close by.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be07928a-8022-4963-a15e-eb3097beef9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "People(people=[Person(person_name='Alex', person_height=60, person_hair_color='blond', dog_breed=None, dog_name=None, dog_extra_info=None), Person(person_name='Claudia', person_height=66, person_hair_color='brunette', dog_breed='beagle', dog_name='Harry', dog_extra_info='likes to play with other dogs, especially with Milo, a border collie')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Person(BaseModel):\n",
    "    person_name: str\n",
    "    person_height: int\n",
    "    person_hair_color: str\n",
    "    dog_breed: Optional[str]\n",
    "    dog_name: Optional[str]\n",
    "    dog_extra_info: Optional[str]\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: Sequence[Person]\n",
    "\n",
    "\n",
    "runnable = create_structured_output_runnable(People, llm)\n",
    "runnable.invoke(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a949c60",
   "metadata": {},
   "source": [
    "This gives us additional information about the dogs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed9f5e-33be-4667-aa82-af49cc874e1d",
   "metadata": {},
   "source": [
    "### Specifying extraction mode\n",
    "\n",
    "`create_structured_output_runnable` supports varying implementations of the underlying extraction under the hood, which are configured via the `mode` parameter. This parameter can be one of `\"openai-functions\"`, `\"openai-tools\"`, or `\"openai-json\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e0b00-d6e6-432d-b9b0-8d0a3c0c6572",
   "metadata": {},
   "source": [
    "#### OpenAI Functions and Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ccdbb1-cbe5-45af-87e4-dde42baee5eb",
   "metadata": {},
   "source": [
    "Some LLMs are fine-tuned to support the invocation of functions or tools. If they are given an input schema for a tool and recognize an occasion to use it, they may emit JSON output conforming to that schema. We can leverage this to drive structured data extraction from natural language.\n",
    "\n",
    "OpenAI originally released this via a [`functions` parameter in its chat completions API](https://openai.com/blog/function-calling-and-other-api-updates). This has since been deprecated in favor of a [`tools` parameter](https://platform.openai.com/docs/guides/function-calling), which can include (multiple) functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b02442-2884-4b45-a5a0-4fdac729fdb3",
   "metadata": {},
   "source": [
    "Using OpenAI Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b1c2266-b04b-4a23-83a9-da3cd2f88137",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_structured_output_runnable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m runnable \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_structured_output_runnable\u001b[49m(Person, llm, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai-functions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlex is 5 feet tall and has blond hair.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m runnable\u001b[38;5;241m.\u001b[39minvoke(text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_structured_output_runnable' is not defined"
     ]
    }
   ],
   "source": [
    "runnable = create_structured_output_runnable(Person, llm, mode=\"openai-functions\")\n",
    "\n",
    "text = \"Alex is 5 feet tall and has blond hair.\"\n",
    "runnable.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07427b-a582-4489-a486-4c24a6c3165f",
   "metadata": {},
   "source": [
    "Using OpenAI Tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ca93a-ffd9-4d37-8baa-377757405357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(person_name='Alex', person_height=152, person_hair_color='blond', dog_breed=None, dog_name=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable = create_structured_output_runnable(Person, llm, mode=\"openai-tools\")\n",
    "\n",
    "runnable.invoke(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018a8fc-1799-4c9d-b655-a66f618204b3",
   "metadata": {},
   "source": [
    "The corresponding [LangSmith trace](https://smith.langchain.com/public/04cc37a7-7a1c-4bae-b972-1cb1a642568c/r) illustrates the tool call that generated our structured output.\n",
    "\n",
    "![Image description](../../static/img/extraction_trace_tool.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2662d5-9492-4acc-935b-eb8fccebbe0f",
   "metadata": {},
   "source": [
    "#### JSON Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd98ba-c887-4c30-8c9e-896ae90ac56a",
   "metadata": {},
   "source": [
    "Some LLMs support generating JSON more generally. OpenAI implements this via a [`response_format` parameter](https://platform.openai.com/docs/guides/text-generation/json-mode) in its chat completions API.\n",
    "\n",
    "Note that this method may require explicit prompting (e.g., OpenAI requires that input messages contain the word \"json\" in some form when using this parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e4679-eadc-42c8-b882-92a600083f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(person_name='Alex', person_height=5, person_hair_color='blond', dog_breed=None, dog_name=None, dog_extra_info=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"You extract information in structured JSON formats.\n",
    "\n",
    "Extract a valid JSON blob from the user input that matches the following JSON Schema:\n",
    "\n",
    "{output_schema}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "runnable = create_structured_output_runnable(\n",
    "    Person,\n",
    "    llm,\n",
    "    mode=\"openai-json\",\n",
    "    prompt=prompt,\n",
    "    enforce_function_usage=False,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"input\": inp})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22d8262-a9b8-415c-a142-d0ee4db7ec2b",
   "metadata": {},
   "source": [
    "### Few-shot examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c75f6-99d7-4d7b-a58f-b0ea7e8f338a",
   "metadata": {},
   "source": [
    "Suppose we want to tune the behavior of our extractor. There are a few options available. For example, if we want to redact names but retain other information, we could adjust the system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d16ad6-824e-434a-906a-d94e78259d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(person_name='REDACTED', person_height=5, person_hair_color='blond', dog_breed=None, dog_name=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"\"\"You extract information in structured JSON formats.\n",
    "\n",
    "Extract a valid JSON blob from the user input that matches the following JSON Schema:\n",
    "\n",
    "{output_schema}\n",
    "\n",
    "Redact all names.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"human\", \"{input}\")]\n",
    ")\n",
    "runnable = create_structured_output_runnable(\n",
    "    Person,\n",
    "    llm,\n",
    "    mode=\"openai-json\",\n",
    "    prompt=prompt,\n",
    "    enforce_function_usage=False,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"input\": inp})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be611688-1224-4d5a-9e34-a158b3c04296",
   "metadata": {},
   "source": [
    "Few-shot examples are another, effective way to illustrate intended behavior. For instance, if we want to redact names with a specific character string, a one-shot example will convey this. We can use a `FewShotChatMessagePromptTemplate` to easily accommodate both a fixed set of examples as well as the dynamic selection of examples based on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeee951-7f73-4e24-9033-c81a08af14dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(person_name='#####', person_height=5, person_hair_color='blond', dog_breed=None, dog_name=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Samus is 6 ft tall and blonde.\",\n",
    "        \"output\": Person(\n",
    "            person_name=\"######\",\n",
    "            person_height=5,\n",
    "            person_hair_color=\"blonde\",\n",
    "        ).dict(),\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), few_shot_prompt, (\"human\", \"{input}\")]\n",
    ")\n",
    "runnable = create_structured_output_runnable(\n",
    "    Person,\n",
    "    llm,\n",
    "    mode=\"openai-json\",\n",
    "    prompt=prompt,\n",
    "    enforce_function_usage=False,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"input\": inp})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51846211-e86b-4807-9348-eb263999f7f7",
   "metadata": {},
   "source": [
    "Here, the [LangSmith trace](https://smith.langchain.com/public/6fe5e694-9c04-48f7-83ff-e541da764781/r) for the chat model call shows how the one-shot example is formatted into the prompt.\n",
    "\n",
    "![Image description](../../static/img/extraction_trace_few_shot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd9f121",
   "metadata": {},
   "source": [
    "## Option 2: Parsing\n",
    "\n",
    "[Output parsers](/docs/modules/model_io/output_parsers/) are classes that help structure language model responses. \n",
    "\n",
    "As shown above, they are used to parse the output of the runnable created by `create_structured_output_runnable`.\n",
    "\n",
    "They can also be used more generally, if a LLM is instructed to emit its output in a certain format. Parsers include convenience methods for generating formatting instructions for use in prompts.\n",
    "\n",
    "Below we implement an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64650362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    person_name: str\n",
    "    person_height: int\n",
    "    person_hair_color: str\n",
    "    dog_breed: Optional[str]\n",
    "    dog_name: Optional[str]\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: Sequence[Person]\n",
    "\n",
    "\n",
    "# Run\n",
    "query = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blond.\"\"\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=People)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Run\n",
    "_input = prompt.format_prompt(query=query)\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f3bf2-31b1-4b07-94f5-9568acf3ffdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "People(people=[Person(person_name='Alex', person_height=5, person_hair_color='blond', dog_breed=None, dog_name=None), Person(person_name='Claudia', person_height=6, person_hair_color='brunette', dog_breed=None, dog_name=None)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.invoke(_input.to_string())\n",
    "\n",
    "parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826899df",
   "metadata": {},
   "source": [
    "We can see from the [LangSmith trace](https://smith.langchain.com/public/aec42dd3-d471-4d34-801b-20dd88444931/r) that we get the same output as above.\n",
    "\n",
    "![Image description](../../static/img/extraction_trace_parsing.png)\n",
    "\n",
    "We can see that we provide a two-shot prompt in order to instruct the LLM to output in our desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c350e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why couldn't the bicycle find its way home?\", punchline='Because it lost its bearings!')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Run\n",
    "_input = prompt.format_prompt(query=joke_query)\n",
    "model = ChatOpenAI(temperature=0)\n",
    "output = model.invoke(_input.to_string())\n",
    "parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3601bde",
   "metadata": {},
   "source": [
    "As we can see, we get an output of the `Joke` class, which respects our originally desired schema: 'setup' and 'punchline'.\n",
    "\n",
    "We can look at the [LangSmith trace](https://smith.langchain.com/public/557ad630-af35-43e9-b043-93800539025f/r) to see exactly what is going on under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ae371",
   "metadata": {},
   "source": [
    "### Going deeper\n",
    "\n",
    "* The [output parser](/docs/modules/model_io/output_parsers/) documentation includes various parser examples for specific types (e.g., lists, datetime, enum, etc).  \n",
    "* The experimental [Anthropic function calling](https://python.langchain.com/docs/integrations/chat/anthropic_functions) support provides similar functionality to Anthropic chat models.\n",
    "* [LlamaCPP](https://python.langchain.com/docs/integrations/llms/llamacpp#grammars) natively supports constrained decoding using custom grammars, making it easy to output structured content using local LLMs \n",
    "* [JSONFormer](/docs/integrations/llms/jsonformer_experimental) offers another way for structured decoding of a subset of the JSON Schema.\n",
    "* [Kor](https://eyurtsev.github.io/kor/) is another library for extraction where schema and examples can be provided to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab95ecf",
   "metadata": {},
   "source": [
    "## Extracting from different input formats\n",
    "\n",
    "There are many types of data we may wish to extract from. We will look into examples of extracting information from binary files and longer documents that do not fit into the model context window.\n",
    "\n",
    "### Longer Documents\n",
    "\n",
    "In the case of extracting from a very long text file, we want to split the content using langchain provided tools and then deduplicate the data from the resulting extractions.\n",
    "\n",
    "Below we implement an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3879ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "with open(r\"./test.txt\", \"r\") as fp:\n",
    "    doc_content = fp.read().strip()\n",
    "\n",
    "runnable = create_structured_output_runnable(People, llm, mode=\"openai-tools\")\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(doc_content)\n",
    "extract_responses = runnable.batch(texts)\n",
    "dedup_responses = []\n",
    "for response in extract_responses:\n",
    "    for person in response.people:\n",
    "        if person not in dedup_responses:\n",
    "            dedup_responses.append(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f4b6a-4829-40d1-94e7-146a2b87e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Binary Files\n",
    "\n",
    "Besides raw text data, you may wish to extract information from other file types such as powerpoints or pdfs. To do so, we can use the Blob object along with a PDF parser.\n",
    "\n",
    "A code example is below:\n",
    "\n",
    "from langchain.document_loaders.parsers import PDFMinerParser\n",
    "from langchain_community.document_loaders import Blob\n",
    "\n",
    "with open(r\"./test.pdf\", \"rb\") as fp:\n",
    "    file_data = fp.read()\n",
    "\n",
    "blob = Blob.from_data(\n",
    "    data=file_data,\n",
    "    mime_type=\"application/pdf\",\n",
    ")\n",
    "\n",
    "parser = PDFMinerParser()\n",
    "parsed_data = parser.parse(blob=blob)\n",
    "text = parsed_data[0].page_content.strip()\n",
    "\n",
    "runnable = create_structured_output_runnable(People, llm, mode=\"openai-tools\")\n",
    "runnable.invoke(text)\n",
    "\n",
    "### Mime Type Based Parsing\n",
    "\n",
    "In addition, you can choose to not specify the file and parser type directly, instead letting the code infer it from the file it gets passed.\n",
    "\n",
    "A code example of this is below:\n",
    "\n",
    "import magic\n",
    "from langchain.document_loaders.parsers import BS4HTMLParser, PDFMinerParser\n",
    "from langchain.document_loaders.parsers.generic import MimeTypeBasedParser\n",
    "from langchain.document_loaders.parsers.txt import TextParser\n",
    "\n",
    "HANDLERS = {\n",
    "    \"application/pdf\": PDFMinerParser(),\n",
    "    \"text/plain\": TextParser(),\n",
    "    \"text/html\": BS4HTMLParser(),\n",
    "}\n",
    "\n",
    "MIMETYPE_BASED_PARSER = MimeTypeBasedParser(\n",
    "    handlers=HANDLERS,\n",
    "    fallback_parser=None,\n",
    ")\n",
    "\n",
    "with open(r\"./test.html\", \"rb\") as fp:\n",
    "    file_data = fp.read()\n",
    "\n",
    "mime = magic.Magic(mime=True)\n",
    "mime_type = mime.from_buffer(file_data)\n",
    "\n",
    "blob = Blob.from_data(\n",
    "    data=file_data,\n",
    "    mime_type=mime_type,\n",
    ")\n",
    "\n",
    "parser = HANDLERS[mime_type]\n",
    "parsed_data = parser.parse(blob=blob)\n",
    "runnable.invoke(parsed_data[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
