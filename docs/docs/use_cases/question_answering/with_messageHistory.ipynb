{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6d790a3a",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 9\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a2616",
   "metadata": {
    "id": "0a254ed4"
   },
   "source": [
    "# With MessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbdcb1d",
   "metadata": {
    "id": "fbb72b56"
   },
   "source": [
    "In the previous content, we introduced how to use RAG to enhance the capabilities of the chat question and answer system.However, if we want to introduce RAG into a conversation with chat memory, so that we can enhance the customization capabilities in a conversation with chat history. In this tutorial, we will be based on the original chat record saved in Sqlite, allowing users to pass in the previous conversation [blog of lilianweng](https://lilianweng.github.io/posts/2023-06-23-agent/) data to expand RAG in chat conversations.About creating a simple conversation chain which uses ConversationEntityMemory backed by a SqliteEntityStore,please see this [doc](https://python.langchain.com/docs/integrations/memory/sqlite)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf0e7b",
   "metadata": {
    "id": "1b79e462"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41caefd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T03:18:30.908881Z",
     "start_time": "2024-03-02T03:18:30.894154Z"
    },
    "id": "8394ead4"
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef416f",
   "metadata": {
    "id": "3bf3acc9"
   },
   "source": [
    "We’ll use an OpenAI chat model, SQLChatMessageHistory with Sqlite, embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any ChatModel or LLM, Embeddings, and VectorStore or Retriever.\n",
    "\n",
    "We’ll use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baeb93b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-02T03:55:32.589Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28445,
     "status": "ok",
     "timestamp": 1709351008927,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "133f8484",
    "outputId": "8773fb72-bffb-4871-d8a3-527615f2a83a"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3190901",
   "metadata": {
    "id": "63fb6a78"
   },
   "source": [
    "We need to set environment variable OPENAI_API_KEY, which can be done directly or loaded from a .env file like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a62b24",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-02T03:20:53.509Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12323,
     "status": "ok",
     "timestamp": 1709351023982,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "1d41841f",
    "outputId": "2856131a-8ec9-433b-d9a6-3fea45d03211"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# import dotenv\n",
    "\n",
    "# dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b89e72e",
   "metadata": {
    "id": "751fcbaa"
   },
   "source": [
    "### LangSmith\n",
    "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n",
    "\n",
    "Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e11488",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-02T03:21:55.635Z"
    },
    "id": "119024ea"
   },
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bb6eac",
   "metadata": {
    "id": "855d4a67"
   },
   "source": [
    "## Chat With MessageHisotry Using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297780bc",
   "metadata": {
    "id": "07d8b2e7"
   },
   "source": [
    "### import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd1bb25",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-02T03:55:05.652Z"
    },
    "executionInfo": {
     "elapsed": 375,
     "status": "ok",
     "timestamp": 1709351103307,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "049f79c0"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fe6e76",
   "metadata": {
    "id": "b735273e"
   },
   "source": [
    "Load the file from [lilianweng's blog](https://lilianweng.github.io/posts/2023-06-23-agent/), do chunk and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c000f13",
   "metadata": {
    "executionInfo": {
     "elapsed": 3036,
     "status": "ok",
     "timestamp": 1709351109606,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "409fc990"
   },
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "bs_strainer = bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182f8e22",
   "metadata": {
    "id": "9b5d99c8"
   },
   "source": [
    "create prompt and add model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc0405f8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-02T03:34:32.822Z"
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1709351116856,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "41ff396b"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're an assistant who's good at something. Here is some {context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb3d75b",
   "metadata": {
    "id": "4ba47d1c"
   },
   "source": [
    "create the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c07e15a5",
   "metadata": {
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1709351130129,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "6f746717"
   },
   "outputs": [],
   "source": [
    "context = itemgetter(\"question\") | retriever | format_docs\n",
    "first_step = RunnablePassthrough.assign(context=context)\n",
    "chain = first_step | prompt | llm\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: SQLChatMessageHistory(\n",
    "        session_id=session_id, connection_string=\"sqlite:///sqlite.db\"\n",
    "    ),\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f709d02",
   "metadata": {
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1709351138210,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "3fdbcf1d"
   },
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"test_session\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ef0c7",
   "metadata": {
    "id": "9964a160"
   },
   "source": [
    "try the first question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4904791",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2943,
     "status": "ok",
     "timestamp": 1709351145243,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "83cfb629",
    "outputId": "aeed2858-48ac-4a85-d6b2-2cd8b7c7551e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The document mainly contains instructions for creating a software architecture for an API-Bank benchmark system. It outlines the core classes, functions, and methods required for the system, as well as provides guidance on how to structure the code across multiple files. The system is designed to evaluate the performance of tool-augmented Large Language Models (LLMs) by simulating interactions with various API tools through annotated dialogues. The APIs cover a wide range of functionalities such as search engines, calculators, calendars, smart home controls, and more. The system allows the LLM to search for the appropriate API and make calls using the API documentation.'\n"
     ]
    }
   ],
   "source": [
    "result = chain_with_history.invoke({\"question\": \"What does the document mainly contain?\"}, config)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c984431",
   "metadata": {
    "id": "cec9ba00"
   },
   "source": [
    "try the section question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2195a983",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1129,
     "status": "ok",
     "timestamp": 1709351177625,
     "user": {
      "displayName": "Picker",
      "userId": "08933220343927086323"
     },
     "user_tz": -480
    },
    "id": "01342e8a",
    "outputId": "4e9be6a9-88da-4381-c0a7-08114c6ab1c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='\"Designing an API-Bank Benchmark System for Evaluating LLM Performance\"'\n"
     ]
    }
   ],
   "source": [
    "result = chain_with_history.invoke({\"question\": \"Summarize a title for me\"}, config)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
