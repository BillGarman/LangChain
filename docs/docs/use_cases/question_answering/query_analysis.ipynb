{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a47da0d0-0927-4adb-93e6-99a434f732cf",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2195672-0cab-4967-ba8a-c6544635547d",
   "metadata": {},
   "source": [
    "# Query analysis\n",
    "\n",
    "In any question answering application we need to search for, or retrieve, information based on a user question. In the simplest case, we can search on the user input directly. This approach has a few common failure modes:\n",
    "\n",
    "* The data has multiple attributes that a user input could be referring to,\n",
    "* The user input contains multiple distinct questions in it,\n",
    "* Search quality is sensitive to phrasing.\n",
    "\n",
    "To handle these, we can do **query analysis** to translate the raw user question into a query or queries optimized for our index. To illustrate, let's build a Q&A bot over the LangChain YouTube videos that performs query analysis.\n",
    "\n",
    "**NOTE**: This guide assumes familiarity with the basic building blocks of a simple RAG application outlined in the [Quickstart](/docs/use_cases/question_answering/quickstart)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4079b57-4369-49c9-b2ad-c809b5408d7e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168ef5c-e54e-49a6-8552-5502854a6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain-community langchain-openai youtube-transcript-api pytube chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d66a45-a05c-4d22-b011-b1cdbdfc8f9c",
   "metadata": {},
   "source": [
    "#### Set environment variables\n",
    "\n",
    "We'll use OpenAI in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e2979e-a818-4b96-ac25-039336f94319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b48b8-16d7-4089-bc17-f2d240b3935a",
   "metadata": {},
   "source": [
    "### Load documents\n",
    "\n",
    "We can use the `YouTubeLoader` to load transcripts of a few LangChain videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae6921e1-3d5a-431c-9999-29a5f33201e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=pbAd8O1Lvm4\",\n",
    "    \"https://www.youtube.com/watch?v=ylrew7qb8sQ\",\n",
    "    \"https://www.youtube.com/watch?v=uRya4zRrRx4\",\n",
    "    \"https://www.youtube.com/watch?v=hvAPnpSfSGo\",\n",
    "    \"https://www.youtube.com/watch?v=ZcEMLz27sL4\",\n",
    "    \"https://www.youtube.com/watch?v=3wAON0Lqviw\",\n",
    "    \"https://www.youtube.com/watch?v=jx7xuHlfsEQ\",\n",
    "    \"https://www.youtube.com/watch?v=xn1jEjRyJ2U\",\n",
    "    \"https://www.youtube.com/watch?v=SaDzIVkYqyY\",\n",
    "    \"https://www.youtube.com/watch?v=gqhlqdawHT4\",\n",
    "    \"https://www.youtube.com/watch?v=Ce03oEotdPs\",\n",
    "    \"https://www.youtube.com/watch?v=rZus0JtRqXE\",\n",
    "    \"https://www.youtube.com/watch?v=HAn9vnJy6S4\",\n",
    "    \"https://www.youtube.com/watch?v=dA1cHGACXCo\",\n",
    "    \"https://www.youtube.com/watch?v=ZcEMLz27sL4\",\n",
    "    \"https://www.youtube.com/watch?v=hvAPnpSfSGo\",\n",
    "    \"https://www.youtube.com/watch?v=EhlPDL4QrWY\",\n",
    "    \"https://www.youtube.com/watch?v=mmBo8nlu2j0\",\n",
    "    \"https://www.youtube.com/watch?v=rQdibOsL1ps\",\n",
    "    \"https://www.youtube.com/watch?v=28lC4fqukoc\",\n",
    "    \"https://www.youtube.com/watch?v=es-9MgxB-uc\",\n",
    "    \"https://www.youtube.com/watch?v=wLRHwKuKvOE\",\n",
    "    \"https://www.youtube.com/watch?v=ObIltMaRJvY\",\n",
    "    \"https://www.youtube.com/watch?v=DjuXACWYkkU\",\n",
    "    \"https://www.youtube.com/watch?v=o7C9ld6Ln-M\",\n",
    "]\n",
    "docs = []\n",
    "for url in urls:\n",
    "    docs.extend(YoutubeLoader.from_youtube_url(url, add_video_info=True).load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e1a99ee-1078-4373-b80a-630af48bf94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'WebVoyager',\n",
       " 'LangGraph: Planning Agents',\n",
       " 'LangGraph: Multi-Agent Workflows',\n",
       " 'Streaming Events: Introducing a new `stream_events` method',\n",
       " 'LangSmith: In-Depth Platform Overview',\n",
       " 'LangSmith in 10 Minutes',\n",
       " 'RAG from scratch: Part 8 (Query Translation -- Step Back)',\n",
       " 'RAG from scratch: Part 9 (Query Translation -- HyDE)',\n",
       " 'RAG from scratch: Part 7 (Query Translation -- Decomposition - v1)',\n",
       " 'LangChain Agents with Open Source Models!',\n",
       " 'Gemini + Google Retrieval Agent from a LangChain Template',\n",
       " 'OpenGPTs',\n",
       " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve',\n",
       " 'Streaming Events: Introducing a new `stream_events` method',\n",
       " 'LangGraph: Multi-Agent Workflows',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'Auto-Prompt Builder (with Hosted LangServe)',\n",
       " 'Build a Full Stack RAG App With TypeScript',\n",
       " 'Getting Started with Multi-Modal LLMs',\n",
       " 'SQL Research Assistant',\n",
       " 'Skeleton-of-Thought: Building a New Template from Scratch',\n",
       " 'Benchmarking RAG over LangChain Docs',\n",
       " 'Building a Research Assistant from Scratch',\n",
       " 'LangServe and LangChain Templates Webinar']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845149b7-130e-4228-ac80-d0a9286ef1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hi this is Lance from Lang chain I'm going to be talking about using Lang graph to build a diverse and sophisticated rag flows so just to set the stage the basic rag flow you can see here starts with a question retrieval of relevant documents from an index which are passed into the context window of an llm for generation of an answer grounded in the ret documents so that's kind of the basic outline and we can see it's like a very linear path um in practice though you often encounter a few differ\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7748415-ddbf-4c55-a242-c28833c03caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'pbAd8O1Lvm4',\n",
       " 'title': 'Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'description': 'Unknown',\n",
       " 'view_count': 7946,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/pbAd8O1Lvm4/hq720.jpg',\n",
       " 'publish_date': '2024-02-07 00:00:00',\n",
       " 'length': 1058,\n",
       " 'author': 'LangChain'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561697c8-b848-4b12-847c-ab6a8e2d1ae6",
   "metadata": {},
   "source": [
    "We can see that along with a transcription each document also has a title, view count, publication date, and length.\n",
    "\n",
    "### Indexing documents\n",
    "\n",
    "We'll use a vector store to index our documents, and we'll chunk them first to make our retrievals more concise and precise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83734980-ee66-4d03-acbf-20e81492008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000, chunk_overlap=500, add_start_index=True\n",
    ")\n",
    "chunked_docs = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    chunked_docs, embeddings, collection_name=\"langchain_youtube\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d8d0a-5c1b-46b0-862c-a4eccfd5ae3c",
   "metadata": {},
   "source": [
    "## Retrieval without query analysis\n",
    "\n",
    "We can perform similarity search on a user question directly to find chunks relevant to the question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09435e9b-57b4-41b1-b34a-449815bdfae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenGPTs\n",
      "it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good visibility into what is going on we can see here we can see the response that we got back from tavil um and then we can see um the response from the AI and so there's lots of dad jokes in here this is u\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"how do I build a RAG agent\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79ef1b-7edd-4b68-98e5-c0e4c0dd02e6",
   "metadata": {},
   "source": [
    "This works pretty well! Our first result is quite relevant to the question.\n",
    "\n",
    "Now what if we remembered that there was a video series titled \"RAG from scratch\" and wanted to find that specifically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a54de6ad-6f9f-49ca-86fd-5fcf3d318161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-reflective RAG with LangGraph: Self-RAG and CRAG\n",
      "hi this is Lance from Lang chain I'm going to be talking about using Lang graph to build a diverse and sophisticated rag flows so just to set the stage the basic rag flow you can see here starts with a question retrieval of relevant documents from an index which are passed into the context window of an llm for generation of an answer grounded in the ret documents so that's kind of the basic outline and we can see it's like a very linear path um in practice though you often encounter a few differ\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\n",
    "    \"rag from scratch\",\n",
    ")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58067b4c-847d-4ad6-be65-2ac8afb1cd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'OpenGPTs',\n",
       " 'LangServe and LangChain Templates Webinar']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[res.metadata[\"title\"] for res in search_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891e8f5-ef0c-4ec0-b25f-eda7a5350a85",
   "metadata": {},
   "source": [
    "Since we're only searching over the transcriptions, and not over titles, our search misses all of the relevant documents. \n",
    "\n",
    "What if we wanted to search for results from a specific time period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7adbfc11-ca01-4883-8978-e4f6e4a1d23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenGPTs\n",
      "2024-01-31 00:00:00\n",
      "it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good visibility into what is going on we can see here we can see the response that we got back from tavil um and then we can see um the response from the AI and so there's lots of dad jokes in here this is u\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"videos on RAG published in 2023\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].metadata[\"publish_date\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790e2db-3c6e-440b-b6e8-ebdd6600fda5",
   "metadata": {},
   "source": [
    "Our first result is from 2024, and not very relevant to the input. Since we're just searching against document contents, there's no way for the results to be filtered on any document attributes.\n",
    "\n",
    "What if we wanted to know about deploying a LangChain chain as a REST API?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d52288ab-b048-4aa2-bf91-8183e14a9709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve\n",
      " further and see um that the inputs to each of those kind of Lambda steps is going to be one of those documents and then we're outputting um like the highlights DL segment and then formatting that with our prompt template uh if you recall from when we were constructing it um so now we have kind of our uh fully constructed chain uh for our uh search enable chatbot with XF um and now let's convert that to Lang serve um so to do that we'll go back to vs code um and here we're going to um start with\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"chain as rest api\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[1500:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3996c-35d8-42d9-a92b-b8c546d35346",
   "metadata": {},
   "source": [
    "This brings up LangServe, the package for deploying chains as REST API's, as desired.\n",
    "\n",
    "But what if we added that we wanted a chain that made use of multi-modal models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5183436a-9236-4b09-8774-08186dadc4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Events: Introducing a new `stream_events` method\n",
      "streaming is uh an incredibly important ux consideration for building L Ms in a few ways first of all even if you're just working with a single llm call it can often take a while and you might want to stream individual tokens to the user so they can see what's happening as the llm responds second of all a lot of the things that we build in the laying chain are more complicated chains or agents and so being able to stream the intermediate steps what tool are being called what the input to those t\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\n",
    "    \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    ")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c362d2-ec2d-4e0f-a122-130e809229cc",
   "metadata": {},
   "source": [
    "Our first result ends up not being about LangServe or multi-modal models. In reality \"chains as rest API\" and \"using multi-modal models\" are two fairly distinct questions that should be queried for separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57396e23-c192-4d97-846b-5eacea4d6b8d",
   "metadata": {},
   "source": [
    "## Query analysis\n",
    "\n",
    "To handle these failure modes we can perform **query analysis**. Specifically, we can perform:\n",
    "\n",
    "* **Query structuring**: If our documents have multiple searchable/filterable attributes, we can infer from any raw user question which specific attributes should be searched/filtered over. For example, when a user input specific something about video publication date, that should become a filter on the `publish_date` attribute of each document.\n",
    "* **Query decomposition**: If a user input contains multiple distinct questions, we can decompose the input into separate queries.\n",
    "* **Query expansion**: If an index is sensitive to query phrasing, we can multiple paraphrased versions of the user question to increase our chances of retrieving a relevant result.\n",
    "\n",
    "To do this we'll define a query schema and use a function-calling model to convert a user question into a structured query or queries. The structured nature of the query schema allows us to do query structuring and routing, and the fact that we can extract multiple of these \n",
    "allows us to do decomposition and expansion.\n",
    "\n",
    "### Query schema\n",
    "In this case we'll have explicit min and max attributes for view count, publication date, and video length so that those can be filtered on. And we'll add separate attributes for searches against the transcript contents versus the video title. We'll also add some sorting attributes that we'll touch on later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b51dd76-820d-41a4-98c8-893f6fe0d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Literal, Optional, Tuple\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class TutorialSearch(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    content_search: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    title_search: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Alternate version of the content search query to apply to video titles. \"\n",
    "            \"Should be succinct and only include key words that could be in a video \"\n",
    "            \"title.\"\n",
    "        ),\n",
    "    )\n",
    "    min_view_count: Optional[int] = Field(\n",
    "        None, description=\"Minimum view count filter, inclusive.\"\n",
    "    )\n",
    "    max_view_count: Optional[int] = Field(\n",
    "        None, description=\"Maximum view count filter, exclusive.\"\n",
    "    )\n",
    "    earliest_publish_date: Optional[datetime.date] = Field(\n",
    "        None, description=\"Earliest publish date filter, inclusive.\"\n",
    "    )\n",
    "    latest_publish_date: Optional[datetime.date] = Field(\n",
    "        None, description=\"Latest publish date filter, exclusive.\"\n",
    "    )\n",
    "    min_length_sec: Optional[int] = Field(\n",
    "        None, description=\"Minimum video length in seconds, inclusive.\"\n",
    "    )\n",
    "    max_length_sec: Optional[int] = Field(\n",
    "        None, description=\"Maximum video length in seconds, exclusive.\"\n",
    "    )\n",
    "    sort_by: Literal[\n",
    "        \"relevance\",\n",
    "        \"view_count\",\n",
    "        \"publish_date\",\n",
    "        \"length\",\n",
    "    ] = Field(\"relevance\", description=\"Attribute to sort by.\")\n",
    "    sort_order: Literal[\"ascending\", \"descending\"] = Field(\n",
    "        \"descending\", description=\"Whether to sort in ascending or descending order.\"\n",
    "    )\n",
    "    relevance_rank: int = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"The index of this search query when all generated queries are sorted by \"\n",
    "            \"the expected relevance of their results to the original user question. \"\n",
    "            \"Each query must have a distinct index. A lower rank indicates higher \"\n",
    "            \"relevance to the user question.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        for field in self.__fields__:\n",
    "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
    "                self.__fields__[field], \"default\", None\n",
    "            ):\n",
    "                print(f\"{field}: {getattr(self, field)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b08c52-1ce9-4d8b-a779-cbe8efde51d1",
   "metadata": {},
   "source": [
    "### Query generation\n",
    "\n",
    "To convert user questions to structured queries we'll make use of OpenAI's function-calling API. Since the latest OpenAI models can return multiple function invocations each turn, this approach automatically supports query expansion and decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "783c03c3-8c72-4f88-9cf4-5829ce6745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a list of database queries optimized to retrieve the most relevant results.\n",
    "\n",
    "Perform query expansion. If there are multiple common ways of phrasing a user question \\\n",
    "or common synonyms for key words in the question, make sure to return multiple versions \\\n",
    "of the query with the different phrasings.\n",
    "\n",
    "Perform query decomposition. If the user input contains a multi-part question, make \\\n",
    "sure to return a separate query for each distinct sub-question.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(\"examples\", optional=True),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([TutorialSearch])\n",
    "query_analyzer = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | PydanticToolsParser(tools=[TutorialSearch])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403517a-b8e3-44ac-b0a6-02f8305635a2",
   "metadata": {},
   "source": [
    "Let's see what queries our analyzer generates for the questions we searched earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "92bc7bac-700d-4666-b523-f0f8c3644ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: RAG from scratch\n",
      "title_search: RAG\n",
      "relevance_rank: 1\n",
      "\n",
      "content_search: Reactive Agile Governance from scratch\n",
      "title_search: Reactive Agile Governance\n",
      "relevance_rank: 2\n",
      "\n",
      "content_search: How to build RAG applications from the beginning\n",
      "title_search: RAG applications\n",
      "relevance_rank: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke(\"rag from scratch\"):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af62af17-4f90-4dbd-a8b4-dfff51f1db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: RAG\n",
      "title_search: RAG\n",
      "earliest_publish_date: 2023-01-01\n",
      "latest_publish_date: 2024-01-01\n",
      "relevance_rank: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke(\"videos on RAG published in 2023\"):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "87590c6d-edd7-4805-bf68-c906907f9291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: multi-modal models chain\n",
      "title_search: multi-modal models chain\n",
      "relevance_rank: 1\n",
      "\n",
      "content_search: chain into REST API\n",
      "title_search: chain REST API\n",
      "relevance_rank: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke(\n",
    "    \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    "):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba81f0-f00b-4656-b840-037bf4306c60",
   "metadata": {},
   "source": [
    "### Improvements: Adding examples to prompt\n",
    "\n",
    "To tune our results we can add some examples of inputs questions and gold standard output queries to our prompt. We'll focus on examples that show how to route and expand queries, to either be against titles or content, how to structure them with filters, and how to decompose them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4d00d74b-7bc7-4224-ad09-fff8e7aeeaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "171f3c37-36da-4a80-911e-d0447168b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Web Voyager? How about Gemini?\"\n",
    "queries = [\n",
    "    TutorialSearch(\n",
    "        content_search=\"what is Web Voyager\",\n",
    "        title_search=\"Web Voyager\",\n",
    "        relevance_rank=1,\n",
    "    ),\n",
    "    TutorialSearch(\n",
    "        content_search=\"What is Gemini\", title_search=\"Gemini\", relevance_rank=2\n",
    "    ),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "92523941-5aa0-4d1e-a795-1d14529b48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Have they released any chat langchain updates since 2024?\"\n",
    "queries = [\n",
    "    TutorialSearch(\n",
    "        title_search=\"chat langchain\",\n",
    "        content_search=\"chat langchain\",\n",
    "        earliest_publish_date=datetime.date(2024, 1, 1),\n",
    "        relevance_rank=1,\n",
    "    ),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "844df58a-abd3-4c06-9a59-b7eccbbefc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to build multi-agent system and stream intermediate steps from it\"\n",
    "queries = [\n",
    "    TutorialSearch(\n",
    "        content_search=\"How to build multi-agent system\",\n",
    "        title_search=\"multi-agent system\",\n",
    "        relevance_rank=1,\n",
    "    ),\n",
    "    TutorialSearch(\n",
    "        content_search=\"how to stream intermediate steps from multi-agent system\",\n",
    "        title_search=\"stream intermediate steps multi-agent system\",\n",
    "        relevance_rank=2,\n",
    "    ),\n",
    "    TutorialSearch(\n",
    "        content_search=\"how to stream intermediate steps\",\n",
    "        title_search=\"stream intermediate steps\",\n",
    "        relevance_rank=3,\n",
    "    ),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ee464-9f72-4a76-96fb-a87aeb29daa3",
   "metadata": {},
   "source": [
    "Now we need to update our prompt template and chain so that the examples are included in each prompt. Since we're working with OpenAI function-calling, we'll need to do a bit of extra structuring to send example inputs and outputs to the model. We'll create a `tool_example_to_messages` helper function to handle this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80a33517-afa5-4152-a041-55e01eadf04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Dict, List\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "\n",
    "\n",
    "def tool_example_to_messages(example: Dict) -> List[BaseMessage]:\n",
    "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
    "    openai_tool_calls = []\n",
    "    for tool_call in example[\"tool_calls\"]:\n",
    "        openai_tool_calls.append(\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool_call.__class__.__name__,\n",
    "                    \"arguments\": tool_call.json(),\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    messages.append(\n",
    "        AIMessage(content=\"\", additional_kwargs={\"tool_calls\": openai_tool_calls})\n",
    "    )\n",
    "    tool_outputs = example.get(\"tool_outputs\") or [\n",
    "        \"This is an example of a correct usage of this tool. Well done. Make sure to continue using the tool this way.\"\n",
    "    ] * len(openai_tool_calls)\n",
    "    for output, tool_call in zip(tool_outputs, openai_tool_calls):\n",
    "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
    "    return messages\n",
    "\n",
    "example_msgs = [msg for ex in examples for msg in tool_example_to_messages(ex)]\n",
    "query_analyzer_with_examples = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt.partial(examples=example_msgs)\n",
    "    | llm_with_tools\n",
    "    | PydanticToolsParser(tools=[TutorialSearch])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "82824a2b-8985-430c-817a-6c8466bddf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: rag from scratch\n",
      "title_search: rag from scratch\n",
      "relevance_rank: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer_with_examples.invoke(\"rag from scratch\"):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dc2a270d-c239-4333-8167-5d9d1b0ce7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: how to use multi-modal models in a chain\n",
      "title_search: multi-modal models chain\n",
      "relevance_rank: 1\n",
      "\n",
      "content_search: how to turn chain into a REST API\n",
      "title_search: chain REST API\n",
      "relevance_rank: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer_with_examples.invoke(\n",
    "    \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    "):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a13f6aac-90f2-4495-84c2-6ce9de53a8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: How to do extraction with agent\n",
      "title_search: extraction with agent\n",
      "relevance_rank: 1\n",
      "\n",
      "content_search: How to build agent with anthropic\n",
      "title_search: build agent anthropic\n",
      "relevance_rank: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer_with_examples.invoke(\n",
    "    \"How to do extraction with agent? How to build agent with anthropic\"\n",
    "):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c65b2f-7881-45fc-a47b-a4eaaf48245f",
   "metadata": {},
   "source": [
    "## Retrieval with query analysis\n",
    "\n",
    "Our query analysis looks pretty good; now let's try using our generated queries to actually perform retrieval. We'll define a custom retrieval lambda that takes our output queries and correctly applies them to our indexes. \n",
    "\n",
    "Before we do that, we'll also need to create a separate index since we created an additional search field, `title_search`, for searching against video titles specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87dd377-b2f8-4732-84f5-fb441f8fe3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "docstore = InMemoryStore()\n",
    "docstore.mset([(doc.metadata[\"source\"], doc) for doc in docs])\n",
    "\n",
    "title_docs = []\n",
    "for doc in docs:\n",
    "    metadata = deepcopy(doc.metadata)\n",
    "    title_docs.append(Document(metadata.pop(\"title\"), metadata=metadata))\n",
    "title_vectorstore = Chroma.from_documents(\n",
    "    title_docs, embeddings, collection_name=\"langchain_youtube_titles\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184f53b-139f-4dc8-95d5-affd65b0e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.chains.query_constructor.ir import StructuredQuery, Comparator, Comparison, Operation, Operator\n",
    "from langchain.retrievers.self_query.chroma import ChromaTranslator\n",
    "\n",
    "def query_to_chroma_filter(query: TutorialSearch) -> dict:\n",
    "    comparisons = []\n",
    "    if query.min_view_count is not None:\n",
    "        comparisons.append(Comparison(comparator=Comparator.\n",
    "    if query.max_view_count is not None:\n",
    "        ...\n",
    "    if query.min_length_sec is not None:\n",
    "        ...\n",
    "    if query.max_length_sec is not None:\n",
    "        ...\n",
    "    if query.earliest_publish_date is not None:\n",
    "        ...\n",
    "    if query.latest_publish_date is not None:\n",
    "        ...\n",
    "\n",
    "\n",
    "def content_search(query: TutorialSearch) -> List[Document]:\n",
    "    ...\n",
    "\n",
    "def title_search(query: TutorialSearch) -> List[Document]:\n",
    "    title_vectorstore.similarity_search(query.title_search, \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ad8a7-7990-4066-9228-d35a555f7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "\n",
    "retrieval = (\n",
    "    query_analyzer_with_examples\n",
    "    | RunnableMap(content_docs = \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4410a877-0242-4415-ac86-4e6642c12b1f",
   "metadata": {},
   "source": [
    "## Routing: Working with multiple indexes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc228c39-01f0-4475-b9bf-15d33033dbb7",
   "metadata": {},
   "source": [
    "## Sorting: Going beyond similarity search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "608f11cc-a24f-499a-941f-28f68cc16cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What has LangChain released lately?\"\n",
    "queries = [\n",
    "    LangChainYouTubeSearch(sort_by=\"publish_date\", relevance_rank=1),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9695291a-7f14-4eea-8a79-693bf3e0b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the most popular videos about RAG?\"\n",
    "queries = [\n",
    "    LangChainYouTubeSearch(title_search=\"RAG\", sort_by=\"view_count\", relevance_rank=1),\n",
    "    LangChainYouTubeSearch(\n",
    "        content_search=\"RAG\", sort_by=\"view_count\", relevance_rank=2\n",
    "    ),\n",
    "    LangChainYouTubeSearch(\n",
    "        content_search=\"retrieval augmented generation\",\n",
    "        sort_by=\"view_count\",\n",
    "        relevance_rank=3,\n",
    "    ),\n",
    "    LangChainYouTubeSearch(\n",
    "        content_search=\"retrieval\", sort_by=\"view_count\", relevance_rank=4\n",
    "    ),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa52db91-622d-439b-bc2d-ed4b17c350eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are some short videos about agentic systems with local models?\"\n",
    "queries = [\n",
    "    LangChainYouTubeSearch(\n",
    "        content_search=\"how to build an agent using a local model\",\n",
    "        sort_by=\"length\",\n",
    "        sort_order=\"ascending\",\n",
    "        relevance_rank=1,\n",
    "    ),\n",
    "    LangChainYouTubeSearch(\n",
    "        title_search=\"agent local model\",\n",
    "        sort_by=\"length\",\n",
    "        sort_order=\"ascending\",\n",
    "        relevance_rank=2,\n",
    "    ),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5965eb3a-9dc1-4a01-9453-87f248de046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LangChainYouTubeSearch(content='local LLMs', title=None, min_view_count=None, max_view_count=None, earliest_publish_date=None, latest_publish_date=None, min_length_sec=None, max_length_sec=None, sort_by='publish_date', sort_order='descending', relevance_rank=1)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke(\"Recent talks about local llms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a5b80f99-5733-407a-a593-8f37868c4a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LangChainYouTubeSearch(content='new videos about streaming state machines', title=None, min_view_count=None, max_view_count=None, earliest_publish_date=None, latest_publish_date=None, min_length_sec=None, max_length_sec=None, sort_by='relevance', sort_order='descending', relevance_rank=1)]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke(\"new videos about streaming state machines\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-venv-2",
   "language": "python",
   "name": "poetry-venv-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
