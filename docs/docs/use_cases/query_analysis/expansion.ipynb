{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a47da0d0-0927-4adb-93e6-99a434f732cf",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2195672-0cab-4967-ba8a-c6544635547d",
   "metadata": {},
   "source": [
    "# Expansion\n",
    "\n",
    "Information retrieval systems can be sensitive to phrasing and specific keywords. To mitigate this, one classic retrieval technique is to generate multiple paraphrased versions of a query and return results for all versions of the query. This is called **query expansion**. LLMs are a great tool for generating these alternate versions of a query.\n",
    "\n",
    "Let's take a look at how we might do query expansion for our Q&A bot over the LangChain YouTube videos, which we started in the [Quickstart](/docs/use_cases/query_analysis/quickstart)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4079b57-4369-49c9-b2ad-c809b5408d7e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168ef5c-e54e-49a6-8552-5502854a6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d66a45-a05c-4d22-b011-b1cdbdfc8f9c",
   "metadata": {},
   "source": [
    "#### Set environment variables\n",
    "\n",
    "We'll use OpenAI in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e2979e-a818-4b96-ac25-039336f94319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e1a99ee-1078-4373-b80a-630af48bf94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'WebVoyager',\n",
       " 'LangGraph: Planning Agents',\n",
       " 'LangGraph: Multi-Agent Workflows',\n",
       " 'Streaming Events: Introducing a new `stream_events` method',\n",
       " 'LangSmith: In-Depth Platform Overview',\n",
       " 'LangSmith in 10 Minutes',\n",
       " 'RAG from scratch: Part 8 (Query Translation -- Step Back)',\n",
       " 'RAG from scratch: Part 9 (Query Translation -- HyDE)',\n",
       " 'RAG from scratch: Part 7 (Query Translation -- Decomposition - v1)',\n",
       " 'LangChain Agents with Open Source Models!',\n",
       " 'Gemini + Google Retrieval Agent from a LangChain Template',\n",
       " 'OpenGPTs',\n",
       " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve',\n",
       " 'Streaming Events: Introducing a new `stream_events` method',\n",
       " 'LangGraph: Multi-Agent Workflows',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'Auto-Prompt Builder (with Hosted LangServe)',\n",
       " 'Build a Full Stack RAG App With TypeScript',\n",
       " 'Getting Started with Multi-Modal LLMs',\n",
       " 'SQL Research Assistant',\n",
       " 'Skeleton-of-Thought: Building a New Template from Scratch',\n",
       " 'Benchmarking RAG over LangChain Docs',\n",
       " 'Building a Research Assistant from Scratch',\n",
       " 'LangServe and LangChain Templates Webinar']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845149b7-130e-4228-ac80-d0a9286ef1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hi this is Lance from Lang chain I'm going to be talking about using Lang graph to build a diverse and sophisticated rag flows so just to set the stage the basic rag flow you can see here starts with a question retrieval of relevant documents from an index which are passed into the context window of an llm for generation of an answer grounded in the ret documents so that's kind of the basic outline and we can see it's like a very linear path um in practice though you often encounter a few differ\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57396e23-c192-4d97-846b-5eacea4d6b8d",
   "metadata": {},
   "source": [
    "## Query analysis\n",
    "\n",
    "To handle these failure modes we can perform **query analysis**. Specifically, we can perform:\n",
    "\n",
    "* **Query structuring**: If our documents have multiple searchable/filterable attributes, we can infer from any raw user question which specific attributes should be searched/filtered over. For example, when a user input specific something about video publication date, that should become a filter on the `publish_date` attribute of each document.\n",
    "* **Query decomposition**: If a user input contains multiple distinct questions, we can decompose the input into separate queries.\n",
    "* **Query expansion**: If an index is sensitive to query phrasing, we can multiple paraphrased versions of the user question to increase our chances of retrieving a relevant result.\n",
    "\n",
    "To do this we'll define a **query schema** and use a function-calling model to convert a user question into a structured query or queries. The structured nature of the query schema allows us to do query structuring and routing, and the fact that we can extract multiple of these \n",
    "allows us to do decomposition and expansion.\n",
    "\n",
    "### Query schema\n",
    "In this case we'll have explicit min and max attributes for view count, publication date, and video length so that those can be filtered on. And we'll add separate attributes for searches against the transcript contents versus the video title. We'll also add some sorting attributes that we'll touch on later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b51dd76-820d-41a4-98c8-893f6fe0d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Literal, Optional, Tuple\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class SubQuery(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    sub_query: str = Field(\n",
    "        ...,\n",
    "        description=\"A very specific query against the database.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b08c52-1ce9-4d8b-a779-cbe8efde51d1",
   "metadata": {},
   "source": [
    "### Query generation\n",
    "\n",
    "To convert user questions to structured queries we'll make use of OpenAI's function-calling API. Since the latest OpenAI models can return multiple function invocations each turn, this approach automatically supports query expansion and decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "783c03c3-8c72-4f88-9cf4-5829ce6745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a list of database queries optimized to retrieve the most relevant results.\n",
    "\n",
    "Perform query expansion. If there are multiple common ways of phrasing a user question \\\n",
    "or common synonyms for key words in the question, make sure to return multiple versions \\\n",
    "of the query with the different phrasings.\n",
    "\n",
    "Perform query decomposition. If the user input contains a multi-part question, make \\\n",
    "sure to return a separate query for each distinct sub-question.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(\"examples\", optional=True),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([TutorialSearch])\n",
    "query_analyzer = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | PydanticToolsParser(tools=[TutorialSearch])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403517a-b8e3-44ac-b0a6-02f8305635a2",
   "metadata": {},
   "source": [
    "Let's see what queries our analyzer generates for the questions we searched earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "92bc7bac-700d-4666-b523-f0f8c3644ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: RAG from scratch\n",
      "title_search: RAG\n",
      "relevance_rank: 1\n",
      "\n",
      "content_search: Reactive Agile Governance from scratch\n",
      "title_search: Reactive Agile Governance\n",
      "relevance_rank: 2\n",
      "\n",
      "content_search: How to build RAG applications from the beginning\n",
      "title_search: RAG applications\n",
      "relevance_rank: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke(\"rag from scratch\"):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af62af17-4f90-4dbd-a8b4-dfff51f1db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: RAG\n",
      "title_search: RAG\n",
      "earliest_publish_date: 2023-01-01\n",
      "latest_publish_date: 2024-01-01\n",
      "relevance_rank: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke(\"videos on RAG published in 2023\"):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "87590c6d-edd7-4805-bf68-c906907f9291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: multi-modal models chain\n",
      "title_search: multi-modal models chain\n",
      "relevance_rank: 1\n",
      "\n",
      "content_search: chain into REST API\n",
      "title_search: chain REST API\n",
      "relevance_rank: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke(\n",
    "    \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    "):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba81f0-f00b-4656-b840-037bf4306c60",
   "metadata": {},
   "source": [
    "### Improvements: Adding examples to the prompt\n",
    "\n",
    "To tune our results we can add some examples of inputs questions and gold standard output queries to our prompt. We'll focus on examples that show how to route and expand queries, to either be against titles or content, how to structure them with filters, and how to decompose them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4d00d74b-7bc7-4224-ad09-fff8e7aeeaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "171f3c37-36da-4a80-911e-d0447168b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Web Voyager? How about Gemini?\"\n",
    "queries = [\n",
    "    TutorialSearch(\n",
    "        content_search=\"what is Web Voyager\",\n",
    "        title_search=\"Web Voyager\",\n",
    "        relevance_rank=1,\n",
    "    ),\n",
    "    TutorialSearch(\n",
    "        content_search=\"What is Gemini\", title_search=\"Gemini\", relevance_rank=2\n",
    "    ),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "92523941-5aa0-4d1e-a795-1d14529b48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Have they released any chat langchain updates since 2024?\"\n",
    "queries = [\n",
    "    TutorialSearch(\n",
    "        title_search=\"chat langchain\",\n",
    "        content_search=\"chat langchain\",\n",
    "        earliest_publish_date=datetime.date(2024, 1, 1),\n",
    "        relevance_rank=1,\n",
    "    ),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "844df58a-abd3-4c06-9a59-b7eccbbefc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to build multi-agent system and stream intermediate steps from it\"\n",
    "queries = [\n",
    "    TutorialSearch(\n",
    "        content_search=\"How to build multi-agent system\",\n",
    "        title_search=\"multi-agent system\",\n",
    "        relevance_rank=1,\n",
    "    ),\n",
    "    TutorialSearch(\n",
    "        content_search=\"how to stream intermediate steps from multi-agent system\",\n",
    "        title_search=\"stream intermediate steps multi-agent system\",\n",
    "        relevance_rank=2,\n",
    "    ),\n",
    "    TutorialSearch(\n",
    "        content_search=\"how to stream intermediate steps\",\n",
    "        title_search=\"stream intermediate steps\",\n",
    "        relevance_rank=3,\n",
    "    ),\n",
    "]\n",
    "examples.append({\"input\": question, \"tool_calls\": queries})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ee464-9f72-4a76-96fb-a87aeb29daa3",
   "metadata": {},
   "source": [
    "Now we need to update our prompt template and chain so that the examples are included in each prompt. Since we're working with OpenAI function-calling, we'll need to do a bit of extra structuring to send example inputs and outputs to the model. We'll create a `tool_example_to_messages` helper function to handle this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80a33517-afa5-4152-a041-55e01eadf04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Dict, List\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "\n",
    "\n",
    "def tool_example_to_messages(example: Dict) -> List[BaseMessage]:\n",
    "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
    "    openai_tool_calls = []\n",
    "    for tool_call in example[\"tool_calls\"]:\n",
    "        openai_tool_calls.append(\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool_call.__class__.__name__,\n",
    "                    \"arguments\": tool_call.json(),\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    messages.append(\n",
    "        AIMessage(content=\"\", additional_kwargs={\"tool_calls\": openai_tool_calls})\n",
    "    )\n",
    "    tool_outputs = example.get(\"tool_outputs\") or [\n",
    "        \"This is an example of a correct usage of this tool. Well done. Make sure to continue using the tool this way.\"\n",
    "    ] * len(openai_tool_calls)\n",
    "    for output, tool_call in zip(tool_outputs, openai_tool_calls):\n",
    "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
    "    return messages\n",
    "\n",
    "\n",
    "example_msgs = [msg for ex in examples for msg in tool_example_to_messages(ex)]\n",
    "query_analyzer_with_examples = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt.partial(examples=example_msgs)\n",
    "    | llm_with_tools\n",
    "    | PydanticToolsParser(tools=[TutorialSearch])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "82824a2b-8985-430c-817a-6c8466bddf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: rag from scratch\n",
      "title_search: rag from scratch\n",
      "relevance_rank: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer_with_examples.invoke(\"rag from scratch\"):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dc2a270d-c239-4333-8167-5d9d1b0ce7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: how to use multi-modal models in a chain\n",
      "title_search: multi-modal models chain\n",
      "relevance_rank: 1\n",
      "\n",
      "content_search: how to turn chain into a REST API\n",
      "title_search: chain REST API\n",
      "relevance_rank: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer_with_examples.invoke(\n",
    "    \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    "):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a13f6aac-90f2-4495-84c2-6ce9de53a8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: How to do extraction with agent\n",
      "title_search: extraction with agent\n",
      "relevance_rank: 1\n",
      "\n",
      "content_search: How to build agent with anthropic\n",
      "title_search: build agent anthropic\n",
      "relevance_rank: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer_with_examples.invoke(\n",
    "    \"How to do extraction with agent? How to build agent with anthropic\"\n",
    "):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-venv-2",
   "language": "python",
   "name": "poetry-venv-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
