{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a47da0d0-0927-4adb-93e6-99a434f732cf",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 0\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2195672-0cab-4967-ba8a-c6544635547d",
   "metadata": {},
   "source": [
    "# [WIP] Quickstart\n",
    "\n",
    "## Problem\n",
    "In any question answering application we need to search for, or retrieve, information based on a user question. In the simplest case, we can search on the user input directly. This approach has a few common failure modes:\n",
    "\n",
    "* The data has multiple attributes that a user input could be referring to,\n",
    "* The user input contains multiple distinct questions in it,\n",
    "* Search quality is sensitive to phrasing.\n",
    "\n",
    "To handle these, we can do **query analysis** to translate the raw user question into a query or queries optimized for our indexes and tools. \n",
    "\n",
    ":::{.callout-note} \n",
    "This guide assumes familiarity with the basic building blocks of a simple RAG application outlined in the [Quickstart](/docs/use_cases/question_answering/quickstart).\n",
    ":::\n",
    "\n",
    ":::{.callout-note}\n",
    "We focus here on retrieval, but query analysis is useful wherever unstructured user input needs to routed, structured, or otherwise optimized for downstream use.\n",
    ":::\n",
    "\n",
    "## Solution\n",
    "\n",
    "Query analysis is the process of transforming a user input into a query optimized for your indexes or tools. This can involve any of the folowing steps:\n",
    "\n",
    "* [Query decomposition](/docs/use_cases/query_analysis/decomposition): If a user input contains multiple distinct questions, we can decompose the input into separate queries that will each be executed independently.\n",
    "* [Query expansion](/docs/use_cases/query_analysis/expansion): If an index is sensitive to query phrasing, we can multiple paraphrased versions of the user question to increase our chances of retrieving a relevant result.\n",
    "* [Query structuring](/docs/use_cases/query_analysis/structuring): If our documents have multiple searchable/filterable attributes, we can infer from any raw user question which specific attributes should be searched/filtered over. For example, when a user input specific something about video publication date, that should become a filter on the `publish_date` attribute of each document.\n",
    "* [Routing](/docs/use_cases/query_analysis/routing): If we have multiple indexes and only a subset are useful for any given user input, we can route the input to only retrieve results from the relevant ones.|\n",
    "\n",
    "To illustrate, let's build a Q&A bot over the LangChain YouTube videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4079b57-4369-49c9-b2ad-c809b5408d7e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168ef5c-e54e-49a6-8552-5502854a6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain langchain-community langchain-openai youtube-transcript-api pytube elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d66a45-a05c-4d22-b011-b1cdbdfc8f9c",
   "metadata": {},
   "source": [
    "#### Set environment variables\n",
    "\n",
    "We'll use OpenAI in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e2979e-a818-4b96-ac25-039336f94319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e12035-57bf-4395-aaa0-69cfcff1d0e9",
   "metadata": {},
   "source": [
    "### Set up integrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35227397-62d4-4593-b001-7625c167e7e6",
   "metadata": {},
   "source": [
    "We'll use Elasticsearch for our vectorstore. We can run an Elasticsearch instance locally with Docker:\n",
    "\n",
    "```bash\n",
    "docker run -p 9200:9200 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" -e \"xpack.security.http.ssl.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.9.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b48b8-16d7-4089-bc17-f2d240b3935a",
   "metadata": {},
   "source": [
    "### Load documents\n",
    "\n",
    "We can use the `YouTubeLoader` to load transcripts of a few LangChain videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6921e1-3d5a-431c-9999-29a5f33201e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=pbAd8O1Lvm4\",\n",
    "    \"https://www.youtube.com/watch?v=ylrew7qb8sQ\",\n",
    "    \"https://www.youtube.com/watch?v=uRya4zRrRx4\",\n",
    "    \"https://www.youtube.com/watch?v=hvAPnpSfSGo\",\n",
    "    \"https://www.youtube.com/watch?v=wLRHwKuKvOE\",\n",
    "    \"https://www.youtube.com/watch?v=ObIltMaRJvY\",\n",
    "    \"https://www.youtube.com/watch?v=DjuXACWYkkU\",\n",
    "    \"https://www.youtube.com/watch?v=o7C9ld6Ln-M\",\n",
    "    \"https://www.youtube.com/watch?v=h0OPWlEOank\",\n",
    "]\n",
    "docs = []\n",
    "for url in urls:\n",
    "    docs.extend(YoutubeLoader.from_youtube_url(url, add_video_info=True).load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7da456-3023-4f04-bba1-f7e2c468c7fe",
   "metadata": {},
   "source": [
    "Here are the titles of the videos we've loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e1a99ee-1078-4373-b80a-630af48bf94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'WebVoyager',\n",
       " 'LangGraph: Planning Agents',\n",
       " 'LangGraph: Multi-Agent Workflows',\n",
       " 'Skeleton-of-Thought: Building a New Template from Scratch',\n",
       " 'Benchmarking RAG over LangChain Docs',\n",
       " 'Building a Research Assistant from Scratch',\n",
       " 'LangServe and LangChain Templates Webinar',\n",
       " 'RAG from scratch: Part 7 (Query Translation -- Decomposition)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a71032-14c3-4517-aa9a-3a5e88eaeb92",
   "metadata": {},
   "source": [
    "Here's the metadata associated with each video. We can see that each document also has a title, view count, publication date, and length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7748415-ddbf-4c55-a242-c28833c03caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'pbAd8O1Lvm4',\n",
       " 'title': 'Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'description': 'Unknown',\n",
       " 'view_count': 7946,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/pbAd8O1Lvm4/hq720.jpg',\n",
       " 'publish_date': '2024-02-07 00:00:00',\n",
       " 'length': 1058,\n",
       " 'author': 'LangChain'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db72331-1e79-4910-8faa-473a0e370277",
   "metadata": {},
   "source": [
    "And here's a sample from a document's contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845149b7-130e-4228-ac80-d0a9286ef1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hi this is Lance from Lang chain I'm going to be talking about using Lang graph to build a diverse and sophisticated rag flows so just to set the stage the basic rag flow you can see here starts with a question retrieval of relevant documents from an index which are passed into the context window of an llm for generation of an answer grounded in the ret documents so that's kind of the basic outline and we can see it's like a very linear path um in practice though you often encounter a few differ\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561697c8-b848-4b12-847c-ab6a8e2d1ae6",
   "metadata": {},
   "source": [
    "### Indexing documents\n",
    "\n",
    "Whenever we perform retrieval we need to create an index of documents that we can query. We'll use a vector store to index our documents, and we'll chunk them first to make our retrievals more concise and precise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83734980-ee66-4d03-acbf-20e81492008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma, ElasticsearchStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# clean up metadata\n",
    "for doc in docs:\n",
    "    doc.metadata[\"publish_date\"] = datetime.datetime.strptime(\n",
    "        doc.metadata[\"publish_date\"], \"%Y-%m-%d %H:%M:%S\"\n",
    "    ).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000, chunk_overlap=500, add_start_index=True\n",
    ")\n",
    "chunked_docs = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = ElasticsearchStore.from_documents(\n",
    "    chunked_docs,\n",
    "    embeddings,\n",
    "    index_name=\"langchain_youtube_3\",\n",
    "    es_url=\"http://localhost:9200\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d8d0a-5c1b-46b0-862c-a4eccfd5ae3c",
   "metadata": {},
   "source": [
    "## Retrieval without query analysis\n",
    "\n",
    "We can perform similarity search on a user question directly to find chunks relevant to the question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09435e9b-57b4-41b1-b34a-449815bdfae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-reflective RAG with LangGraph: Self-RAG and CRAG\n",
      "hi this is Lance from Lang chain I'm going to be talking about using Lang graph to build a diverse and sophisticated rag flows so just to set the stage the basic rag flow you can see here starts with a question retrieval of relevant documents from an index which are passed into the context window of an llm for generation of an answer grounded in the ret documents so that's kind of the basic outline and we can see it's like a very linear path um in practice though you often encounter a few differ\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"how do I build a RAG agent\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79ef1b-7edd-4b68-98e5-c0e4c0dd02e6",
   "metadata": {},
   "source": [
    "This works pretty well! Our first result is quite relevant to the question.\n",
    "\n",
    "Now what if we remembered that there was a video series titled \"RAG from scratch\" and wanted to find that specifically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a54de6ad-6f9f-49ca-86fd-5fcf3d318161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-reflective RAG with LangGraph: Self-RAG and CRAG\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\n",
    "    \"rag from scratch\",\n",
    ")\n",
    "print(search_results[0].metadata[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58067b4c-847d-4ad6-be65-2ac8afb1cd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'LangServe and LangChain Templates Webinar',\n",
       " 'Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'WebVoyager']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[res.metadata[\"title\"] for res in search_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891e8f5-ef0c-4ec0-b25f-eda7a5350a85",
   "metadata": {},
   "source": [
    "Since we're only searching over the transcriptions, and not over titles, our search misses all of the relevant documents. \n",
    "\n",
    "What if we wanted to search for results from a specific time period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7adbfc11-ca01-4883-8978-e4f6e4a1d23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-reflective RAG with LangGraph: Self-RAG and CRAG\n",
      "2024-02-07\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"videos on RAG published in 2023\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].metadata[\"publish_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790e2db-3c6e-440b-b6e8-ebdd6600fda5",
   "metadata": {},
   "source": [
    "Our first result is from 2024, and not very relevant to the input. Since we're just searching against document contents, there's no way for the results to be filtered on any document attributes.\n",
    "\n",
    "What if we wanted to know about deploying a LangChain chain as a REST API?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d52288ab-b048-4aa2-bf91-8183e14a9709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a Research Assistant from Scratch\n",
      "playground as well so if we go to L serve we can see this beautiful little example app right here I'm going to copy this I'm going to now I'm just going to add this in pretty halfhazard L at the bottom um I need to pip install Ling serve it's going install a bunch of packages um takes care of that looks like I need to install fast API as well easy um we can do this um what I want is I want to take this chain this is the main thing I want to serve the thing that I copy has a bunch of extraneous t\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\"chain as rest api\")\n",
    "print(search_results[0].metadata[\"title\"])\n",
    "print(search_results[0].page_content[-2000:-1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3996c-35d8-42d9-a92b-b8c546d35346",
   "metadata": {},
   "source": [
    "This brings up LangServe, the package for deploying chains as REST API's, as desired.\n",
    "\n",
    "But what if we added that we wanted a chain that made use of multi-modal models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5183436a-9236-4b09-8774-08186dadc4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton-of-Thought: Building a New Template from Scratch\n"
     ]
    }
   ],
   "source": [
    "search_results = vectorstore.similarity_search(\n",
    "    \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    ")\n",
    "print(search_results[0].metadata[\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c362d2-ec2d-4e0f-a122-130e809229cc",
   "metadata": {},
   "source": [
    "Our first result ends up not being about LangServe or multi-modal models. In reality \"chains as rest API\" and \"using multi-modal models\" are two fairly distinct questions that should be queried for separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57396e23-c192-4d97-846b-5eacea4d6b8d",
   "metadata": {},
   "source": [
    "## Query analysis\n",
    "\n",
    "To handle these failure modes we can perform **query analysis**. Specifically, we can perform:\n",
    "\n",
    "* **Query structuring**: If our documents have multiple searchable/filterable attributes, we can infer from any raw user question which specific attributes should be searched/filtered over. For example, when a user input specific something about video publication date, that should become a filter on the `publish_date` attribute of each document.\n",
    "* **Query decomposition**: If a user input contains multiple distinct questions, we can decompose the input into separate queries.\n",
    "* **Query expansion**: If an index is sensitive to query phrasing, we can multiple paraphrased versions of the user question to increase our chances of retrieving a relevant result.\n",
    "\n",
    "To do this we'll define a **query schema** and use a function-calling model to convert a user question into a structured query or queries. The structured nature of the query schema allows us to do query structuring and routing, and the fact that we can extract multiple of these \n",
    "allows us to do decomposition and expansion.\n",
    "\n",
    "### Query schema\n",
    "To keep things simple we'll just have a min and max attribute for publication date so that it can be filtered on. And we'll add separate attributes for searches against the transcript contents versus the video title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b51dd76-820d-41a4-98c8-893f6fe0d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Literal, Optional, Tuple\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class TutorialSearch(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    content_search: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    title_search: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Alternate version of the content search query to apply to video titles. \"\n",
    "            \"Should be succinct and only include key words that could be in a video \"\n",
    "            \"title.\"\n",
    "        ),\n",
    "    )\n",
    "    earliest_publish_date: Optional[datetime.date] = Field(\n",
    "        None, description=\"Earliest publish date filter, inclusive.\"\n",
    "    )\n",
    "    latest_publish_date: Optional[datetime.date] = Field(\n",
    "        None, description=\"Latest publish date filter, exclusive.\"\n",
    "    )\n",
    "    relevance_rank: int = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"The index of this search query when all generated queries are sorted by \"\n",
    "            \"the expected relevance of their results to the original user question. \"\n",
    "            \"Each query must have a distinct index. A lower rank indicates higher \"\n",
    "            \"relevance to the user question.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        for field in self.__fields__:\n",
    "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
    "                self.__fields__[field], \"default\", None\n",
    "            ):\n",
    "                print(f\"{field}: {getattr(self, field)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b08c52-1ce9-4d8b-a779-cbe8efde51d1",
   "metadata": {},
   "source": [
    "### Query generation\n",
    "\n",
    "To convert user questions to structured queries we'll make use of OpenAI's function-calling API. Since the latest OpenAI models can return multiple function invocations each turn, this approach automatically supports query expansion and decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "783c03c3-8c72-4f88-9cf4-5829ce6745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a list of database queries optimized to retrieve the most relevant results.\n",
    "\n",
    "Perform query expansion. If there are multiple common ways of phrasing a user question \\\n",
    "or common synonyms for key words in the question, make sure to return multiple versions \\\n",
    "of the query with the different phrasings.\n",
    "\n",
    "Perform query decomposition. If the user input contains a multi-part question, make \\\n",
    "sure to return a separate query for each distinct sub-question.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([TutorialSearch])\n",
    "query_analyzer = prompt | llm_with_tools | PydanticToolsParser(tools=[TutorialSearch])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403517a-b8e3-44ac-b0a6-02f8305635a2",
   "metadata": {},
   "source": [
    "Let's see what queries our analyzer generates for the questions we searched earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "92bc7bac-700d-4666-b523-f0f8c3644ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: RAG from scratch\n",
      "title_search: RAG\n",
      "relevance_rank: 1\n",
      "\n",
      "content_search: Reactive Agile Governance from scratch\n",
      "title_search: Reactive Agile Governance\n",
      "relevance_rank: 2\n",
      "\n",
      "content_search: How to build RAG applications from the beginning\n",
      "title_search: RAG applications\n",
      "relevance_rank: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke({\"question\": \"rag from scratch\"}):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af62af17-4f90-4dbd-a8b4-dfff51f1db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: RAG\n",
      "title_search: RAG\n",
      "earliest_publish_date: 2023-01-01\n",
      "latest_publish_date: 2024-01-01\n",
      "relevance_rank: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke({\"question\": \"videos on RAG published in 2023\"}):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "87590c6d-edd7-4805-bf68-c906907f9291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: multi-modal models chain\n",
      "title_search: multi-modal models chain\n",
      "relevance_rank: 1\n",
      "\n",
      "content_search: chain into REST API\n",
      "title_search: chain REST API\n",
      "relevance_rank: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in query_analyzer.invoke(\n",
    "    {\n",
    "        \"question\": \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    "    }\n",
    "):\n",
    "    query.pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c65b2f-7881-45fc-a47b-a4eaaf48245f",
   "metadata": {},
   "source": [
    "## Retrieval with query analysis\n",
    "\n",
    "Our query analysis looks pretty good; now let's try using our generated queries to actually perform retrieval. We'll define a custom retrieval lambda that takes our output queries and correctly applies them to our indexes. \n",
    "\n",
    "Before we do that, we'll also need to create a separate index since we created an additional search field, `title_search`, for searching against video titles specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b87dd377-b2f8-4732-84f5-fb441f8fe3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "docstore = InMemoryStore()\n",
    "docstore.mset([(doc.metadata[\"source\"], doc) for doc in docs])\n",
    "\n",
    "title_docs = []\n",
    "for doc in docs:\n",
    "    metadata = deepcopy(doc.metadata)\n",
    "    title_docs.append(Document(metadata.pop(\"title\"), metadata=metadata))\n",
    "title_vectorstore = ElasticsearchStore.from_documents(\n",
    "    title_docs,\n",
    "    embeddings,\n",
    "    index_name=\"langchain_youtube_titles_3\",\n",
    "    es_url=\"http://localhost:9200\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6184f53b-139f-4dc8-95d5-affd65b0e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.chains.query_constructor.ir import (\n",
    "    Comparator,\n",
    "    Comparison,\n",
    "    Operation,\n",
    "    Operator,\n",
    "    StructuredQuery,\n",
    ")\n",
    "from langchain.retrievers.self_query.elasticsearch import ElasticsearchTranslator\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def query_to_filter(query: TutorialSearch) -> dict:\n",
    "    comparisons = []\n",
    "    if query.earliest_publish_date is not None:\n",
    "        comparisons.append(\n",
    "            Comparison(\n",
    "                comparator=Comparator.GTE,\n",
    "                attribute=\"publish_date\",\n",
    "                value={\"type\": \"date\", \"date\": query.earliest_publish_date},\n",
    "            )\n",
    "        )\n",
    "    if query.latest_publish_date is not None:\n",
    "        comparisons.append(\n",
    "            Comparison(\n",
    "                comparator=Comparator.LT,\n",
    "                attribute=\"publish_date\",\n",
    "                value={\"type\": \"date\", \"date\": query.latest_publish_date},\n",
    "            )\n",
    "        )\n",
    "    if comparisons:\n",
    "        filter = Operation(operator=Operator.AND, arguments=comparisons)\n",
    "        return ElasticsearchTranslator().visit_operation(filter)\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def content_search(input: dict) -> List[Document]:\n",
    "    return vectorstore.similarity_search_with_score(\n",
    "        input[\"query\"].content_search, filter=input[\"filter\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def title_search(input: dict) -> List[Document]:\n",
    "    title_docs_scores = title_vectorstore.similarity_search_with_score(\n",
    "        input[\"query\"].title_search, filter=input[\"filter\"]\n",
    "    )\n",
    "    docs = docstore.mget(\n",
    "        [title_doc.metadata[\"source\"] for title_doc, _ in title_docs_scores]\n",
    "    )\n",
    "    scores = [scores for _, scores in title_docs_scores]\n",
    "    return list(zip(docs, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "232ad8a7-7990-4066-9228-d35a555f7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queries_and_filters(queries: List[TutorialSearch]) -> List[Dict]:\n",
    "    return [{\"query\": q, \"filter\": query_to_filter(q)} for q in queries]\n",
    "\n",
    "\n",
    "search = RunnablePassthrough.assign(\n",
    "    content_docs=content_search, title_docs=title_search\n",
    ")\n",
    "retrieval = query_analyzer_with_examples | queries_and_filters | search.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "028b0203-ac01-42b9-a630-492729609e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retrieval.invoke(\"rag from scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b7ebbd3e-08b5-4f7d-85a5-4a005f9e9dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'OpenGPTs',\n",
       " 'LangServe and LangChain Templates Webinar']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for result in results for doc, _ in result[\"content_docs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b46bb84d-57f9-49fb-86eb-52459d953c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'RAG from scratch: Part 8 (Query Translation -- Step Back)',\n",
       " 'Build a Full Stack RAG App With TypeScript',\n",
       " 'RAG from scratch: Part 7 (Query Translation -- Decomposition - v1)']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for result in results for doc, _ in result[\"title_docs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "35040f41-ba34-4079-ab96-f23568a09515",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retrieval.invoke(\n",
    "    \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "38f10523-7811-4596-b78e-02acbbe3e5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Streaming Events: Introducing a new `stream_events` method',\n",
       " 'Streaming Events: Introducing a new `stream_events` method',\n",
       " 'Getting Started with Multi-Modal LLMs',\n",
       " 'LangChain Agents with Open Source Models!',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'Building a Research Assistant from Scratch',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for result in results for doc, _ in result[\"content_docs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5c8d8ec8-42c8-40aa-90db-fc7ed851dcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain Agents with Open Source Models!',\n",
       " 'Getting Started with Multi-Modal LLMs',\n",
       " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve',\n",
       " 'LangGraph: Multi-Agent Workflows',\n",
       " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
       " 'Benchmarking RAG over LangChain Docs',\n",
       " 'Build a Full Stack RAG App With TypeScript',\n",
       " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata[\"title\"] for result in results for doc, _ in result[\"title_docs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e7f683b5-b1c5-4dec-b163-2efc162a2b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retrieval.invoke(\"RAG tutorial published in 2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1ad52512-b3e8-42a3-8701-d9e87fb8b46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Getting Started with Multi-Modal LLMs', '2023-12-20'),\n",
       " ('LangServe and LangChain Templates Webinar', '2023-11-02'),\n",
       " ('Getting Started with Multi-Modal LLMs', '2023-12-20'),\n",
       " ('SQL Research Assistant', '2023-12-19')]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    (doc.metadata[\"title\"], doc.metadata[\"publish_date\"])\n",
    "    for result in results\n",
    "    for doc, _ in result[\"content_docs\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a90a6-b61b-45d5-96d4-920a3e455594",
   "metadata": {},
   "source": [
    "### Generation\n",
    "\n",
    "Most of the time we're doing retrieval so that we can generate an informed response. To do this we'll need to consolidate our current retrieval results. Currently we return a search result for each constructed query, and that result contains both document chunks that match the query and full documents whose titles match the query. There's a few possible ways we could combine these results:\n",
    "\n",
    "* Dump all docs in the prompt. This might work fine for models with large context windows, but could cause issues with smaller windows since the results from the title search can be quite long.\n",
    "* Compress title docs. If our title docs are too large to pass to the model, we can do some post-processing to chunk them and discard all but the most relevant chunks.\n",
    "* Use title docs to rerank chunked docs. We could use the returned titles to rerank the chunked docs by taking into account how relevant their titles are. We might fetch 20 chunks and 20 titles, rerank the chunks based on their content + title relevance, and then include the top 10 reranked chunks in the prompt.\n",
    "\n",
    "To keep things simple here we'll go with the first approach, which is including all results in the model prompt. To account for the fact that title docs are much longer, we'll update our retrieval to return fewer title docs than chunked docs. We'll also update our retrieval to drop any chunks which are a part of one of the title docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f3ab293a-c5ef-4be5-b0ae-0d5830512417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "def content_search(input: dict, config: RunnableConfig) -> List[Document]:\n",
    "    return vectorstore.similarity_search_with_score(\n",
    "        input[\"query\"].content_search,\n",
    "        filter=input[\"filter\"],\n",
    "        k=config[\"configurable\"][\"content_k\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def title_search(input: dict, config: RunnableConfig) -> List[Document]:\n",
    "    title_docs_scores = title_vectorstore.similarity_search_with_score(\n",
    "        input[\"query\"].title_search,\n",
    "        filter=input[\"filter\"],\n",
    "        k=config[\"configurable\"][\"title_k\"],\n",
    "    )\n",
    "    docs = docstore.mget(\n",
    "        [title_doc.metadata[\"source\"] for title_doc, _ in title_docs_scores]\n",
    "    )\n",
    "    scores = [scores for _, scores in title_docs_scores]\n",
    "    return list(zip(docs, scores))\n",
    "\n",
    "\n",
    "def dedup(input: dict):\n",
    "    titles = [doc.metadata[\"source\"] for doc, _ in input[\"title_docs\"]]\n",
    "    content_docs = []\n",
    "    for doc, score in input[\"content_docs\"]:\n",
    "        if doc.metadata[\"source\"] not in titles:\n",
    "            content_docs.append((doc, score))\n",
    "    input[\"content_docs\"] = content_docs\n",
    "    return input\n",
    "\n",
    "\n",
    "search = (\n",
    "    RunnablePassthrough.assign(content_docs=content_search, title_docs=title_search)\n",
    "    | dedup\n",
    ")\n",
    "\n",
    "retrieval = query_analyzer_with_examples | queries_and_filters | search.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0fafd316-2d57-4c2c-8b73-d410bcf157b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 3)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = retrieval.invoke(\n",
    "    \"what's RAG\", config={\"configurable\": {\"content_k\": 10, \"title_k\": 3}}\n",
    ")\n",
    "len(results[0][\"content_docs\"]), len(results[0][\"title_docs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63070dde-384a-457e-b606-139a6a5db79e",
   "metadata": {},
   "source": [
    "Now we can set up the actual question-answering portion of our Q&A application. Again, for simplicity we'll just dump all the retrieved documents into a single prompt and let the model answer based off that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2508ecf1-cfe3-4fa8-8ce9-78e2ed1b9bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def format_docs(input: list):\n",
    "    docs = []\n",
    "    for query_results in input:\n",
    "        docs.extend(query_results[\"content_docs\"])\n",
    "        docs.extend(query_results[\"title_docs\"])\n",
    "    return \"\\n\\n\".join(doc.page_content for doc, _ in docs)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user question ONLY using the following up-to-date context. Do not worry about your last update date:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-4-0125-preview\", temperature=0)\n",
    "parser = StrOutputParser()\n",
    "qa_chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": retrieval | format_docs}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5f38f004-4954-4a9e-b435-0371017ccd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2024, the LangGraph team has been actively working on enhancing the capabilities and applications of LangGraph, focusing on multi-agent workflows, collaboration, and hierarchical agent teams. Here are some key updates and developments:\n",
      "\n",
      "1. **Multi-Agent Collaboration**: The team introduced concepts for multi-agent collaboration, where multiple agents work on the same state of messages, sharing information and tasks to achieve complex goals. This approach allows for a more collaborative and efficient problem-solving process, where agents can leverage each other's strengths.\n",
      "\n",
      "2. **Agent Supervisor Model**: A new model called the \"Agent Supervisor\" was introduced, where a supervisor agent oversees the operation of multiple independent agents. Each agent works on its task and returns the final answer to the supervisor, which then decides the next course of action. This model allows for a clear separation of concerns and can enable more complex workflows by combining the outputs of various agents.\n",
      "\n",
      "3. **Hierarchical Agent Teams**: The team explored hierarchical agent teams, where agents are organized in a hierarchical structure, with each agent or team of agents handling specific parts of a larger task. This structure allows for the decomposition of complex tasks into manageable sub-tasks, which can be tackled by specialized agents or teams.\n",
      "\n",
      "4. **Enhanced Debugging and Observability with LangSmith**: Integration with LangSmith has been improved to provide better debugging and observability for multi-agent workflows created with LangGraph. This allows developers to track the execution of agents, understand the flow of information between agents, and debug issues more effectively.\n",
      "\n",
      "5. **Expanded Toolset for Agents**: The toolset available for agents to use within LangGraph workflows has been expanded, including new tools for data retrieval, processing, and analysis. This expansion enables agents to perform a wider range of tasks and handle more complex workflows.\n",
      "\n",
      "6. **Improved Performance and Scalability**: Efforts have been made to improve the performance and scalability of LangGraph, ensuring that it can handle larger and more complex multi-agent workflows efficiently. This includes optimizations in the execution of agent workflows and enhancements in the underlying infrastructure.\n",
      "\n",
      "7. **Community Contributions and Templates**: The LangGraph team has encouraged community contributions, leading to a richer set of templates and examples for building multi-agent workflows. These community-contributed resources provide valuable starting points and inspiration for developers creating their own workflows.\n",
      "\n",
      "8. **Educational Resources and Documentation**: The team has released new educational resources, including tutorials, videos, and documentation, to help developers get started with LangGraph and build effective multi-agent workflows. These resources cover best practices, design patterns, and case studies to illustrate the capabilities of LangGraph.\n",
      "\n",
      "These updates reflect the LangGraph team's commitment to advancing the state of multi-agent systems and providing developers with powerful tools to build complex, collaborative workflows."
     ]
    }
   ],
   "source": [
    "for chunk in qa_chain.stream(\n",
    "    \"what are some updates the team has posted about LangGraph in 2024\",\n",
    "    config={\"configurable\": {\"content_k\": 10, \"title_k\": 3}},\n",
    "):\n",
    "    print(chunk, flush=True, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b2abc-6a71-4be4-ac63-d2d772515825",
   "metadata": {},
   "source": [
    "And here's what the trace would look like for this call: https://smith.langchain.com/public/9a70236f-5510-443c-a9da-084d9d07345d/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-venv-2",
   "language": "python",
   "name": "poetry-venv-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
