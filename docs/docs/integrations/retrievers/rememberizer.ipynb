{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rememberizer\n",
    "\n",
    ">[Rememberizer](https://rememberizer.ai/) is a knowledge enhancement service for AI applications created by  SkyDeck AI Inc.\n",
    "\n",
    "This notebook shows how to retrieve documents from `Rememberizer` into the Document format that is used downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "\n",
    "You will need an API key: you can get one after creating a common knowledge at [https://rememberizer.ai](https://rememberizer.ai/). Once you have an API key, you must set it as an environment variable `REMEMBERIZER_API_KEY` or pass it as `rememberizer_api_key` when initializing `RememberizerRetriever`.\n",
    "\n",
    "`RememberizerRetriever` has these arguments:\n",
    "- optional `top_k_results`: default=10. Use it to limit number of returned documents. \n",
    "- optional `rememberizer_api_key`: required if you don't set the environment variable `REMEMBERIZER_API_KEY`.\n",
    "\n",
    "`get_relevant_documents()` has one argument, `query`: free text which used to find documents in the common knowledge of `Rememberizer.ai`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "## Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API key\n",
    "from getpass import getpass\n",
    "\n",
    "REMEMBERIZER_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.retrievers import RememberizerRetriever\n",
    "\n",
    "os.environ[\"REMEMBERIZER_API_KEY\"] = REMEMBERIZER_API_KEY\n",
    "retriever = RememberizerRetriever(top_k_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(query=\"How does Large Language Models works?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 13646493,\n",
       " 'document_id': '17s3LlMbpkTk0ikvGwV0iLMCj-MNubIaP',\n",
       " 'name': 'What is a large language model (LLM)_ _ Cloudflare.pdf',\n",
       " 'type': 'application/pdf',\n",
       " 'path': '/langchain/What is a large language model (LLM)_ _ Cloudflare.pdf',\n",
       " 'url': 'https://drive.google.com/file/d/17s3LlMbpkTk0ikvGwV0iLMCj-MNubIaP/view',\n",
       " 'size': 337089,\n",
       " 'created_time': '',\n",
       " 'modified_time': '',\n",
       " 'integration': {'id': 347, 'integration_type': 'google_drive'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata  # meta-information of the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before, or contextualized in new ways. on some level they \" understand \" semantics in that they can associate words and concepts by their meaning, having seen them grouped together in that way millions or billions of times. how developers can quickly start building their own llms to build llm applications, developers need easy access to multiple data sets, and they need places for those data sets \n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:400])  # a content of the Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage in a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\") \n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What is RAG? \n",
      "\n",
      "**Answer**: RAG stands for Retrieval-Augmented Generation. It is an AI framework that retrieves facts from an external knowledge base to enhance the responses generated by large language models, such as LLMs. This helps ensure the information provided is accurate and up-to-date, as well as allows users to understand the generative process of the models. \n",
      "\n",
      "-> **Question**: How does Large Language Models works? \n",
      "\n",
      "**Answer**: Large Language Models (LLMs) work by analyzing massive datasets of language, typically gathered from the internet, to comprehend and generate human language text. LLMs are built on machine learning, specifically using a type of neural network called transformer models. These models use a mathematical technique called self-attention to understand context in human language, allowing them to interpret and generate text even in new or vague contexts. LLMs are trained via deep learning, where they learn to recognize distinctions between characters, words, and sentences without human intervention. They can be fine-tuned for specific tasks, such as interpreting questions or generating responses. Additionally, LLMs use neural networks with multiple layers, including input and output layers, to process and generate text based on the data they have been trained on. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is RAG?\",\n",
    "    \"How does Large Language Models works?\",\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
