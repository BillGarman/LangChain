{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cc6caafa",
      "metadata": {
        "id": "cc6caafa"
      },
      "source": [
        "# NVIDIA AI Playground LLM\n",
        "\n",
        ">[NVIDIA AI Playground](https://www.nvidia.com/en-us/research/ai-playground/) gives users easy access to hosted endpoints for generative AI models like Llama-2, SteerLM, Mistral, etc. Using the API, you can query NVCR (NVIDIA Container Registry) function endpoints and get quick results from a DGX-hosted cloud compute environment. All models are source-accessible and can be deployed on your own compute cluster.\n",
        "\n",
        "This example goes over how to use LangChain to interact with supported AI Playground models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MDA2as02gJVq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDA2as02gJVq",
        "outputId": "4c774147-3fc8-4a8a-fbd2-bd02198cee67"
      },
      "outputs": [],
      "source": [
        "from langchain.llms.nv_aiplay import NVCRModel, NVAIPlayClient  ## Core backbone interface clients\n",
        "from langchain.llms import NVAIPlayLLM                          ## Generic NVAIPlay Models\n",
        "from langchain.llms.nv_aiplay import LlamaLLM                   ## Llama-default NVAIPlay Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccff689e",
      "metadata": {
        "id": "ccff689e"
      },
      "source": [
        "## Setup\n",
        "\n",
        "**To get started:**\n",
        "1. Create a free account with the [NVIDIA GPU Cloud](https://catalog.ngc.nvidia.com/) service, which hosts AI solution catalogs, containers, models, etc.\n",
        "2. Navigate to `Catalog > AI Foundation Models > (Model with API endpoint)`.\n",
        "3. Select the `API` option and click `Generate Key`.\n",
        "4. Save the generated key as `NVAPI_KEY`. From there, you should have access to the endpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "sGGzUoaOgABq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGGzUoaOgABq",
        "outputId": "6989564d-cbcb-433f-d374-e3773293a25c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVAPI Key (starts with nvapi-): ··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "## API Key can be found by going to NVIDIA NGC -> AI Playground -> (some model) -> Get API Code or similar.\n",
        "## 10K free queries to any endpoint (which is a lot actually).\n",
        "\n",
        "# del os.environ['NVAPI_KEY']  ## delete\n",
        "if os.environ.get('NVAPI_KEY', '').startswith('nvapi-'):\n",
        "    print('Valid NVAPI_KEY already in environment. Delete to reset')\n",
        "else:\n",
        "    nvapi_key = getpass.getpass('NVAPI Key (starts with nvapi-): ')\n",
        "    assert nvapi_key.startswith('nvapi-'), \\\n",
        "        f\"{nvapi_key[:5]}... is not a valid key\"\n",
        "    os.environ['NVAPI_KEY'] = nvapi_key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc24d0c",
      "metadata": {
        "id": "acc24d0c"
      },
      "source": [
        "## Underlying Requests API\n",
        "\n",
        "A selection of useful models are hosted in a DGX-powered service known as NVIDIA GPU Cloud (NGC). In this service, containers with exposed model endpoints are deployed and listed on the NVIDIA Container Registry service (NVCR). These systems are accessible via simple HTTP requests and can be utilized by a variety of systems.\n",
        "\n",
        "The `NVCRModel` class implements the basic interfaces to communicate with NVCR, limiting the utility functions to those relevant for AI Playground. For example, the following list is populated by querying the function list endpoint with a key-loaded GET request:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3QizTW-Xhcs3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QizTW-Xhcs3",
        "outputId": "88069b80-2ec8-402f-8628-783273db5fac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'playground_nvolveqa_40k': '091a03bb-7364-4087-8090-bd71e9277520',\n",
              " 'playground_llama2_code_34b': 'df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8',\n",
              " 'playground_sdxl': '89848fb8-549f-41bb-88cb-95d6597044a4',\n",
              " 'playground_clip': '8c21289c-0b18-446d-8838-011b7249c513',\n",
              " 'playground_gpt_qa_8b': '0c60f14d-46cb-465e-b994-227e1c3d5047',\n",
              " 'playground_llama2_70b': '0e349b44-440a-44e1-93e9-abe8dcb27158',\n",
              " 'playground_neva_22b': '8bf70738-59b9-4e5f-bc87-7ab4203be7a0',\n",
              " 'playground_gpt_steerlm_8b': '1423ff2f-d1c7-4061-82a7-9e8c67afd43a',\n",
              " 'playground_mistral': '35ec3354-2681-4d0e-a8dd-80325dcf7c63',\n",
              " 'playground_fuyu_8b': '9f757064-657f-4c85-abd7-37a7a9b6ee11',\n",
              " 'playground_llama2_13b': 'e0bb7fb9-5333-4a27-8534-c6288f921d3f',\n",
              " 'playground_llama2_code_13b': 'f6a96af4-8bf9-4294-96d6-d71aa787612e'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NVCRModel().available_models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T7lnwJXrmM6s",
      "metadata": {
        "id": "T7lnwJXrmM6s"
      },
      "source": [
        "From this, you can easily send over a request in the style shown in the AI Playground API window for Python. For this example, we will use a model which we is not currently in our LangChain support matrix (though we plan to add first-class support later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "_0Bin73Qhp2k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0Bin73Qhp2k",
        "outputId": "a6165a72-6ab4-4e82-f119-38abfc87fdf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The image is a gray scale photograph of a checkered pattern, possibly a portion of\n",
            " a chessboard or a security camera image. The pattern consists of a series of white\n",
            " and black squares, creating a visually striking design. The squares are organized\n",
            " in a grid-like pattern, covering the entire image from top to bottom and left to\n",
            " right. The contrast between the white and black squares is quite noticeable, emphasizing\n",
            " the checkered pattern and making it the central focus of the image.\n",
            "The image is a gray scale photograph of a checkered pattern, possibly a portion of\n",
            " a chessboard or a security camera image. The pattern consists of a series of white\n",
            " and black squares, creating a visually striking design. The squares are organized\n",
            " in a grid-like pattern, covering the entire image from top to bottom and left to\n",
            " right. The contrast between the white and black squares is quite noticeable, emphasizing\n",
            " the checkered pattern and making it the central focus of the image."
          ]
        }
      ],
      "source": [
        "client = NVCRModel()\n",
        "\n",
        "model = 'neva'\n",
        "payload = {\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"content\": \"Hi! What is in this image? <img src=\\\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAIAQMAAAD+wSzIAAAABlBMVEX///+/v7+jQ3Y5AAAADklEQVQI12P4AIX8EAgALgAD/aNpbtEAAAAASUVORK5CYII==\\\" />\",\n",
        "      \"role\": \"user\"\n",
        "    },\n",
        "    {\n",
        "      \"labels\": {\n",
        "        \"creativity\": 6,\n",
        "        \"helpfulness\": 6,\n",
        "        \"humor\": 0,\n",
        "        \"quality\": 6\n",
        "      },\n",
        "      \"role\": \"assistant\"\n",
        "    }\n",
        "  ],\n",
        "  \"temperature\": 0.2,\n",
        "  \"top_p\": 0.7,\n",
        "  \"max_tokens\": 512,\n",
        "  \"stream\": True\n",
        "}\n",
        "\n",
        "def print_with_newlines(generator):\n",
        "    buffer = \"\"\n",
        "    for response in generator:\n",
        "        content = response.get('content')\n",
        "        if len(buffer) > 80 and content.startswith(' '):\n",
        "            buffer = \"\"\n",
        "            print()\n",
        "        elif content.startswith('\\n'):\n",
        "            buffer = \"\"\n",
        "        buffer += content\n",
        "        print(content, end='')\n",
        "\n",
        "## Generate-style response\n",
        "# print(client.get_req_generation(model, payload))\n",
        "# print()\n",
        "## NOTE: if an invalid name is specified, it will try to find a model that contains the provided name\n",
        "\n",
        "## Stream-style response\n",
        "print_with_newlines(client.get_req_stream(model, payload))\n",
        "print()\n",
        "\n",
        "async def print_with_newlines_async(responses):\n",
        "    buffer = \"\"\n",
        "    async for response in responses:\n",
        "        content = response['content']\n",
        "        if len(buffer) > 80 and content.startswith(' '):\n",
        "            buffer = \"\"\n",
        "            print()\n",
        "        elif '\\n' in content:\n",
        "            buffer = \"\"\n",
        "        buffer += content\n",
        "        print(content, end='')\n",
        "\n",
        "## Stream-style response\n",
        "await print_with_newlines_async(client.get_req_astream(model, payload))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m7dOxaDFoKiz",
      "metadata": {
        "id": "m7dOxaDFoKiz"
      },
      "source": [
        "As we can see, this is a general-purpose backbone API which can be built upon quite nicely to facilitate the LangChain generation/streaming/astreaming APIs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nT5FOJbdo7y4",
      "metadata": {
        "id": "nT5FOJbdo7y4"
      },
      "source": [
        "## Integration With LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "agnoxxZypQ_Z",
      "metadata": {
        "id": "agnoxxZypQ_Z"
      },
      "source": [
        "Based on this core support, we have a base connector `NVAIPlayBaseModel` which implements all of the components necessary to interface with both the `LLM` and `SimpleChatModel` classes via inheritance. This notebook will demonstrate the LLM portion with key features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6RrXHC_XqWc1",
      "metadata": {
        "id": "6RrXHC_XqWc1"
      },
      "source": [
        "### **Supported Models**\n",
        "\n",
        "Querying `available_models` will still give you all of the models offered by your API credentials:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Jdl2NUfMhi4J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdl2NUfMhi4J",
        "outputId": "11e7965c-c92a-46d6-bf1a-3a7b193e70df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['playground_mistral',\n",
              " 'playground_sdxl',\n",
              " 'playground_gpt_qa_8b',\n",
              " 'playground_clip',\n",
              " 'playground_neva_22b',\n",
              " 'playground_gpt_steerlm_8b',\n",
              " 'playground_nvolveqa_40k',\n",
              " 'playground_fuyu_8b',\n",
              " 'playground_llama2_13b',\n",
              " 'playground_llama2_code_34b',\n",
              " 'playground_llama2_code_13b',\n",
              " 'playground_llama2_70b']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NVAIPlayLLM().available_models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WMW79Iegqj4e",
      "metadata": {
        "id": "WMW79Iegqj4e"
      },
      "source": [
        "All of these models are *technically* supported and can all be accessed via `NVCRModel`, but some models have first-class LangChain support and others are more experimental.\n",
        "\n",
        "**Ready-To-Use Chat Models** have been tested and are top-priority for our LangChain support. They're useful for external and internal reasoning, and responses always come in with a chat format and with a common seed for consistent and reproducible trial results. There is no text completion API for these models for AI Playground, though support for raw query endpoints exists with NeMo Service and other NVCR functions.\n",
        "- `llama2_13b`/`llama2_70b`: Chat-trained variants of Llama-2\n",
        "- `llama2_code_13b`/`llama2_code_43b`: Code-trained variants of Llama-2\n",
        "- `mistral`: Instruction-tuned variant of Mistral.\n",
        "\n",
        "The following is a brief showcase of the generate API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bf0a425c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf0a425c",
        "outputId": "e8cbef68-928b-4619-d106-0b414485d957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm just an AI, I don't have have personal opinions or beliefs. However, I can provide\n",
            " you with some information about the best quarterbacks in the NFL based on their performance\n",
            " and achievements.\n",
            "\n",
            "There are several quarterbacks in the NFL who are widely considered to be among the\n",
            " best. Some of the most notable include:\n",
            "\n",
            "1. Tom Brady: Brady is widely considered to be one of the greatest quarterbacks of\n",
            " all time. He has won six Super Bowls with the New England Patriots and has been named\n",
            " Super Bowl MVP four times.\n",
            "2. Aaron Rodgers: Rodgers is a three-time NFL MVP and has led the Green Bay Packers\n",
            " to two Super Bowl victories. He is known for his accuracy and ability to read defenses.\n",
            "3. Drew Brees: Brees is a two-time Super Bowl MVP and has led the New Orleans Saints\n",
            " to three Super Bowl victories. He is known for his accuracy and ability to read defenses.\n",
            "4. Patrick Mahomes: Mahomes is a two-time NFL MVP and has led the Kansas City Chiefs\n",
            " to two Super Bowl victories. He is known for his speed and ability to make plays\n",
            " outside of the pocket.\n",
            "5. Russell Wilson: Wilson is a two-time NFL MVP and has led the Seattle Seahawks\n",
            " to two Super Bowl victories. He is known for his accuracy and ability to read defenses.\n",
            "\n",
            "These are just a few examples of the best quarterbacks in the NFL. There are many\n",
            " other talented quarterbacks in the league, and opinions on who is the best can vary\n",
            " depending on individual perspectives and criteria.\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms.nv_aiplay import LlamaLLM\n",
        "\n",
        "def print_with_newlines(generator):\n",
        "    buffer = \"\"\n",
        "    for content in generator:\n",
        "        if len(buffer) > 80 and content.startswith(' '):\n",
        "            buffer = \"\"\n",
        "            print()\n",
        "        elif '\\n' in content:\n",
        "            buffer = \"\"\n",
        "        buffer += content\n",
        "        print(content, end='')\n",
        "\n",
        "# Single prompt\n",
        "llm = LlamaLLM()\n",
        "print_with_newlines(llm(\"Who's the best quarterback in the NFL?\"))\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GrhSaKiyyD1l",
      "metadata": {
        "id": "GrhSaKiyyD1l"
      },
      "source": [
        "We currently also support streaming and asynchronous streaming in a similar fashion as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "nBXBmtmNx-U_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBXBmtmNx-U_",
        "outputId": "af0863fe-19d8-4ab3-96c7-6f8a698fed26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm just an AI, I don't have have personal opinions or beliefs. However, I can provide\n",
            " you with some information about the best quarterbacks in the NFL based on their performance and achievements.\n",
            "\n",
            "There are several quarterbacks in the NFL who\n",
            " are widely considered to be among the best. Some of the most notable include:\n",
            "\n",
            "1. Tom Brady: Brady is widely considered to be one of the greatest quarterbacks\n",
            " of all time. He has won six Super Bowls with the New England Patriots and has been\n",
            " named Super Bowl MVP four times.\n",
            "2. Aaron Rodgers: Rodgers is a three-time NFL MVP and has led the Green Bay Packers\n",
            " to two Super Bowl victories. He is known for his accuracy and ability to read defenses.\n",
            "3. Drew Brees: Brees is a two-time Super Bowl MVP and has led the New Orleans Saints\n",
            " to three Super Bowl victories. He is known for his accuracy and ability to read defenses.\n",
            "4. Patrick Mahomes: Mahomes is a two-time NFL MVP and has led the Kansas City Chiefs\n",
            " to two Super Bowl victories. He is known for his speed and ability to make plays\n",
            " outside of the pocket.\n",
            "5. Russell Wilson: Wilson is a two-time NFL MVP and has led the Seattle Seahawks\n",
            " to two Super Bowl victories. He is known for his accuracy and ability to read defenses.\n",
            "\n",
            "These are just a few examples of the best quarterbacks in the NFL. There are many\n",
            " other talented quarterbacks in the league, and opinions on who is the best can vary\n",
            " depending on individual perspectives and criteria."
          ]
        }
      ],
      "source": [
        "async def print_with_newlines_async(responses):\n",
        "    buffer = \"\"\n",
        "    async for content in responses:\n",
        "        if len(buffer) > 80 and content.startswith(' '):\n",
        "            buffer = \"\"\n",
        "            print()\n",
        "        elif '\\n' in content:\n",
        "            buffer = \"\"\n",
        "        buffer += content\n",
        "        print(content, end='')\n",
        "\n",
        "# print_with_newlines(llm.stream(\"Who's the best quarterback in the NFL?\"))\n",
        "await print_with_newlines_async(llm.astream(\"Who's the best quarterback in the NFL?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nG_72MV9yvZo",
      "metadata": {
        "id": "nG_72MV9yvZo"
      },
      "source": [
        "We additionally also support other utilities provided by `LLM` i.e. generate, chain invoke, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "Asr-G8BMJpbB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Asr-G8BMJpbB",
        "outputId": "59ec7161-08bf-4d5e-c499-114b0aa9f440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Generation(text='There are many amazing cartoon series that have captured the hearts of audiences around the world. Here are a few examples:\\n\\n1. \"The Simpsons\" - This long-running series follows the misadventures of the Simpson family, a working-class family living in the fictional town of Springfield.\\n2. \"South Park\" - This controversial series follows the misadventures of four foul-mouthed fourth graders')]\n",
            "[Generation(text=\"There are many great movies that have been released over the years, and it's difficult to narrow it down to a specific list. However, here are some movies that are widely considered to be great:\\n\\n1. The Shawshank Redemption (1994) - a highly acclaimed drama about the power of hope and redemption.\\n2. The Godfather (1972) - a crime drama that explores the world of organized\")]\n",
            "[Generation(text='There are many great anime series out there, but here are some that are highly recommended:\\n\\n1. Attack on Titan (2013) - a dark and suspenseful series that explores the relationship between humans and titans.\\n2. Fullmetal Alchemist: Brotherhood (2009) - a fantasy series that follows the adventures of two brothers on a quest to restore their bodies.\\n3. Death Note (')]\n"
          ]
        }
      ],
      "source": [
        "# Calling multiple prompts\n",
        "output = llm.generate(\n",
        "    [\n",
        "        \"What are some amazing cartoon series?\",\n",
        "        \"What are some great movies?\",\n",
        "        \"What are some fantastic anime series?\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "for gen in output.generations:\n",
        "    print(gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "xsgHoB0VK4kr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsgHoB0VK4kr",
        "outputId": "1c672bae-83c5-419e-bf0d-f69145ac5632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mConcurrent executed in 3.52 seconds.\u001b[0m\n",
            "\u001b[1mSerial executed in 11.69 seconds.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "## Example from https://python.langchain.com/docs/modules/model_io/llms/async_llm\n",
        "\n",
        "import asyncio\n",
        "import time\n",
        "\n",
        "\n",
        "def invoke_serially():\n",
        "    for _ in range(10):\n",
        "        resp = llm.invoke(\"Hello, how are you?\")\n",
        "\n",
        "\n",
        "async def async_invoke(llm):\n",
        "    resp = await llm.ainvoke(\"Hello, how are you?\")\n",
        "\n",
        "\n",
        "async def invoke_concurrently():\n",
        "    tasks = [async_invoke(llm) for _ in range(10)]\n",
        "    await asyncio.gather(*tasks)\n",
        "\n",
        "\n",
        "s = time.perf_counter()\n",
        "# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\n",
        "await invoke_concurrently()\n",
        "elapsed = time.perf_counter() - s\n",
        "print(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\n",
        "\n",
        "s = time.perf_counter()\n",
        "invoke_serially()\n",
        "elapsed = time.perf_counter() - s\n",
        "print(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MzZDM4IqztmZ",
      "metadata": {
        "id": "MzZDM4IqztmZ"
      },
      "source": [
        "At the same time, there are also some specific APIs that we support for the sake of convenience since the underlying requests API is chat-oriented. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "m5me4gY2z0UG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5me4gY2z0UG",
        "outputId": "6073daa9-82ab-416b-b434-e09e4d3a0978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "def fibonacci(n):\n",
            "    a, b = 0, 1\n",
            "    for i in range(n):\n",
            "        a, b = b, a + b\n",
            "    return a\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(llm(\"\"\"\n",
        "///ROLE SYS: Only generate python code. Do not add any discussions about it.\n",
        "///ROLE USER: Please implement Fibanocci in python without recursion. Your response should start and end in ```\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x9nDDwiC0aff",
      "metadata": {
        "id": "x9nDDwiC0aff"
      },
      "source": [
        "You can add your own custom support for such a system by subclassing the `NVAIPlayBaseModel` class."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "137662a6",
      "metadata": {
        "id": "137662a6"
      },
      "source": [
        "# Simple Chains"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79efa62d",
      "metadata": {
        "id": "79efa62d"
      },
      "source": [
        "You can use the LangChain Expression Language to create a simple chain with non-chat models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "fd2c6bc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd2c6bc1",
        "outputId": "056e2728-56c3-4c68-9d88-9a7bc9fadaa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's a joke about graphics:\n",
            "\n",
            "Why did the graphic designer break up with his girlfriend?\n",
            "\n",
            "Because he couldn't handle her curves!\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = LlamaLLM(\n",
        "    temperature = 0.1,\n",
        "    max_tokens = 100,\n",
        "    top_p = 1.0\n",
        ")\n",
        "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}?\")\n",
        "chain = prompt | llm\n",
        "\n",
        "print(chain.invoke({\"topic\": \"graphics\"}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "f644ff28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f644ff28",
        "outputId": "7a8d3049-61ec-4112-9d21-b9bfe66e6c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's a joke about graphics:\n",
            "\n",
            "Why did the graphic designer break up with his girlfriend?\n",
            "\n",
            "Because he couldn't handle her curves!"
          ]
        }
      ],
      "source": [
        "for token in chain.stream({\"topic\": \"graphics\"}):\n",
        "    print(token, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0a29826",
      "metadata": {
        "id": "d0a29826"
      },
      "source": [
        "In all of this, do remember that the raw completion API is not exposed in AIPlayground, so you should not include instruction formatting in your inputs."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
