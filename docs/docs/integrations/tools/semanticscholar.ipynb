{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Scholar API Tool\n",
    "\n",
    "This notebook demos how to use the semantic scholar tool with an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by installing semanticscholar api\n",
    "# !pip install semanticscholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "tools = load_tools(\n",
    "    [\"semanticscholar\"], llm=llm\n",
    ")\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "\n",
    "agent_chain.handle_parsing_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find information about biases in large language models and how people have tried to mitigate them. I also need to find a list of papers and techniques related to this topic. To break down the task into subtasks, I can first search for biases in large language models and then search for techniques to mitigate them. Finally, I can use the information from the papers to generate new research questions.\n",
      "Action: semanticscholar\n",
      "Action Input: \"biases in large language models\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished year: 2023\n",
      "Title: Biases in Large Language Models: Origins, Inventory, and Discussion\n",
      "Authors: Roberto Navigli, Simone Conia, Björn Ross\n",
      "Astract: In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstream approaches to Natural Language Processing (NLP). We first introduce data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types of social bias evidenced in the text generated by language models trained on such corpora, ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. We conclude with directions focused on measuring, reducing, and tackling the aforementioned types of bias.\n",
      "\n",
      "\n",
      "Published year: 2023\n",
      "Title: Surfacing Biases in Large Language Models using Contrastive Input Decoding\n",
      "Authors: G. Yona, Or Honovich, Itay Laish, Roee Aharoni\n",
      "Astract: Ensuring that large language models (LMs) are fair, robust and useful requires an understanding of how different modifications to their inputs impact the model's behaviour. In the context of open-text generation tasks, however, such an evaluation is not trivial. For example, when introducing a model with an input text and a perturbed,\"contrastive\"version of it, meaningful differences in the next-token predictions may not be revealed with standard decoding strategies. With this motivation in mind, we propose Contrastive Input Decoding (CID): a decoding algorithm to generate text given two inputs, where the generated text is likely given one input but unlikely given the other. In this way, the contrastive generations can highlight potentially subtle differences in how the LM output differs for the two inputs in a simple and interpretable manner. We use CID to highlight context-specific biases that are hard to detect with standard decoding strategies and quantify the effect of different input perturbations.\n",
      "\n",
      "\n",
      "Published year: 2023\n",
      "Title: Benchmarking Cognitive Biases in Large Language Models as Evaluators\n",
      "Authors: Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, Dongyeop Kang\n",
      "Astract: Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 49.6%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences. Our project page is at: https://minnesotanlp.github.io/cobbler.\n",
      "\n",
      "\n",
      "Published year: 2022\n",
      "Title: \u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found some papers related to biases in large language models. Now I need to search for techniques to mitigate these biases.\n",
      "Action: semanticscholar\n",
      "Action Input: \"techniques to mitigate biases in large language models\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished year: 2023\n",
      "Title: FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Language Models\n",
      "Authors: Hrishikesh Viswanath, Tianyi Zhang\n",
      "Astract: Studies have shown that large pretrained language models exhibit biases against social groups based on race, gender etc, which they inherit from the datasets they are trained on. Various researchers have proposed mathematical tools for quantifying and identifying these biases. There have been methods proposed to mitigate such biases. In this paper, we present a comprehensive quantitative evaluation of different kinds of biases such as race, gender, ethnicity, age etc. exhibited by popular pretrained language models such as BERT, GPT-2 etc. and also present a toolkit that provides plug-and-play interfaces to connect mathematical tools to identify biases with large pretrained language models such as BERT, GPT-2 etc. and also present users with the opportunity to test custom models against these metrics. The toolkit also allows users to debias existing and custom models using the debiasing techniques proposed so far. The toolkit is available at https://github.com/HrishikeshVish/Fairpy.\n",
      "\n",
      "\n",
      "Published year: 2021\n",
      "Title: An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models\n",
      "Authors: Nicholas Meade, Elinor Poole-Dayan, Siva Reddy\n",
      "Astract: Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model’s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.\n",
      "\n",
      "\n",
      "Published year: 2023\n",
      "Title: Large Language Model (LLM) Bias Index -- LLMBI\n",
      "Authors: Abiodun Finbarrs Oketunji, Muhammad Anas, Deepthi Saina\n",
      "Astract: The Large Language Model Bias Index (LLMBI) is a pioneering approach designed to quantify and address biases inherent in large language models (LLMs), such as GPT-4. We recognise the increasing prevalence and impact of LLMs across diverse sectors. This research introduces a novel metric, LLMBI, to systematically measure and mitigate biases potentially skewing model responses. We formulated LLMBI using a composite scoring system incorporating multiple dimensions of bias, including but not limited to age, gender, and racial biases. To operationalise this metric, we engaged in a multi-step process involving collecting and annotating LLM responses, applying sophisticated Natural Language Processing (NLP) techniques for bias detection, and computing the LLMBI score through a specially crafted mathematical formula. The formula integrates weighted averages\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found several papers related to biases in large language models and techniques to mitigate them. Now I can generate new research questions based on the information from these papers.\n",
      "Action: semanticscholar\n",
      "Action Input: \"research questions on biases in large language models\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished year: 2023\n",
      "Title: Keeping Users Engaged During Repeated Administration of the Same Questionnaire: Using Large Language Models to Reliably Diversify Questions\n",
      "Authors: Hye Sun Yun, Mehdi Arjmand, Phillip Raymond Sherlock, Michael Paasche-Orlow, James W. Griffith, Timothy Bickmore\n",
      "Astract: Standardized, validated questionnaires are vital tools in HCI research and healthcare, offering dependable self-report data. However, their repeated use in longitudinal or pre-post studies can induce respondent fatigue, impacting data quality via response biases and decreased response rates. We propose utilizing large language models (LLMs) to generate diverse questionnaire versions while retaining good psychometric properties. In a longitudinal study, participants engaged with our agent system and responded daily for two weeks to either a standardized depression questionnaire or one of two LLM-generated questionnaire variants, alongside a validated depression questionnaire. Psychometric testing revealed consistent covariation between the external criterion and the focal measure administered across the three conditions, demonstrating the reliability and validity of the LLM-generated variants. Participants found the repeated administration of the standardized questionnaire significantly more repetitive compared to the variants. Our findings highlight the potential of LLM-generated variants to invigorate questionnaires, fostering engagement and interest without compromising validity.\n",
      "\n",
      "\n",
      "Published year: 2023\n",
      "Title: Integrating UMLS Knowledge into Large Language Models for Medical Question Answering\n",
      "Authors: Rui Yang, Edison Marrese-Taylor, Yuhe Ke, Lechao Cheng, Qingyu Chen, Irene Li\n",
      "Astract: Large language models (LLMs) have demonstrated powerful text generation capabilities, bringing unprecedented innovation to the healthcare field. While LLMs hold immense promise for applications in healthcare, applying them to real clinical scenarios presents significant challenges, as these models may generate content that deviates from established medical facts and even exhibit potential biases. In our research, we develop an augmented LLM framework based on the Unified Medical Language System (UMLS), aiming to better serve the healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our benchmark models, and conduct automatic evaluations using the ROUGE Score and BERTScore on 104 questions from the LiveQA test set. Additionally, we establish criteria for physician-evaluation based on four dimensions: Factuality, Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician evaluation with 20 questions on the LiveQA test set. Multiple resident physicians conducted blind reviews to evaluate the generated content, and the results indicate that this framework effectively enhances the factuality, completeness, and relevance of generated content. Our research demonstrates the effectiveness of using UMLS-augmented LLMs and highlights the potential application value of LLMs in in medical question-answering.\n",
      "\n",
      "\n",
      "Published year: 2023\n",
      "Title: MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models\n",
      "Authors: Yifan Wei, Yisong Su, Huanhuan Ma, Xiaoyan Yu, Fangyu Lei, Yuanzhe Zhang, Jun Zhao, Kang Liu\n",
      "Astract: Large language models (LLMs) have shown nearly saturated performance on many natural language processing (NLP) tasks. As a result, it is natural for people to believe that LLMs have also mast\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found several papers related to biases in large language models and techniques to mitigate them. Based on the information from these papers, I can generate new research questions. Some potential research questions could be:\n",
      "\n",
      "1. How can we further measure and quantify biases in large language models beyond the existing metrics?\n",
      "2. What are the long-term effects of biases in large language models on downstream applications and user experiences?\n",
      "3. How can we develop more effective debiasing techniques that do not compromise the language modeling ability of the models?\n",
      "4. What are the ethical considerations and implications of using large language models with biases in real-world applications?\n",
      "5. How can we ensure transparency and accountability in the training and deployment of large language models to mitigate biases?\n",
      "6. What are the potential biases introduced by fine-tuning large language models on specific domains or datasets?\n",
      "7. How can we involve diverse perspectives and stakeholders in the evaluation and mitigation of biases in large language models?\n",
      "8. What are the potential biases introduced by the design and implementation choices of large language models, such as the choice of training data or model architecture?\n",
      "9. How can we develop techniques to mitigate biases in real-time or interactive applications that rely on large language models?\n",
      "10. What are the implications of biases in large language models for marginalized or underrepresented communities, and how can we address these concerns?\n",
      "\n",
      "Final Answer: The biases in large language models have been a topic of research, and various techniques have been proposed to mitigate them. Some potential research questions include measuring and quantifying biases, understanding the long-term effects, developing effective debiasing techniques, addressing ethical considerations, and involving diverse perspectives in the evaluation and mitigation process.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The biases in large language models have been a topic of research, and various techniques have been proposed to mitigate them. Some potential research questions include measuring and quantifying biases, understanding the long-term effects, developing effective debiasing techniques, addressing ethical considerations, and involving diverse perspectives in the evaluation and mitigation process.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(\n",
    "    \"What are some biases in the large language models? How have people tried to mitigate them? \"\n",
    "    \"show me a list of papers and techniques. Based on your findings write new research questions \"\n",
    "    \"to work on. Break down the task into subtasks for search.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
