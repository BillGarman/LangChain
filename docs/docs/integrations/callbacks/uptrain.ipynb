{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/integrations/callbacks/uptrain.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UpTrain\n",
    "\n",
    "> UpTrain [[github](https://github.com/uptrain-ai/uptrain) || [website](https://uptrain.ai/) || [docs](https://docs.uptrain.ai/getting-started/introduction)] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UpTrain Callback Handler\n",
    "\n",
    "This notebook showcases the UpTrain callback handler seamlessly integrating into your pipeline, facilitating diverse evaluations. We have chosen a few evaluations that we deemed apt for evaluating the chains. These evaluations run automatically, with results displayed in the output. More details on UpTrain's evaluations can be found [here](https://github.com/uptrain-ai/uptrain?tab=readme-ov-file#pre-built-evaluations-we-offer-). \n",
    "\n",
    "Selected retievers from Langchain are highlighted for demonstration:\n",
    "\n",
    "## 1. **RAG Query Engine Evaluations**:\n",
    "The RAG query engine plays a crucial role in retrieving context and generating responses. To ensure its performance and response quality, we conduct the following evaluations:\n",
    "\n",
    "- **Context Relevance**: Determines if the context extracted from the query is relevant to the response.\n",
    "- **Factual Accuracy**: Assesses if the LLM is hallcuinating or providing incorrect information.\n",
    "- **Response Completeness**: Checks if the response contains all the information requested by the query.\n",
    "\n",
    "## 2. **Multi Query Generation Evaluation**:\n",
    "MultiQueryRetriever creates multiple variants of a question having a similar meaning to the original question. Given the complexity, we include the previous evaluations and add:\n",
    "\n",
    "- **Multi Query Accuracy**: Assures that the multi-queries generated mean the same as the original query.\n",
    "\n",
    "## 3. **Re-Ranking Evaluations**:\n",
    "Re-ranking involves reordering nodes based on relevance to the query and choosing top n nodes. Since the number of nodes can reduce once the re-ranking is complete, we perform the following evaluations:\n",
    "\n",
    "- **Context Reranking**: Checks if the order of re-ranked nodes is more relevant to the query than the original order.\n",
    "- **Context Conciseness**: Examines whether the reduced number of nodes still provides all the required information.\n",
    "\n",
    "These evaluations collectively ensure the robustness and effectiveness of the RAG query engine, MultiQueryRetriever, and the re-ranking process in the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU uptrain langchain faiss flashrank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruvchawla/work/langchain/.venv/lib/python3.11/site-packages/lazy_loader/__init__.py:185: RuntimeWarning: subpackages can technically be lazily loaded, but it causes the package to be eagerly loaded even if it is already lazily loaded.So, you probably shouldn't use subpackages with this lazy feature.\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader, WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "from langchain.callbacks.uptrain_callback import UpTrainCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"../../modules/state_of_the_union.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Vanilla RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UpTrain callback handler will automatically capture the query, context and response once generated and will run the following three evaluations *(Graded from 0 to 1)* on the response:\n",
    "- **Context Relevance**: Check if the context extractedfrom the query is relevant to the response.\n",
    "- **Factual Accuracy**: Check how factually accurate the response is.\n",
    "- **Response Completeness**: Check if the response contains all the information that the query is asking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-03 19:03:22.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate_on_server\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <50 to the Uptrain\u001b[0m\n",
      "\u001b[32m2024-04-03 19:03:43.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m365\u001b[0m - \u001b[1mLocal server not running, start the server to log data and visualize in the dashboard!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What did the president say about Ketanji Brown Jackson\n",
      "Response: The president mentioned that he had nominated Ketanji Brown Jackson to serve on the United States Supreme Court 4 days ago. He described her as one of the nation's top legal minds who will continue Justice Breyerâ€™s legacy of excellence. He also mentioned that she is a former top litigator in private practice, a former federal public defender, and comes from a family of public school educators and police officers. He described her as a consensus builder and noted that since her nomination, she has received a broad range of support from various groups including the Fraternal Order of Police and former judges appointed by both Democrats and Republicans.\n",
      "\n",
      "Context Relevance Score: 0.9\n",
      "Factual Accuracy Score: 1.0\n",
      "Response Completeness Score: 0.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the RAG prompt\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "rag_prompt_text = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create the chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt_text\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create the uptrain callback handler\n",
    "uptrain_callback = UpTrainCallbackHandler()\n",
    "config = {\"callbacks\": [uptrain_callback]}\n",
    "\n",
    "# Invoke the chain with a query\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = chain.invoke(query, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multi Query RAG Evaluation\n",
    "\n",
    "The **MultiQueryRetriever** is used to tackle the problem that the RAG pipeline might not return the best set of documents based on the query. It generates multiple queries that mean the same as the original query and then fetches documents for each.\n",
    "\n",
    "To evluate this retriever, UpTrain will run the following evaluation:\n",
    "- **Multi Query Accuracy**: Checks if the multi-queries generated mean the same as the original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-03 19:03:52.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate_on_server\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <50 to the Uptrain\u001b[0m\n",
      "\u001b[32m2024-04-03 19:03:58.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m365\u001b[0m - \u001b[1mLocal server not running, start the server to log data and visualize in the dashboard!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What did the president say about Ketanji Brown Jackson\n",
      "Multi Queries:\n",
      "  How did the president comment on Ketanji Brown Jackson?\n",
      "  What were the president's remarks regarding Ketanji Brown Jackson?\n",
      "  Can you provide the president's statements about Ketanji Brown Jackson?\n",
      "\n",
      "Multi Query Accuracy Score: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-03 19:04:09.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate_on_server\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <50 to the Uptrain\u001b[0m\n",
      "\u001b[32m2024-04-03 19:04:41.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m365\u001b[0m - \u001b[1mLocal server not running, start the server to log data and visualize in the dashboard!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What did the president say about Ketanji Brown Jackson\n",
      "Response: The president mentioned that he had nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to serve on the United States Supreme Court 4 days ago. He described her as one of the nation's top legal minds who will continue Justice Breyerâ€™s legacy of excellence. He also mentioned that she is a former top litigator in private practice, a former federal public defender, and comes from a family of public school educators and police officers. Since her nomination, she has received a broad range of support, including from the Fraternal Order of Police and former judges appointed by both Democrats and Republicans.\n",
      "\n",
      "Context Relevance Score: 1.0\n",
      "Factual Accuracy Score: 1.0\n",
      "Response Completeness Score: 0.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the retriever\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever, llm=llm\n",
    ")\n",
    "\n",
    "# Create the uptrain callback\n",
    "uptrain_callback = UpTrainCallbackHandler()\n",
    "config = {\"callbacks\": [uptrain_callback]}\n",
    "\n",
    "# Create the RAG prompt\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "rag_prompt_text = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": multi_query_retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt_text\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invoke the chain with a query\n",
    "question = \"What did the president say about Ketanji Brown Jackson\"\n",
    "# unique_docs = multi_query_retriever.get_relevant_documents(query=question, callbacks=[uptrain_callback])\n",
    "docs = chain.invoke(question, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Context Re-Ranking\n",
    "\n",
    "The re-ranking process involves reordering nodes based on relevance to the query and choosing the top n nodes. Since the number of nodes can reduce once the re-ranking is complete, we perform the following evaluations:\n",
    "- **Context Reranking**: Check if the order of re-ranked nodes is more relevant to the query than the original order.\n",
    "- **Context Conciseness**: Check if the reduced number of nodes still provides all the required information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-03 19:04:44.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate_on_server\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <50 to the Uptrain\u001b[0m\n",
      "\u001b[32m2024-04-03 19:04:54.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m365\u001b[0m - \u001b[1mLocal server not running, start the server to log data and visualize in the dashboard!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What did the president say about Ketanji Brown Jackson\n",
      "\n",
      "Context Conciseness Score: 0.0\n",
      "Context Reranking Score: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-03 19:04:59.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate_on_server\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <50 to the Uptrain\u001b[0m\n",
      "\u001b[32m2024-04-03 19:05:21.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m365\u001b[0m - \u001b[1mLocal server not running, start the server to log data and visualize in the dashboard!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What did the president say about Ketanji Brown Jackson\n",
      "Response: The President mentioned that he had nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to serve on the United States Supreme Court 4 days ago. He described her as one of the nation's top legal minds who will continue Justice Breyerâ€™s legacy of excellence.\n",
      "\n",
      "Context Relevance Score: 1.0\n",
      "Factual Accuracy Score: 1.0\n",
      "Response Completeness Score: 0.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the retriever\n",
    "compressor = FlashrankRerank()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever)\n",
    "\n",
    "# Create the uptrain callback\n",
    "uptrain_callback = UpTrainCallbackHandler()\n",
    "config = {\"callbacks\": [uptrain_callback]}\n",
    "\n",
    "# Invoke the chain with a query\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "result = chain.invoke(query, config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
