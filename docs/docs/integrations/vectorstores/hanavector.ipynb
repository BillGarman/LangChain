{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAP HANA Cloud Vector Engine\n",
    "\n",
    ">SAP HANA Cloud Vector Engine is a vector store fully integrated into the SAP HANA Cloud database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation of the HANA database driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pip install necessary package\n",
    "%pip install --upgrade --quiet  hdbcli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `OpenAIEmbeddings` so we have to get the OpenAI API Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T08:02:16.802456Z",
     "start_time": "2023-09-09T08:02:07.065604Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Use OPENAI_API_KEY env variable \n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T08:02:23.144824Z",
     "start_time": "2023-09-09T08:02:22.047801Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores.hanavector import HanaDB\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T08:02:25.452472Z",
     "start_time": "2023-09-09T08:02:25.441563Z"
    }
   },
   "outputs": [],
   "source": [
    "text_documents = TextLoader(\"../../modules/state_of_the_union.txt\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "text_chunks = text_splitter.split_documents(text_documents)\n",
    "print(f\"Number of document chunks: {len(text_chunks)}\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T08:02:28.174088Z",
     "start_time": "2023-09-09T08:02:28.162698Z"
    }
   },
   "outputs": [],
   "source": [
    "from hdbcli import dbapi\n",
    "\n",
    "# Use connection settings from the environment\n",
    "connection = dbapi.connect(\n",
    "    address=os.environ.get(\"HANA_DB_ADDRESS\"),\n",
    "    port=os.environ.get(\"HANA_DB_PORT\"),\n",
    "    user=os.environ.get(\"HANA_DB_USER\"),\n",
    "    password=os.environ.get(\"HANA_DB_PASSWORD\"),\n",
    "    autocommit=True,\n",
    "    sslValidateCertificate=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Similarity Search with Cosine Simliarity (Default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings that are used are specified by the given table name.\n",
    "If the table does not exist, a new table is created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T08:04:16.696625Z",
     "start_time": "2023-09-09T08:02:31.817790Z"
    }
   },
   "outputs": [],
   "source": [
    "db = HanaDB(\n",
    "    embedding=embeddings,\n",
    "    connection=connection,\n",
    "    table_name = \"STATE_OF_THE_UNION\"\n",
    ")\n",
    "\n",
    "# Delete already existing documents from the table\n",
    "db.delete(filter={})\n",
    "\n",
    "# add the loaded document chunks\n",
    "db.add_documents(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db.similarity_search(query, k=2)\n",
    "\n",
    "for doc in docs:\n",
    "    print(\"-\" * 80)\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the same content with Euclidian Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "db = HanaDB(\n",
    "    embedding=embeddings,\n",
    "    connection=connection,\n",
    "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,\n",
    "    table_name = \"STATE_OF_THE_UNION\"\n",
    ")\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db.similarity_search(query, k=2)\n",
    "for doc in docs:\n",
    "    print(\"-\" * 80)\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Maximal Marginal Relevance Search (MMR)\n",
    "Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T08:05:23.276819Z",
     "start_time": "2023-09-09T08:05:21.972256Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = db.max_marginal_relevance_search(query, k=2)\n",
    "for doc in docs:\n",
    "    print(\"-\" * 80)\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Vectorstore Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = HanaDB(\n",
    "    connection=connection,\n",
    "    embedding=embeddings,\n",
    "    table_name = \"LANGCHAIN_DEMO_BASIC\"\n",
    ")\n",
    "\n",
    "# Delete already existing documents from the table\n",
    "db.delete(filter={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add plain documents\n",
    "We can add documents to the existing table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [Document(page_content=\"plain\"), Document(page_content=\"docs\")]\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add documents with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [Document(page_content=\"foo\", metadata={\"start\": 100, \"end\": 150, \"doc_name\": \"foo.txt\", \"quality\": \"bad\"}), \n",
    "        Document(page_content=\"bar\", metadata={\"start\": 200, \"end\": 250, \"doc_name\": \"bar.txt\", \"quality\": \"good\"})]\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query documents with specific metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(\"foobar\", k=2, filter={\"quality\": \"bad\"})\n",
    "# With filtering on \"quality\"==\"bad\", only one document should be returned\n",
    "for doc in docs:\n",
    "    print(\"-\" * 80)\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a VectorStore as a Retriever in Chains for retrieval augmented generation (RAG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Access the vector DB with the previously filled table STATE_OF_THE_UNION\n",
    "db = HanaDB(\n",
    "    connection=connection,\n",
    "    embedding=embeddings,\n",
    "    table_name = \"LANGCHAIN_DEMO_RETRIEVAL_CHAIN\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo')\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", output_key='answer', return_messages=True)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = '''\n",
    "You are an expert state of the union topics. You are provided multiple context items that are related to the prompt you have to answer.\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "Question: {question}\n",
    "'''\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ConversationalRetrievalChain which handles the chat history and the retrieval of similar document chunks to be added to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    db.as_retriever(search_kwargs={'k': 5}),\n",
    "    return_source_documents=True,\n",
    "    memory=memory,\n",
    "    verbose=False,\n",
    "    combine_docs_chain_kwargs={'prompt': PROMPT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask the first question (and verify how many text chunks have been used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What about Mexico and Guatemala?\"\n",
    "\n",
    "result = qa_chain({\"question\": question})\n",
    "print('Answer from LLM:')\n",
    "print('================')\n",
    "print(result[\"answer\"])\n",
    "\n",
    "source_docs = result[\"source_documents\"]\n",
    "print('================')\n",
    "print(f\"Number of used source document chunks: {len(source_docs)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
