{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infinity\n",
    "\n",
    "`Infinity` allows to create `Embeddings` using a MIT-licensed Embedding Server via the AsyncEmbeddingEngine. \n",
    "\n",
    "This notebook goes over how to use Langchain with Embeddings with the [Infinity Github Project](https://github.com/michaelfeil/infinity).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import InfinityEmbeddingsLocal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: install infinity\n",
    "\n",
    "To install infinity use the following command. For further details check out the [Docs on Github](https://github.com/michaelfeil/infinity).\n",
    "Install the torch and onnx dependencies. \n",
    "\n",
    "```bash\n",
    "pip install infinity_emb[torch,optimum]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the infinity package\n",
    "%pip install --upgrade --quiet  infinity_emb[optimum,torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed your documents using your Infinity instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Baguette is a dish.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"numpy is a lib for linear algebra\",\n",
    "    \"You escaped what I've escaped - You'd be in Paris getting fucked up too\",\n",
    "]\n",
    "query = \"Where is Paris?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    }
   ],
   "source": [
    "embeddings = InfinityEmbeddingsLocal(\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    # revision\n",
    "    revision=None,\n",
    "    # best to keep at 32\n",
    "    batch_size=32,\n",
    "    # for AMD/Nvidia GPUs via torch\n",
    "    device=\"cuda\",\n",
    "    # warm up model before execution\n",
    ")\n",
    "\n",
    "\n",
    "async def embed():\n",
    "    # TODO: This function is just to showcase that your call can run async.\n",
    "\n",
    "    # important: use engine inside of `async with` statement to start/stop the batching engine.\n",
    "    async with embeddings:\n",
    "        # avoid closing and starting the engine often.\n",
    "        # rather keep it running.\n",
    "        # you may call `await embeddings.__aenter__()` and `__aexit__()\n",
    "        # if you are sure when to manually start/stop execution` in a more granular way\n",
    "\n",
    "        documents_embedded = await embeddings.aembed_documents(documents)\n",
    "        query_result = await embeddings.aembed_query(query)\n",
    "        print(\"embeddings created successful\")\n",
    "    return documents_embedded, query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings created successful\n"
     ]
    }
   ],
   "source": [
    "# run the async code however you would like\n",
    "# if you are in a jupyter notebook, you can use the following\n",
    "documents_embedded, query_result = await embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Baguette is a dish.': 0.31341904,\n",
       " 'Paris is the capital of France.': 0.8148992,\n",
       " 'numpy is a lib for linear algebra': 0.0044837985,\n",
       " \"You escaped what I've escaped - You'd be in Paris getting fucked up too\": 0.50892454}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (demo) compute similarity\n",
    "import numpy as np\n",
    "\n",
    "scores = np.array(documents_embedded) @ np.array(query_result).T\n",
    "dict(zip(documents, scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0a0263b650d907a3bfe41c0f8d6a63a071b884df3cfdc1579f00cdc1aed6b03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
