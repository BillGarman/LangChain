{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: Google Cloud Vertex AI\n",
    "keywords: [gemini, vertex, ChatVertexAI, gemini-pro]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatVertexAI\n",
    "\n",
    "Note: This is separate from the Google PaLM integration. Google has chosen to offer an enterprise version of PaLM through GCP, and this supports the models made available through there. \n",
    "\n",
    "ChatVertexAI exposes all foundational models available in Google Cloud:\n",
    "\n",
    "- Gemini (`gemini-pro` and `gemini-pro-vision`)\n",
    "- PaLM 2 for Text (`text-bison`)\n",
    "- Codey for Code Generation (`codechat-bison`)\n",
    "For a full and updated list of available models visit VertexAI documentation\n",
    "\n",
    "By default, Google Cloud [does not use](https://cloud.google.com/vertex-ai/docs/generative-ai/data-governance#foundation_model_development) customer data to train its foundation models as part of Google Cloud`s AI/ML Privacy Commitment. More details about how Google processes data can also be found in [Google's Customer Data Processing Addendum (CDPA)](https://cloud.google.com/terms/data-processing-addendum).\n",
    "\n",
    "To use `Google Cloud Vertex AI` PaLM you must have the `langchain-google-vertexai` Python package installed and either:\n",
    "- Have credentials configured for your environment (gcloud, workload identity, etc...)\n",
    "- Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variable\n",
    "\n",
    "This codebase uses the `google.auth` library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.\n",
    "\n",
    "For more information, see: \n",
    "- https://cloud.google.com/docs/authentication/application-default-credentials#GAC\n",
    "- https://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_vertexai import ChatVertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" J'aime la programmation.\")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are a helpful assistant who translate English to French\"\n",
    "human = \"Translate this sentence from English to French. I love programming.\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chat = ChatVertexAI()\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to construct a simple chain that takes user specified parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' プログラミングが大好きです')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = (\n",
    "    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    ")\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"Japanese\",\n",
    "        \"text\": \"I love programming\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code generation chat models\n",
    "You can now leverage the Codey API for code chat within Vertex AI. The model available is:\n",
    "- `codechat-bison`: for code assistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```python\n",
      "def generate_primes(n):\n",
      "  \"\"\"Generate all prime numbers up to n.\"\"\"\n",
      "\n",
      "  # Create a list of all numbers from 2 to n.\n",
      "  numbers = list(range(2, n + 1))\n",
      "\n",
      "  # Iterate over the numbers in the list.\n",
      "  for i in range(len(numbers)):\n",
      "    # If the number is prime, add it to the list of primes.\n",
      "    if is_prime(numbers[i]):\n",
      "      primes.append(numbers[i])\n",
      "\n",
      "    # Otherwise, remove all multiples of the number from the list.\n",
      "    else:\n",
      "      for j in range(i + 1, len(numbers)):\n",
      "        if numbers[j] % numbers[i] == 0:\n",
      "          numbers.remove(numbers[j])\n",
      "\n",
      "  # Return the list of primes.\n",
      "  return primes\n",
      "\n",
      "def is_prime(n):\n",
      "  \"\"\"Check if n is prime.\"\"\"\n",
      "\n",
      "  # If n is 1, it is not prime.\n",
      "  if n == 1:\n",
      "    return False\n",
      "\n",
      "  # Iterate over all numbers from 2 to the square root of n.\n",
      "  for i in range(2, int(n ** 0.5) + 1):\n",
      "    # If n is divisible by any number from 2 to its square root, it is not prime.\n",
      "    if n % i == 0:\n",
      "      return False\n",
      "\n",
      "  # If n is divisible by no number from 2 to its square root, it is prime.\n",
      "  return True\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "chat = ChatVertexAI(\n",
    "    model_name=\"codechat-bison\", max_output_tokens=1000, temperature=0.5\n",
    ")\n",
    "\n",
    "message = chat.invoke(\"Write a Python function generating all prime numbers\")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous calls\n",
    "\n",
    "We can make asynchronous calls via the Runnables [Async Interface](/docs/expression_language/interface).\n",
    "\n",
    "Note: System message is not yet supported by Gemini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for running these examples in the notebook:\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' अहं प्रोग्रामनं प्रेमामि')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = (\n",
    "    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    ")\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chat = ChatVertexAI(\n",
    "    model_name=\"chat-bison\", max_output_tokens=1000, temperature=0.5\n",
    ")\n",
    "chain = prompt | chat\n",
    "\n",
    "asyncio.run(\n",
    "    chain.ainvoke(\n",
    "        {\n",
    "            \"input_language\": \"English\",\n",
    "            \"output_language\": \"Sanskrit\",\n",
    "            \"text\": \"I love programming\",\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming calls\n",
    "\n",
    "We can also stream outputs via the `stream` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The five most populous countries in the world are:\n",
      "1. China (1.4 billion)\n",
      "2. India (1.3 billion)\n",
      "3. United States (331 million)\n",
      "4. Indonesia (273 million)\n",
      "5. Pakistan (220 million)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"human\", \"List out the 5 most populous countries in the world\")]\n",
    ")\n",
    "\n",
    "chat = ChatVertexAI()\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "for chunk in chain.stream({}):\n",
    "    sys.stdout.write(chunk.content)\n",
    "    sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
