{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49f1e0d",
   "metadata": {},
   "source": [
    "# ChatUnify\n",
    "\n",
    "This notebook covers how to get started with Unify chat models. When using models through the Unify API endpoint, you get:\n",
    "- Single sign-on with all providers.\n",
    "- Dynamic routing to the best provider for your use case.\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3bef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install package\n",
    "!pip install -U langchain-unify langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f3e15",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Make sure to set the following environment variables:\n",
    "\n",
    "- `UNIFY_API_KEY`: You can get a key in the [Unify Console](https://console.unify.ai/login).\n",
    "\n",
    "## Usage \n",
    "### Chaining Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e0dbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"  Sure! Here's the translation:\\n\\nBonjour, comment vas-tu ?\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_unify.chat_models import ChatUnify\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "os.environ[\"UNIFY_API_KEY\"] = \"<YOUR API KEY>\"\n",
    "chat = ChatUnify()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that translates English to French.\"),\n",
    "        (\"human\", \"Translate this sentence from English to French. {english_text}.\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"english_text\": \"Hello, how are you?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37731d2c",
   "metadata": {},
   "source": [
    "### Streaming and SSO\n",
    "One of the advantages of using Unify is that through a single api key, you can access endpoints from different providers like Perplexity, Together AI, OctoAI and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0961ae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Together stream: \n",
      "  Hello! I'm an AI, I don't have a physical location or a specific country of origin. I was created by a group of researcher at Meta AI and my primary function is to assist and converse with users like you, answering questions and providing information on a wide range of topics. I'm here to help and provide assistance, so feel free to ask me anything!!\n",
      "Anyscale stream: \n",
      "  Sure! 9 + 10 is 19."
     ]
    }
   ],
   "source": [
    "chat_together = ChatUnify(model=\"llama-2-70b-chat@together-ai\")\n",
    "print(\"Together stream: \")\n",
    "for chunk in chat_together.stream(\"Hello Together, where are you from?\"):\n",
    "    print(chunk.content, end=\"\")\n",
    "\n",
    "chat_anyscale = ChatUnify(model=\"llama-2-70b-chat@anyscale\")\n",
    "print(\"\\nAnyscale stream: \")\n",
    "for chunk in chat_anyscale.stream(\"What's 9 + 10?\"):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f295f94",
   "metadata": {},
   "source": [
    "### Batching and Dynamic Routing\n",
    "Suppose you're developing an application for live translation and aim to utilize `mixtral-8x7b-instruct-v0.1` for the quickest output speed, and are not concerned with which provider you are using. Unify's dynamic routing gets this data from [our live benchmarks](https://unify.ai/hub/mixtral-8x7b-instruct-v0.1) and routes you to the provider that is performing better than the rest at that instant. \n",
    "\n",
    "You can see how to configure dynamic batching [here](https://unify.ai/docs/hub/concepts/runtime_routing.html#how-to-use-it).\n",
    "\n",
    "As expected, you can also batch your queries while using dynamic routing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61835218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='\"Hey, there\\'s an emergency in translation street, please send help asap!\"\\n\\nGerman Translation: \"Hey, es gibt einen Notfall in der Übersetzungsstraße, bitte senden Sie Hilfe sofort!\"\\n\\n(Note: The translation is done using Google Translate, and the phrase \"Hey\" is not a formal way of greeting in German. Instead, it\\'s more common to use \"Hallo\" or \"Guten Tag\" to greet someone in German.)'),\n",
       " AIMessage(content='\"நான் தன்னோறு தொழிலாளியோ அடைந்திருக்கிறேன், மும்பை-தன்னோறு ரூட் பலatformே எவ்வளவே? கேள்வியோ தந்திரா அவர்களே என்றே தெரிந்து கொண்டிருப்போம்.\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "messages = [\n",
    "    \"Translate the following to German: \"\n",
    "    \"Hey, there's an emergency in translation street, \"\n",
    "    \"please send help asap!\", \n",
    "\n",
    "    \"Translate the following to Tamil: \"\n",
    "    \"I am late for my train, \"\n",
    "    \"which way is the platform for the train to Mumbai please?\"\n",
    "]\n",
    "\n",
    "chat_fastest = ChatUnify(model=\"llama-2-70b-chat@highest-tks-per-sec\")\n",
    "chat_fastest.batch(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e711e6d2",
   "metadata": {},
   "source": [
    "### Async and Cost-based Dynamic Routing\n",
    "You can also run this asynchronously. For a use case such as long document summarization which is input heavy, you may want to optimise for input costs. Unify's dynamic router can do this too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "356e71cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\" OpenAI: Pioneering 'safe' artificial general intelligence.\"),\n",
       " AIMessage(content=' Mistral AI: French startup selling open-source AI language models, raises $2B.'),\n",
       " AIMessage(content=' Meta AI releases large language model LLaMA; 13B version outperforms GPT-3, weights leaked publicly.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    \"Summarize this in 10 words or less. OpenAI is a U.S. based artificial intelligence \"\n",
    "    \"(AI) research organization founded in December 2015, researching artificial intelligence \"\n",
    "    \"with the goal of developing 'safe and beneficial' artificial general intelligence, \"\n",
    "    \"which it defines as 'highly autonomous systems that outperform humans at most economically \"\n",
    "    \"valuable work'. As one of the leading organizations of the AI spring, it has developed \"\n",
    "    \"several large language models, advanced image generation models, and previously, released \"\n",
    "    \"open-source models. Its release of ChatGPT has been credited with starting the AI spring\", \n",
    "\n",
    "    \"Summarize this in 10 words or less. Mistral AI is a French company selling\"\n",
    "    \" artificial intelligence (AI) products. \"\n",
    "    \"It was founded in April 2023 by previous employees of Meta Platforms and Google DeepMind. \"\n",
    "    \"The company raised €385 million in October 2023 and in December 2023 it was valued at \"\n",
    "    \"more than $2 billion. It produces open source large language models, citing the \"\n",
    "    \"foundational importance of open-source software, and as a response to proprietary models. \"\n",
    "    \"As of March 2024, two models have been published and are available as weights. \"\n",
    "    \"Three more models, Small, Medium and Large, are available via API only.\", \n",
    "\n",
    "    \"Summarize this in 10 words or less. LLaMA (Large Language Model Meta AI) is a family of\"\n",
    "    \" autoregressive large language models (LLMs), \"\n",
    "    \"released by Meta AI starting in February 2023. For the first version of LLaMA, four model sizes \"\n",
    "    \"were trained: 7, 13, 33, and 65 billion parameters. LLaMA's developers reported that the 13B \"\n",
    "    \"parameter model's performance on most NLP benchmarks exceeded that of the much larger GPT-3 \"\n",
    "    \"(with 175B parameters) and that the largest model was competitive with state of the art models \"\n",
    "    \"such as PaLM and Chinchilla. Whereas the most powerful LLMs have generally been accessible only \"\n",
    "    \"through limited APIs (if at all), Meta released LLaMA's model weights to the research community \"\n",
    "    \"under a noncommercial license. Within a week of LLaMA's release, its weights were leaked to the \"\n",
    "    \"public on 4chan via BitTorrent.\"\n",
    "]\n",
    "\n",
    "chat_model = ChatUnify(model=\"mixtral-8x7b-instruct-v0.1@lowest-input-cost\")\n",
    "\n",
    "\n",
    "await chat_model.abatch(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
