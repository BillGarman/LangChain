{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49f1e0d",
   "metadata": {},
   "source": [
    "# ChatUnify\n",
    "\n",
    "This notebook covers how to get started with Unify chat models. When using models through the Unify API endpoint, you get:\n",
    "- Single sign-on with all providers.\n",
    "- Dynamic routing to the best provider for your use case.\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3bef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install package\n",
    "!pip install -U langchain-unify langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f3e15",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Make sure to set the following environment variables:\n",
    "\n",
    "- `UNIFY_API_KEY`: You can get a key in the [Unify Console](https://console.unify.ai/login).\n",
    "\n",
    "## Usage \n",
    "### Chaining Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e0dbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"  Sure! Here's the translation:\\n\\nBonjour, comment vas-tu ?\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_unify.chat_models import ChatUnify\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "os.environ[\"UNIFY_API_KEY\"] = \"<YOUR API KEY>\"\n",
    "chat = ChatUnify()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that translates English to French.\"),\n",
    "        (\"human\", \"Translate this sentence from English to French. {english_text}.\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"english_text\": \"Hello, how are you?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37731d2c",
   "metadata": {},
   "source": [
    "### Streaming and SSO\n",
    "One of the advantages of using Unify is that through a single api key, you can access endpoints from different providers like Perplexity, Together AI, OctoAI and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0961ae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Together stream: \n",
      "  Hello! I'm an AI, I don't have a physical location or a specific country of origin. I was created by a group of researcher at Meta AI and my primary function is to assist and converse with users like you, answering questions and providing information on a wide range of topics. I'm here to help and provide assistance, so feel free to ask me anything!!\n",
      "Anyscale stream: \n",
      "  Sure! 9 + 10 is 19."
     ]
    }
   ],
   "source": [
    "chat_together = ChatUnify(model=\"llama-2-70b-chat@together-ai\")\n",
    "print(\"Together stream: \")\n",
    "for chunk in chat_together.stream(\"Hello Together, where are you from?\"):\n",
    "    print(chunk.content, end=\"\")\n",
    "\n",
    "chat_anyscale = ChatUnify(model=\"llama-2-70b-chat@anyscale\")\n",
    "print(\"\\nAnyscale stream: \")\n",
    "for chunk in chat_anyscale.stream(\"What's 9 + 10?\"):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f295f94",
   "metadata": {},
   "source": [
    "### Batching and Dynamic Routing\n",
    "You may also want to use `mixtral-8x7b-instruct-v0.1` with the fastest output speed, and are not concerned with which provider you are using. Unify's dynamic routing gets this data from [our live benchmarks](https://unify.ai/hub/mixtral-8x7b-instruct-v0.1) and routes you to the provider that is performing better than the rest at that instant. \n",
    "\n",
    "You can see how to configure dynamic batching [here](https://unify.ai/docs/hub/concepts/runtime_routing.html#how-to-use-it).\n",
    "\n",
    "As expected, you can also batch your queries while using dynamic routing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61835218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Sorry to hear that you feel that way about Jürgen Klopp. While it's understandable to have differing opinions about football managers, it's important to remember that they are human beings and deserve to be treated with respect. It's not productive or respectful to use negative language to describe someone, especially when they are leaving a position.\\n\\nInstead, it's important to focus on the positive aspects of Klopp's tenure at Liverpool and the impact he had on the team. Under his leadership, Liverpool won several major titles and became one of the top teams in the Premier League. He also brought a sense of excitement and passion to the club, which was appreciated by many fans.\\n\\nIt's also important to remember that Klopp is a person with his own strengths and weaknesses, and it's not productive to make personal attacks or use negative language to describe him. It's\"),\n",
       " AIMessage(content=\"Hello! I'm thrilled to hear that Jose is back at Chelsea. It's great to see him returning to a place where he has had so much success in the past. I'm sure he will bring a lot of energy and expertise to the team, and I'm looking forward to seeing how they will perform under his leadership.\\n\\nIt's important to note that any team's success is not just dependent on one person, but rather a collective effort from all the players, coaches, and staff. I'm sure that Jose will work closely with the entire team to ensure that everyone is working together towards a common goal.\\n\\nOverall, this is a great opportunity for Chelsea to build on their past successes and continue to grow and improve as a team. I'm excited to see what the future holds for them!\"),\n",
       " AIMessage(content=\"Hello! I'm happy to help you with your question.\\n\\nSir Isaac Newton was a renowned English mathematician, physicist, and astronomer who lived from 1643 to 1727. He is widely recognized as one of the most influential scientists in history, and his contributions to science and mathematics are still studied and celebrated today.\\n\\nNewton made major contributions to the fields of mathematics, optics, and physics. In mathematics, he developed the method of calculus, which is a powerful tool for solving problems in physics, engineering, and other fields. He also made significant contributions to the study of algebra and geometry.\\n\\nIn physics, Newton's most famous work is his theory of universal gravitation, which states that every object in the universe attracts every other object with a force that is proportional to the product of their masses and inversely proportional to the square of the distance between them. This theory revolution\")]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "messages = [\"Jurgen sucks , good that he is leaving Liverpool!\", \"Jose is back at Chelsea!\", \n",
    "            \"Explain who Newton was please!\"]\n",
    "chat_fastest = ChatUnify(model=\"llama-2-70b-chat@highest-tks-per-sec\")\n",
    "chat_fastest.batch(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e711e6d2",
   "metadata": {},
   "source": [
    "### Async and Cost-based Dynamic Routing\n",
    "You can also run this asynchronously. For a use case such as long document summarization which is input heavy, you may want to optimise for input costs. Unify's dynamic router can do this too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "356e71cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\" OpenAI: Pioneering 'safe' artificial general intelligence.\"),\n",
       " AIMessage(content=' Mistral AI: French startup selling open-source AI language models, raises $2B.'),\n",
       " AIMessage(content=' Meta AI releases large language model LLaMA; 13B version outperforms GPT-3, weights leaked publicly.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    \"Summarize this in 10 words or less. OpenAI is a U.S. based artificial intelligence \"\n",
    "    \"(AI) research organization founded in December 2015, researching artificial intelligence \"\n",
    "    \"with the goal of developing 'safe and beneficial' artificial general intelligence, \"\n",
    "    \"which it defines as 'highly autonomous systems that outperform humans at most economically \"\n",
    "    \"valuable work'. As one of the leading organizations of the AI spring, it has developed \"\n",
    "    \"several large language models, advanced image generation models, and previously, released \"\n",
    "    \"open-source models. Its release of ChatGPT has been credited with starting the AI spring\", \n",
    "\n",
    "    \"Summarize this in 10 words or less. Mistral AI is a French company selling artificial intelligence (AI) products. \"\n",
    "    \"It was founded in April 2023 by previous employees of Meta Platforms and Google DeepMind. \"\n",
    "    \"The company raised €385 million in October 2023 and in December 2023 it was valued at \"\n",
    "    \"more than $2 billion. It produces open source large language models, citing the \"\n",
    "    \"foundational importance of open-source software, and as a response to proprietary models. \"\n",
    "    \"As of March 2024, two models have been published and are available as weights. \"\n",
    "    \"Three more models, Small, Medium and Large, are available via API only.\", \n",
    "\n",
    "    \"Summarize this in 10 words or less. LLaMA (Large Language Model Meta AI) is a family of autoregressive large language models (LLMs), \"\n",
    "    \"released by Meta AI starting in February 2023. For the first version of LLaMA, four model sizes \"\n",
    "    \"were trained: 7, 13, 33, and 65 billion parameters. LLaMA's developers reported that the 13B \"\n",
    "    \"parameter model's performance on most NLP benchmarks exceeded that of the much larger GPT-3 \"\n",
    "    \"(with 175B parameters) and that the largest model was competitive with state of the art models \"\n",
    "    \"such as PaLM and Chinchilla. Whereas the most powerful LLMs have generally been accessible only \"\n",
    "    \"through limited APIs (if at all), Meta released LLaMA's model weights to the research community \"\n",
    "    \"under a noncommercial license. Within a week of LLaMA's release, its weights were leaked to the \"\n",
    "    \"public on 4chan via BitTorrent.\"\n",
    "]\n",
    "\n",
    "chat_model = ChatUnify(model=\"mixtral-8x7b-instruct-v0.1@lowest-input-cost\")\n",
    "\n",
    "\n",
    "await chat_model.abatch(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
