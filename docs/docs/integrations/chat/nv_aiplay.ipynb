{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cc6caafa",
      "metadata": {
        "id": "cc6caafa"
      },
      "source": [
        "# NVIDIA AI Playground ChatModel\n",
        "\n",
        ">[NVIDIA AI Playground](https://www.nvidia.com/en-us/research/ai-playground/) gives users easy access to hosted endpoints for generative AI models like Llama-2, SteerLM, Mistral, etc. Using the API, you can query NVCR (NVIDIA Container Registry) function endpoints and get quick results from a DGX-hosted cloud compute environment. All models are source-accessible and can be deployed on your own compute cluster.\n",
        "\n",
        "This example goes over how to use LangChain to interact with supported AI Playground models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.llms.nv_aiplay import NVCRModel, NVAIPlayClient  ## Core backbone interface clients\n",
        "from langchain.chat_models import NVAIPlayChat                  ## Generic NVAIPlay Models\n",
        "from langchain.chat_models.nv_aiplay import LlamaChat           ## Llama-default NVAIPlay Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccff689e",
      "metadata": {
        "id": "ccff689e"
      },
      "source": [
        "## Setup\n",
        "\n",
        "**To get started:**\n",
        "1. Create a free account with the [NVIDIA GPU Cloud](https://catalog.ngc.nvidia.com/) service, which hosts AI solution catalogs, containers, models, etc.\n",
        "2. Navigate to `Catalog > AI Foundation Models > (Model with API endpoint)`.\n",
        "3. Select the `API` option and click `Generate Key`.\n",
        "4. Save the generated key as `NVAPI_KEY`. From there, you should have access to the endpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "sGGzUoaOgABq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGGzUoaOgABq",
        "outputId": "77650247-d070-4dbd-f29a-cbf0e40dc5b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVAPI Key (starts with nvapi-): ··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "## API Key can be found by going to NVIDIA NGC -> AI Playground -> (some model) -> Get API Code or similar.\n",
        "## 10K free queries to any endpoint (which is a lot actually).\n",
        "\n",
        "# del os.environ['NVAPI_KEY']  ## delete\n",
        "if os.environ.get('NVAPI_KEY', '').startswith('nvapi-'):\n",
        "    print('Valid NVAPI_KEY already in environment. Delete to reset')\n",
        "else:\n",
        "    nvapi_key = getpass.getpass('NVAPI Key (starts with nvapi-): ')\n",
        "    assert nvapi_key.startswith('nvapi-'), \\\n",
        "        f\"{nvapi_key[:5]}... is not a valid key\"\n",
        "    os.environ['NVAPI_KEY'] = nvapi_key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc24d0c",
      "metadata": {
        "id": "acc24d0c"
      },
      "source": [
        "## Underlying Requests API\n",
        "\n",
        "A selection of useful models are hosted in a DGX-powered service known as NVIDIA GPU Cloud (NGC). In this service, containers with exposed model endpoints are deployed and listed on the NVIDIA Container Registry service (NVCR). These systems are accessible via simple HTTP requests and can be utilized by a variety of systems.\n",
        "\n",
        "The `NVCRModel` class implements the basic interfaces to communicate with NVCR, limiting the utility functions to those relevant for AI Playground. For example, the following list is populated by querying the function list endpoint with a key-loaded GET request:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3QizTW-Xhcs3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QizTW-Xhcs3",
        "outputId": "2efdc0e7-8af1-453b-c087-21a5dc5f2f9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'playground_llama2_code_13b': 'f6a96af4-8bf9-4294-96d6-d71aa787612e',\n",
              " 'playground_neva_22b': '8bf70738-59b9-4e5f-bc87-7ab4203be7a0',\n",
              " 'playground_nvolveqa_40k': '091a03bb-7364-4087-8090-bd71e9277520',\n",
              " 'playground_gpt_qa_8b': '0c60f14d-46cb-465e-b994-227e1c3d5047',\n",
              " 'playground_llama2_code_34b': 'df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8',\n",
              " 'playground_gpt_steerlm_8b': '1423ff2f-d1c7-4061-82a7-9e8c67afd43a',\n",
              " 'playground_fuyu_8b': '9f757064-657f-4c85-abd7-37a7a9b6ee11',\n",
              " 'playground_clip': '8c21289c-0b18-446d-8838-011b7249c513',\n",
              " 'playground_llama2_13b': 'e0bb7fb9-5333-4a27-8534-c6288f921d3f',\n",
              " 'playground_sdxl': '89848fb8-549f-41bb-88cb-95d6597044a4',\n",
              " 'playground_llama2_70b': '0e349b44-440a-44e1-93e9-abe8dcb27158',\n",
              " 'playground_mistral': '35ec3354-2681-4d0e-a8dd-80325dcf7c63'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NVCRModel().available_models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T7lnwJXrmM6s",
      "metadata": {
        "id": "T7lnwJXrmM6s"
      },
      "source": [
        "From this, you can easily send over a request in the style shown in the AI Playground API window for Python. For this example, we will use a model which we is not currently in our LangChain support matrix (though we plan to add first-class support later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "_0Bin73Qhp2k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0Bin73Qhp2k",
        "outputId": "f10c4886-3258-4a24-ec01-250c85d83bec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The image is a gray scale photograph of a checkered pattern, possibly a portion of\n",
            " a chessboard or a security camera image. The pattern consists of a series of white\n",
            " and black squares, creating a visually striking design. The squares are organized\n",
            " in a grid-like pattern, covering the entire image from top to bottom and left to\n",
            " right. The contrast between the white and black squares is quite noticeable, emphasizing\n",
            " the checkered pattern and making it the central focus of the image.\n",
            "The image is a gray scale photograph of a checkered pattern, possibly a portion of\n",
            " a chessboard or a security camera image. The pattern consists of a series of white\n",
            " and black squares, creating a visually striking design. The squares are organized\n",
            " in a grid-like pattern, covering the entire image from top to bottom and left to\n",
            " right. The contrast between the white and black squares is quite noticeable, emphasizing\n",
            " the checkered pattern and making it the central focus of the image."
          ]
        }
      ],
      "source": [
        "client = NVCRModel()\n",
        "\n",
        "model = 'neva'\n",
        "payload = {\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"content\": \"Hi! What is in this image? <img src=\\\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAIAQMAAAD+wSzIAAAABlBMVEX///+/v7+jQ3Y5AAAADklEQVQI12P4AIX8EAgALgAD/aNpbtEAAAAASUVORK5CYII==\\\" />\",\n",
        "      \"role\": \"user\"\n",
        "    },\n",
        "    {\n",
        "      \"labels\": {\n",
        "        \"creativity\": 6,\n",
        "        \"helpfulness\": 6,\n",
        "        \"humor\": 0,\n",
        "        \"quality\": 6\n",
        "      },\n",
        "      \"role\": \"assistant\"\n",
        "    }\n",
        "  ],\n",
        "  \"temperature\": 0.2,\n",
        "  \"top_p\": 0.7,\n",
        "  \"max_tokens\": 512,\n",
        "  \"stream\": True\n",
        "}\n",
        "\n",
        "def print_with_newlines(generator):\n",
        "    buffer = \"\"\n",
        "    for response in generator:\n",
        "        content = response.get('content')\n",
        "        if len(buffer) > 80 and content.startswith(' '):\n",
        "            buffer = \"\"\n",
        "            print()\n",
        "        elif content.startswith('\\n'):\n",
        "            buffer = \"\"\n",
        "        buffer += content\n",
        "        print(content, end='')\n",
        "\n",
        "## Generate-style response\n",
        "# print(client.get_req_generation(model, payload))\n",
        "# print()\n",
        "## NOTE: if an invalid name is specified, it will try to find a model that contains the provided name\n",
        "\n",
        "## Stream-style response\n",
        "print_with_newlines(client.get_req_stream(model, payload))\n",
        "print()\n",
        "\n",
        "async def print_with_newlines_async(responses):\n",
        "    buffer = \"\"\n",
        "    async for response in responses:\n",
        "        content = response['content']\n",
        "        if len(buffer) > 80 and content.startswith(' '):\n",
        "            buffer = \"\"\n",
        "            print()\n",
        "        elif '\\n' in content:\n",
        "            buffer = \"\"\n",
        "        buffer += content\n",
        "        print(content, end='')\n",
        "\n",
        "## Stream-style response\n",
        "await print_with_newlines_async(client.get_req_astream(model, payload))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m7dOxaDFoKiz",
      "metadata": {
        "id": "m7dOxaDFoKiz"
      },
      "source": [
        "As we can see, this is a general-purpose backbone API which can be built upon quite nicely to facilitate the LangChain generation/streaming/astreaming APIs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nT5FOJbdo7y4",
      "metadata": {
        "id": "nT5FOJbdo7y4"
      },
      "source": [
        "## Integration With LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "agnoxxZypQ_Z",
      "metadata": {
        "id": "agnoxxZypQ_Z"
      },
      "source": [
        "Based on this core support, we have a base connector `NVAIPlayBaseModel` which implements all of the components necessary to interface with both the `LLM` and `SimpleChatModel` classes via inheritance. This notebook will demonstrate the `ChatModel` portion with key features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6RrXHC_XqWc1",
      "metadata": {
        "id": "6RrXHC_XqWc1"
      },
      "source": [
        "### **Supported Models**\n",
        "\n",
        "Querying `available_models` will still give you all of the models offered by your API credentials:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Jdl2NUfMhi4J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdl2NUfMhi4J",
        "outputId": "e9c4cc72-8db6-414b-d8e9-95de93fc5db4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['playground_llama2_70b',\n",
              " 'playground_sdxl',\n",
              " 'playground_gpt_steerlm_8b',\n",
              " 'playground_nvolveqa_40k',\n",
              " 'playground_mistral',\n",
              " 'playground_clip',\n",
              " 'playground_gpt_qa_8b',\n",
              " 'playground_llama2_code_34b',\n",
              " 'playground_llama2_code_13b',\n",
              " 'playground_fuyu_8b',\n",
              " 'playground_neva_22b',\n",
              " 'playground_llama2_13b']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NVAIPlayLLM().available_models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WMW79Iegqj4e",
      "metadata": {
        "id": "WMW79Iegqj4e"
      },
      "source": [
        "All of these models are *technically* supported and can all be accessed via `NVCRModel`, but some models have first-class LangChain support and others are more experimental.\n",
        "\n",
        "**Ready-To-Use Chat Models** have been tested and are top-priority for our LangChain support. They're useful for external and internal reasoning, and responses always come in with a chat format and with a common seed for consistent and reproducible trial results. There is no text completion API for these models for AI Playground, though support for raw query endpoints exists with NeMo Service and other NVCR functions.\n",
        "- `llama2_13b`/`llama2_70b`: Chat-trained variants of Llama-2\n",
        "- `llama2_code_13b`/`llama2_code_43b`: Code-trained variants of Llama-2\n",
        "- `mistral`: Instruction-tuned variant of Mistral."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "bf0a425c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf0a425c",
        "outputId": "fae6ca89-3499-455b-ccc3-48ee368c3ca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'role': 'user', 'content': \"Hey, we've just met! How's your day going?\"}\n",
            "content=\"Hello! *smile* I'm doing well, thank you for asking! It's great to meet you too! How about you, how's your day going so far? Is there anything you'd like to talk about or ask? I'm here to help with any questions you might have. *pleasant and respectful tone*\"\n",
            "{'role': 'user', 'content': \"Hey, we've just met! How's your day going?\"}\n",
            "content=\"Hello! *smile* I'm doing well, thank you for asking! It's great to meet you too! How about you, how's your day going so far? Is there anything you'd like to talk about or ask? I'm here to help with any questions you might have. *pleasant and respectful tone*\"\n"
          ]
        }
      ],
      "source": [
        "# from langchain.chat_models.nv_aiplay import LlamaChat\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# Single prompt\n",
        "llm = LlamaChat()\n",
        "print(llm(HumanMessage(content=\"Hey, we've just met! How's your day going?\")))\n",
        "print(llm(\"Hey, we've just met! How's your day going?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GrhSaKiyyD1l",
      "metadata": {
        "id": "GrhSaKiyyD1l"
      },
      "source": [
        "We currently also support streaming and asynchronous streaming in a similar fashion as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "nBXBmtmNx-U_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBXBmtmNx-U_",
        "outputId": "1870e3ad-2da6-4aba-d8df-8ffb0e1fc7f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'role': 'user', 'content': \"Who's the best quarterback in the NFL?\"}\n",
            "As a helpful and respectful assistant, I cannot provide a subjective opinion on who\n",
            " the \"best\" quarterback in the NFL is, as this is a matter of personal opinion and can be influenced by a variety of factors such as team loyalty, personal bias, and individual performance. However, I can\n",
            " provide some information on some of the top-performing quarterbacks in the NFL this\n",
            " season, based on their statistics and achievements.\n",
            "\n",
            "Some of the top-performing quarterbacks in the NFL this season include:\n",
            "\n",
            "1. Lamar Jackson, Baltimore Ravens: Jackson has had an MVP-caliber season, leading\n",
            " the Ravens to a 10-2 record and setting numerous records for rushing yards by a quarterback.\n",
            " He has thrown for 3,107 yards and 32 touchdowns, while also rushing for 1,008 yards\n",
            " and 7 touchdowns.\n",
            "2. Russell Wilson, Seattle Seahawks: Wilson has had another strong season, leading\n",
            " the Seahawks to a 9-3 record and throwing for 3,875 yards and 30 touchdowns. He has\n",
            " also rushed for 242 yards and 2 touchdowns.\n",
            "3. Deshaun Watson, Houston Texans: Watson has had a remarkable season, leading the\n",
            " Texans to a 9-3 record and throwing for 3,852 yards and 25 touchdowns. He has also\n",
            " rushed for 241 yards and 2 touchdowns.\n",
            "4. Tom Brady, New England Patriots: Brady has had another solid season, leading the\n",
            " Patriots to a 10-2 record and throwing for 3,839 yards and 24 touchdowns. He has\n",
            " also rushed for 67 yards and 1 touchdown.\n",
            "5. Aaron Rodgers, Green Bay Packers: Rodgers has had a strong season, leading the\n",
            " Packers to a 9-3 record and throwing for 3,800 yards and 24 touchdowns. He has also\n",
            " rushed for 141 yards and 2 touchdowns.\n",
            "\n",
            "It's important to note that there are many other talented quarterbacks in the NFL,\n",
            " and opinions on who is the \"best\" can vary depending on individual perspectives and\n",
            " criteria."
          ]
        }
      ],
      "source": [
        "async def print_with_newlines_async(responses):\n",
        "    buffer = \"\"\n",
        "    async for content in responses:\n",
        "        content = content.content ## Difference from LLM\n",
        "        if len(buffer) > 80 and content.startswith(' '):\n",
        "            buffer = \"\"\n",
        "            print()\n",
        "        elif '\\n' in content:\n",
        "            buffer = \"\"\n",
        "        buffer += content\n",
        "        print(content, end='')\n",
        "\n",
        "await print_with_newlines_async(llm.astream(\"Who's the best quarterback in the NFL?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MzZDM4IqztmZ",
      "metadata": {
        "id": "MzZDM4IqztmZ"
      },
      "source": [
        "At the same time, there are also some specific APIs that we support for the sake of convenience since the underlying requests API is chat-oriented. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "m5me4gY2z0UG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5me4gY2z0UG",
        "outputId": "3513ab4a-822a-4fde-c903-53ad21b97171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'role': 'system', 'content': 'Only generate python code. Do not add any discussions about it.\\n'}\n",
            "{'role': 'user', 'content': 'Please implement Fibanocci in python without recursion. Your response should start and end in ```'}\n",
            "```\n",
            "def fibonacci(n):\n",
            "    if n <= 1:\n",
            "        return n\n",
            "    else:\n",
            "        a, b = 0, 1\n",
            "        for i in range(n-1):\n",
            "            a, b = b, a + b\n",
            "        return a\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(llm(\"\"\"\n",
        "///ROLE SYS: Only generate python code. Do not add any discussions about it.\n",
        "///ROLE USER: Please implement Fibanocci in python without recursion. Your response should start and end in ```\n",
        "\"\"\").content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x9nDDwiC0aff",
      "metadata": {
        "id": "x9nDDwiC0aff"
      },
      "source": [
        "You can add your own custom support for such a system by subclassing the `NVAIPlayBaseModel` class."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "137662a6",
      "metadata": {
        "id": "137662a6"
      },
      "source": [
        "# Conversation Chains"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79efa62d",
      "metadata": {
        "id": "79efa62d"
      },
      "source": [
        "Like any other integration, NVAIPlayClients are fine to support chat utilities like conversation buffers by default. Below, we show the [LangChain ConversationBufferMemory](https://python.langchain.com/docs/modules/memory/types/buffer) example applied to the LlamaLLM model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "fd2c6bc1",
      "metadata": {
        "id": "fd2c6bc1"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = LlamaLLM(\n",
        "    temperature = 0.1,\n",
        "    max_tokens = 100,\n",
        "    top_p = 1.0\n",
        ")\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f644ff28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "f644ff28",
        "outputId": "bae354cc-2118-4e01-ce20-a717ac94d27d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "{'role': 'user', 'content': 'The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n\\nHuman: Hi there!\\nAI:'}\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi there!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "uHIMZxVSVNBC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "uHIMZxVSVNBC",
        "outputId": "79acc89d-a820-4f2c-bac2-afe99da95580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI: Hello! How can I assist you today?\n",
            "Human: I'm doing well! Just having a conversation with an AI.\n",
            "AI:\u001b[0m\n",
            "{'role': 'user', 'content': \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\nHuman: Hi there!\\nAI: Hello! How can I assist you today?\\nHuman: I'm doing well! Just having a conversation with an AI.\\nAI:\"}\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"That's great! I'm here to help and provide information to the best of my ability. Is there anything specific you'd like to know or discuss?\""
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "LyD1xVKmVSs4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "LyD1xVKmVSs4",
        "outputId": "a1714513-a8fd-4d14-f974-233e39d5c4f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI: Hello! How can I assist you today?\n",
            "Human: I'm doing well! Just having a conversation with an AI.\n",
            "AI: That's great! I'm here to help and provide information to the best of my ability. Is there anything specific you'd like to know or discuss?\n",
            "Human: Tell me about yourself.\n",
            "AI:\u001b[0m\n",
            "{'role': 'user', 'content': \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\nHuman: Hi there!\\nAI: Hello! How can I assist you today?\\nHuman: I'm doing well! Just having a conversation with an AI.\\nAI: That's great! I'm here to help and provide information to the best of my ability. Is there anything specific you'd like to know or discuss?\\nHuman: Tell me about yourself.\\nAI:\"}\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Hello! I'm just an AI, I don't have a physical body, but I exist as a program that can process and analyze vast amounts of data. I am designed to be helpful and assist with a wide range of tasks, from answering questions to providing information on a variety of topics. I am constantly learning and improving, so I can become more knowledgeable and helpful over time. I am also designed to be respectful and socially unbiased, and I\""
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"Tell me about yourself.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
