{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6caafa",
   "metadata": {
    "id": "cc6caafa"
   },
   "source": [
    "# NVIDIA AI Playground ChatModel\n",
    "\n",
    ">[NVIDIA AI Playground](https://www.nvidia.com/en-us/research/ai-playground/) gives users easy access to hosted endpoints for generative AI models like Llama-2, SteerLM, Mistral, etc. Using the API, you can query NVCR (NVIDIA Container Registry) function endpoints and get quick results from a DGX-hosted cloud compute environment. All models are source-accessible and can be deployed on your own compute cluster.\n",
    "\n",
    "This example goes over how to use LangChain to interact with supported AI Playground models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c821816e-1b45-42fd-9772-9269bdbb1b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Core LC Chat Interface\n",
    "from langchain.chat_models import NVAIPlayChat\n",
    "from langchain.chat_models.nv_aiplay import (\n",
    "    ContextChat,\n",
    "    GeneralChat,\n",
    "    SteerChat,\n",
    ")\n",
    "from langchain.llms.nv_aiplay import NVCRModel\n",
    "from langchain.schema import BaseMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff689e",
   "metadata": {
    "id": "ccff689e"
   },
   "source": [
    "## Setup\n",
    "\n",
    "**To get started:**\n",
    "1. Create a free account with the [NVIDIA GPU Cloud](https://catalog.ngc.nvidia.com/) service, which hosts AI solution catalogs, containers, models, etc.\n",
    "2. Navigate to `Catalog > AI Foundation Models > (Model with API endpoint)`.\n",
    "3. Select the `API` option and click `Generate Key`.\n",
    "4. Save the generated key as `NVAPI_KEY`. From there, you should have access to the endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sGGzUoaOgABq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGGzUoaOgABq",
    "outputId": "77650247-d070-4dbd-f29a-cbf0e40dc5b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid NVAPI_KEY already in environment. Delete to reset\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "## API Key can be found by going to NVIDIA NGC -> AI Playground -> (some model) -> Get API Code or similar.\n",
    "## 10K free queries to any endpoint (which is a lot actually).\n",
    "\n",
    "# del os.environ['NVAPI_KEY']  ## delete\n",
    "if os.environ.get(\"NVAPI_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVAPI_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVAPI_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nT5FOJbdo7y4",
   "metadata": {
    "id": "nT5FOJbdo7y4"
   },
   "source": [
    "## Integration With LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agnoxxZypQ_Z",
   "metadata": {
    "id": "agnoxxZypQ_Z"
   },
   "source": [
    "We have a base connector `NVAIPlayBaseModel` which implements all of the components necessary to interface with both the `LLM` and `SimpleChatModel` classes via inheritance. They are constructed off of the `NVCRModel` backbone class, which is demonstrated at the end of this notebook for more advanced use-cases.\n",
    "\n",
    "This notebook will demonstrate the `ChatModel` portion with key features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6RrXHC_XqWc1",
   "metadata": {
    "id": "6RrXHC_XqWc1"
   },
   "source": [
    "### **Supported Models**\n",
    "\n",
    "Querying `available_models` will still give you all of the models offered by your API credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Jdl2NUfMhi4J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jdl2NUfMhi4J",
    "outputId": "e9c4cc72-8db6-414b-d8e9-95de93fc5db4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['playground_llama2_13b',\n",
       " 'playground_llama2_code_13b',\n",
       " 'playground_mistral',\n",
       " 'playground_llama2_code_34b',\n",
       " 'playground_gpt_steerlm_8b',\n",
       " 'playground_sdxl',\n",
       " 'playground_llama2_70b',\n",
       " 'playground_nvolveqa_40k',\n",
       " 'playground_llama2_steerlm_70b',\n",
       " 'playground_fuyu_8b',\n",
       " 'playground_neva_22b',\n",
       " 'playground_clip',\n",
       " 'playground_gpt_qa_8b']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NVAIPlayChat().available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WMW79Iegqj4e",
   "metadata": {
    "id": "WMW79Iegqj4e"
   },
   "source": [
    "All of these models are *technically* supported and can all be accessed via `NVCRModel`, but some models have first-class LangChain support and others are more experimental.\n",
    "\n",
    "**Ready-To-Use LLM Models** have been tested and are top-priority for our LangChain support. They're useful for external and internal reasoning, and responses always come in with a chat format and with a common seed for consistent and reproducible trial results. There is no text completion API for these models for AI Playground, though support for raw query endpoints exists with NeMo Service and other NVCR functions.\n",
    "\n",
    "Models can be invoked by specifying a `model` field that can be tied back to the `available_models` function ids. A selection of classes are also provided (importable from `langchain.chat_models.nv_aiplay` as opposed to `langchain.chat_models` directly) which are pre-populated with default model arguments and updated with reasonable default model options as the service selection evolves:\n",
    "\n",
    "- `GeneralChat`: Pre-populated with a default chat-tuned model. Chat counterpart of `GeneralLLM`\n",
    "- `CodeChat`: Pre-populated with a default code-tuned model. Chat counterpart of `CodeLLM`\n",
    "- `InstructChat`: Pre-populated with a default instruction-tuned model. Chat counterpart of `InstructLLM`.\n",
    "- `SteerChat`: Pre-populated with a default [SteerLM-optimized model](https://developer.nvidia.com/blog/announcing-steerlm-a-simple-and-practical-technique-to-customize-llms-during-inference/). It also pre-populates the labels Field with a default value which can be overridden. Chat counterpart of `SteerLLM`\n",
    "- `ContextChat`: Pre-populated with a default context-reasoning model. This model actively expects \"context\"-role inputs in addition to standard \"user\"-role inputs. Chat counterpart of `ContextLLM`\n",
    "- `ImageChat`: Pre-populates with default image-reasoning multimodal model. This model is capable of taking in images as part of the discussion. Chat counterpart of `ImageLLM`. \n",
    "- Embedding models are also supported, but should be interfaced via the `NVAIEmbeddings` interface.\n",
    "\n",
    "All other current models are experimental and you are free to interface with them using this or the `NVCRModel` abstraction. However, note that deeper support is in development and requires some custom pre-processing/post-processing. \n",
    "\n",
    "**To find out more about a specific model, please navigate to the API section of an AI Playground model [as linked here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/codellama-13b/api).**\n",
    "\n",
    "-----\n",
    "\n",
    "The following is a brief showcase of the generate API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf0a425c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf0a425c",
    "outputId": "fae6ca89-3499-455b-ccc3-48ee368c3ca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content=\"Hello! *smile* I'm doing well, thank you for asking! It's great to meet you too! How about you, how's your day going so far? Is there anything you'd like to talk about or ask? I'm here to help with any questions you might have. *pleasant and respectful tone*\")\n",
      "AIMessage(content=\"Hello! *smile* I'm doing well, thank you for asking! It's great to meet you too! How about you, how's your day going so far? Is there anything you'd like to talk about or ask? I'm here to help with any questions you might have. *pleasant and respectful tone*\")\n"
     ]
    }
   ],
   "source": [
    "## Single prompt\n",
    "# chat = NVAIPlayChat(model=\"llama2_code_13b\")\n",
    "chat = GeneralChat()\n",
    "print(repr(chat(HumanMessage(content=\"Hey, we've just met! How's your day going?\"))))\n",
    "print(repr(chat(\"Hey, we've just met! How's your day going?\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GrhSaKiyyD1l",
   "metadata": {
    "id": "GrhSaKiyyD1l"
   },
   "source": [
    "These models supports streaming and asynchronous utilities, and should work like any other LangChain connector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nBXBmtmNx-U_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBXBmtmNx-U_",
    "outputId": "1870e3ad-2da6-4aba-d8df-8ffb0e1fc7f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a helpful and respectful assistant, I cannot provide a subjective opinion on who\n",
      " the \"best\" quarterback in the NFL is, as this is a matter of personal opinion and\n",
      " can be influenced by a variety of factors such as team loyalty, personal bias, and\n",
      " individual performance. However, I can provide some information on some of the top-performing\n",
      " quarterbacks in the NFL this season, based on their statistics and achievements.\n",
      "\n",
      "Some of the top-performing quarterbacks in the NFL this season include:\n",
      "\n",
      "1. Lamar Jackson, Baltimore Ravens: Jackson has had an MVP-caliber season, leading\n",
      " the Ravens to a 10-2 record and setting numerous records for rushing yards by a quarterback.\n",
      " He has thrown for 3,107 yards and 32 touchdowns, while also rushing for 1,008 yards\n",
      " and 7 touchdowns.\n",
      "2. Russell Wilson, Seattle Seahawks: Wilson has had another strong season, leading\n",
      " the Seahawks to a 9-3 record and throwing for 3,875 yards and 30 touchdowns. He has\n",
      " also rushed for 242 yards and 2 touchdowns.\n",
      "3. Deshaun Watson, Houston Texans: Watson has had a remarkable season, leading the\n",
      " Texans to a 9-3 record and throwing for 3,852 yards and 25 touchdowns. He has also\n",
      " rushed for 241 yards and 2 touchdowns.\n",
      "4. Tom Brady, New England Patriots: Brady has had another solid season, leading the\n",
      " Patriots to a 10-2 record and throwing for 3,839 yards and 24 touchdowns. He has\n",
      " also rushed for 67 yards and 1 touchdown.\n",
      "5. Aaron Rodgers, Green Bay Packers: Rodgers has had a strong season, leading the\n",
      " Packers to a 9-3 record and throwing for 3,800 yards and 24 touchdowns. He has also\n",
      " rushed for 141 yards and 2 touchdowns.\n",
      "\n",
      "It's important to note that there are many other talented quarterbacks in the NFL,\n",
      " and opinions on who is the \"best\" can vary depending on individual perspectives and\n",
      " criteria.\n"
     ]
    }
   ],
   "source": [
    "def print_with_nl(responses):\n",
    "    \"\"\"General helper for printing chat outputs (either generation or stream)\"\"\"\n",
    "    buffer = \"\"\n",
    "    if isinstance(responses, BaseMessage):\n",
    "        content_gen = (letter for letter in responses.content)\n",
    "    else:\n",
    "        content_gen = (chunk.content for chunk in responses)\n",
    "    for content in content_gen:\n",
    "        if len(buffer) > 80 and content.startswith(\" \"):\n",
    "            buffer = \"\"\n",
    "            print()\n",
    "        elif \"\\n\" in content:\n",
    "            buffer = \"\"\n",
    "        buffer += content\n",
    "        print(content, end=\"\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print_with_nl(chat.stream(\"Who's the best quarterback in the NFL?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MzZDM4IqztmZ",
   "metadata": {
    "id": "MzZDM4IqztmZ"
   },
   "source": [
    "At the same time, there are also some specific APIs that we support for the sake of convenience since the underlying requests API is chat-oriented. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "m5me4gY2z0UG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5me4gY2z0UG",
    "outputId": "3513ab4a-822a-4fde-c903-53ad21b97171"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    else:\n",
      "        a, b = 0, 1\n",
      "        for i in range(n-1):\n",
      "            a, b = b, a + b\n",
      "        return a\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print_with_nl(\n",
    "    chat.stream(\n",
    "        \"\"\"\n",
    "///ROLE SYS: Only generate python code. Do not add any discussions about it.\n",
    "///ROLE USER: Please implement Fibanocci in python without recursion. Your response should start and end in ```\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01ab13-9b4f-41c3-84c6-be4bfffec323",
   "metadata": {},
   "source": [
    "We can see exactly what message was passed into our model by querying the underlying client for one of its state variables. More on this in the advanced section near the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7628758f-74e7-4f7d-b184-b1d088733520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last_inputs', 'last_response', 'last_msg', 'available_functions']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'url': 'https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/e0bb7fb9-5333-4a27-8534-c6288f921d3f',\n",
       " 'headers': {'Authorization': SecretStr('**********'),\n",
       "  'Accept': 'text/event-stream',\n",
       "  'content-type': 'application/json'},\n",
       " 'json': {'messages': [{'role': 'system',\n",
       "    'content': 'Only generate python code. Do not add any discussions about it.\\n'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Please implement Fibanocci in python without recursion. Your response should start and end in ```'}],\n",
       "  'temperature': 0.2,\n",
       "  'top_p': 0.7,\n",
       "  'max_tokens': 1024,\n",
       "  'stream': True},\n",
       " 'stream': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(chat.client.state_vars)\n",
    "chat.client.last_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e38570c-ff91-451e-b641-8a2b0df4db1b",
   "metadata": {},
   "source": [
    "### Special-Format Models\n",
    "\n",
    "Certain models also support additional arguments such as the SteerLM model options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac3346c7-6ad2-440b-be10-8943ce3b6eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {}\n",
      "SpongeBob SquarePants is a beloved animated television character and the title character of\n",
      " the hit animated television series \"SpongeBob SquarePants.\" He is an energetic and optimistic yellow\n",
      " sea sponge who lives in a submerged pineapple in the fictional underwater city of Bikini Bottom, located in the\n",
      " Pacific Ocean.\n",
      "\n",
      "Labels: {'verbosity': 0}\n",
      "SpongeBob SquarePants is a yellow sea sponge who lives in a submerged house in the middle of\n",
      " the Pacific Ocean. He works as a fry cook at the Krusty Krab, and he spends his free time with\n",
      " his best friend, Patrick Star, and a cast of other anthropomorphic sea creatures. SpongeBob is\n",
      " known for his optimistic and energetic personality, and he often finds himself in various adventures\n",
      " and misadventures as he pursues his passion for learning and discovery. The series\n",
      " has been running since 1999 and has become a cultural phenomenon, beloved by audiences of\n",
      " all ages.\n",
      "Labels: {'creativity': 4, 'complexity': 0, 'verbosity': 0}\n",
      "SpongeBob SquarePants is a yellow sea sponge who lives in a submerged house in the middle of\n",
      " the Pacific Ocean. He works as a fry cook at the Krusty Krab, and he spends his free time with\n",
      " his best friend, Patrick Star, and other friends like Squidward Tentacles and Sandy Cheeks.\n",
      " SpongeBob is known for his optimism, curiosity, and love of adventure, and he often finds himself\n",
      " in various shenanigans due to his lack of common sense.\n",
      "Labels: {'creativity': 10, 'complexity': 0, 'verbosity': 0}\n",
      "This part failed, as it is gated by LabelModel validation\n",
      "1 validation error for SteerChat\n",
      "__root__ -> creativity\n",
      "  ensure this value is less than or equal to 9 (type=value_error.number.not_le; limit_value=9)\n"
     ]
    }
   ],
   "source": [
    "def try_steerlm(model, msg=\"Tell me about Spongebob Squarepants\", labels={}):\n",
    "    print(f\"Labels: {labels}\")\n",
    "    print_with_nl(model.stream(msg, labels=labels, stop=\"\\n\"))\n",
    "\n",
    "\n",
    "# steer_chat = NVAIPlayChat(model=\"steerlm\")\n",
    "steer_chat = SteerChat()\n",
    "try_steerlm(steer_chat)\n",
    "try_steerlm(steer_chat, labels={\"verbosity\": 0})\n",
    "try_steerlm(steer_chat, labels={\"creativity\": 4, \"complexity\": 0, \"verbosity\": 0})\n",
    "try:\n",
    "    try_steerlm(steer_chat, labels={\"creativity\": 10, \"complexity\": 0, \"verbosity\": 0})\n",
    "except ValueError as e:\n",
    "    print(f\"This part failed, as it is gated by LabelModel validation\\n{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f76a70-d2f3-406c-9f39-c7b45d44383b",
   "metadata": {},
   "source": [
    "Other systems may also populate other kinds of options, such as `ContextChat` which requires context-role inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cff17a1c-fec3-4657-8ff4-ad86b0e8cc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Carmen Sandiego\n",
      "age: unknown\n",
      "location: unknown\n",
      "occupation: unknown\n"
     ]
    }
   ],
   "source": [
    "qa_chat = ContextChat()\n",
    "\n",
    "print_with_nl(\n",
    "    qa_chat(\n",
    "        \"\"\"\n",
    "///ROLE CONTEXT: \n",
    "    Knowledge Base:\n",
    "        {name:unknown, age:unknown, location:unknown, occupation:unknown}\n",
    "    New Info: \n",
    "        Hello World! My name is Carmen Sandiego! \n",
    "///ROLE USER: \n",
    "    Please update the knowledge base off the response, based off only the new info.\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "098775d2-e522-42de-88f2-c8ab3b2433de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/0c60f14d-46cb-465e-b994-227e1c3d5047',\n",
       " 'headers': {'Authorization': SecretStr('**********'),\n",
       "  'Accept': 'application/json'},\n",
       " 'json': {'messages': [{'role': 'context',\n",
       "    'content': '\\n    Knowledge Base:\\n        {name:unknown, age:unknown, location:unknown, occupation:unknown}\\n    New Info: \\n        Hello World! My name is Carmen Sandiego! \\n'},\n",
       "   {'role': 'user',\n",
       "    'content': '\\n    Please update the knowledge base off the response, based off only the new info.'}],\n",
       "  'temperature': 0.2,\n",
       "  'top_p': 0.7,\n",
       "  'max_tokens': 512,\n",
       "  'stream': False},\n",
       " 'stream': False}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chat.client.last_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x9nDDwiC0aff",
   "metadata": {
    "id": "x9nDDwiC0aff"
   },
   "source": [
    "You can add your own custom support for such a system by subclassing the `NVAIPlayBaseModel` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137662a6",
   "metadata": {
    "id": "137662a6"
   },
   "source": [
    "# Conversation Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79efa62d",
   "metadata": {
    "id": "79efa62d"
   },
   "source": [
    "Like any other integration, NVAIPlayClients are fine to support chat utilities like conversation buffers by default. Below, we show the [LangChain ConversationBufferMemory](https://python.langchain.com/docs/modules/memory/types/buffer) example applied to the LlamaChat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd2c6bc1",
   "metadata": {
    "id": "fd2c6bc1"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "chat = GeneralChat(temperature=0.1, max_tokens=100, top_p=1.0)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat, verbose=True, memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f644ff28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "f644ff28",
    "outputId": "bae354cc-2118-4e01-ce20-a717ac94d27d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! It's great to talk to you! I'm here to help answer any questions you may have. What's on your mind today? 😊\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "uHIMZxVSVNBC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "uHIMZxVSVNBC",
    "outputId": "79acc89d-a820-4f2c-bac2-afe99da95580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! It's great to talk to you! I'm here to help answer any questions you may have. What's on your mind today? 😊\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Oh, that's great! I love conversing with humans. It's so much fun to learn about their thoughts and experiences. 🤖 What would you like to talk about? I'm here to help with any questions you may have, big or small. 😊\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "LyD1xVKmVSs4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "LyD1xVKmVSs4",
    "outputId": "a1714513-a8fd-4d14-f974-233e39d5c4f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! It's great to talk to you! I'm here to help answer any questions you may have. What's on your mind today? 😊\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI: Oh, that's great! I love conversing with humans. It's so much fun to learn about their thoughts and experiences. 🤖 What would you like to talk about? I'm here to help with any questions you may have, big or small. 😊\n",
      "Human: Tell me about yourself.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure thing! My name is LLaMA, I'm a large language model trained by a team of researcher at Meta AI. 🤓 I'm here to help answer any questions you may have, and I'm always happy to chat! 😊 I have been trained on a massive dataset of text from the internet, and I can provide information on a wide range of topics. I can also generate text in a variety of styles and formats\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Tell me about yourself.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc24d0c",
   "metadata": {
    "id": "acc24d0c"
   },
   "source": [
    "## **[Advanced]** Underlying Requests API\n",
    "\n",
    "A selection of useful models are hosted in a DGX-powered service known as [NVIDIA GPU Cloud (NGC)](https://catalog.ngc.nvidia.com/). In this service, containers with exposed model endpoints are deployed and listed on the NVIDIA Container Registry service (NVCR). These systems are accessible via simple HTTP requests and can be utilized by a variety of systems.\n",
    "\n",
    "The `NVCRModel` class implements the basic interfaces to communicate with NVCR, limiting the utility functions to those relevant for AI Playground. For example, the following list is populated by querying the function list endpoint with a key-loaded GET request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3QizTW-Xhcs3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3QizTW-Xhcs3",
    "outputId": "2efdc0e7-8af1-453b-c087-21a5dc5f2f9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'playground_llama2_13b': 'e0bb7fb9-5333-4a27-8534-c6288f921d3f',\n",
       " 'playground_llama2_code_13b': 'f6a96af4-8bf9-4294-96d6-d71aa787612e',\n",
       " 'playground_fuyu_8b': '9f757064-657f-4c85-abd7-37a7a9b6ee11',\n",
       " 'playground_gpt_steerlm_8b': '1423ff2f-d1c7-4061-82a7-9e8c67afd43a',\n",
       " 'playground_neva_22b': '8bf70738-59b9-4e5f-bc87-7ab4203be7a0',\n",
       " 'playground_nvolveqa_40k': '091a03bb-7364-4087-8090-bd71e9277520',\n",
       " 'playground_clip': '8c21289c-0b18-446d-8838-011b7249c513',\n",
       " 'playground_llama2_steerlm_70b': 'd6fe6881-973a-4279-a0f8-e1d486c9618d',\n",
       " 'playground_sdxl': '89848fb8-549f-41bb-88cb-95d6597044a4',\n",
       " 'playground_gpt_qa_8b': '0c60f14d-46cb-465e-b994-227e1c3d5047',\n",
       " 'playground_mistral': '35ec3354-2681-4d0e-a8dd-80325dcf7c63',\n",
       " 'playground_llama2_code_34b': 'df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8',\n",
       " 'playground_llama2_70b': '0e349b44-440a-44e1-93e9-abe8dcb27158'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Core NVCR Model base\n",
    "client = NVCRModel()\n",
    "client.available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T7lnwJXrmM6s",
   "metadata": {
    "id": "T7lnwJXrmM6s"
   },
   "source": [
    "From this, you can easily send over a request in the style shown in the AI Playground API window for Python. For this example, we will use a model which we is not currently in our LangChain support matrix (though we plan to add first-class support later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "_0Bin73Qhp2k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0Bin73Qhp2k",
    "outputId": "f10c4886-3258-4a24-ec01-250c85d83bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is a gray scale photograph of a checkered pattern, possibly a portion of\n",
      " a chessboard or a security camera image. The pattern consists of a series of white\n",
      " and black squares, creating a visually striking design. The squares are organized\n",
      " in a grid-like pattern, covering the entire image from top to bottom and left to\n",
      " right. The contrast between the white and black squares is quite noticeable, emphasizing\n",
      " the checkered pattern and making it the central focus of the image.\n"
     ]
    }
   ],
   "source": [
    "client = NVCRModel()\n",
    "\n",
    "model = \"neva\"\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": 'Hi! What is in this image? <img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAIAQMAAAD+wSzIAAAABlBMVEX///+/v7+jQ3Y5AAAADklEQVQI12P4AIX8EAgALgAD/aNpbtEAAAAASUVORK5CYII==\" />',\n",
    "            \"role\": \"user\",\n",
    "        },\n",
    "        {\n",
    "            \"labels\": {\"creativity\": 6, \"helpfulness\": 6, \"humor\": 0, \"quality\": 6},\n",
    "            \"role\": \"assistant\",\n",
    "        },\n",
    "    ],\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.7,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stream\": True,\n",
    "}\n",
    "\n",
    "\n",
    "def print_with_newlines(generator):\n",
    "    buffer = \"\"\n",
    "    for response in generator:\n",
    "        content = response.get(\"content\")\n",
    "        if len(buffer) > 80 and content.startswith(\" \"):\n",
    "            buffer = \"\"\n",
    "            print()\n",
    "        elif content.startswith(\"\\n\"):\n",
    "            buffer = \"\"\n",
    "        buffer += content\n",
    "        print(content, end=\"\")\n",
    "\n",
    "\n",
    "## Generate-style response\n",
    "# print(client.get_req_generation(model, payload))\n",
    "# print()\n",
    "\n",
    "## Stream-style response\n",
    "print_with_newlines(client.get_req_stream(model, payload))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd2b90-1b3a-4547-9070-1ef71e97a3cd",
   "metadata": {},
   "source": [
    "The client manages a lot of the surrounding interfaces such as API key permissions, model name resolution, parameter consistency, etc. and keeps a selection of raw communication history logs for debugging convenience. These can also be queried from the LangChain-default systems via the `client` field, i.e. `llm.client.last_inputs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbfb2dfb-2a84-4e75-b8a1-05ced51f4764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available State Variables: ['last_inputs', 'last_response', 'last_msg', 'available_functions']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'url': 'https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/8bf70738-59b9-4e5f-bc87-7ab4203be7a0',\n",
       " 'headers': {'Authorization': SecretStr('**********'),\n",
       "  'Accept': 'text/event-stream',\n",
       "  'content-type': 'application/json'},\n",
       " 'json': {'messages': [{'content': 'Hi! What is in this image? <img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAIAQMAAAD+wSzIAAAABlBMVEX///+/v7+jQ3Y5AAAADklEQVQI12P4AIX8EAgALgAD/aNpbtEAAAAASUVORK5CYII==\" />',\n",
       "    'role': 'user'},\n",
       "   {'labels': {'creativity': 6, 'helpfulness': 6, 'humor': 0, 'quality': 6},\n",
       "    'role': 'assistant'}],\n",
       "  'temperature': 0.2,\n",
       "  'top_p': 0.7,\n",
       "  'max_tokens': 512,\n",
       "  'stream': True},\n",
       " 'stream': True}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Available State Variables:\", client.state_vars)\n",
    "client.last_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m7dOxaDFoKiz",
   "metadata": {
    "id": "m7dOxaDFoKiz"
   },
   "source": [
    "As we can see, this is a general-purpose backbone API which can be built upon quite nicely to facilitate the LangChain generation/streaming/astreaming APIs. It also allows a nice window for advanced status checking and debugging for those who need to dig into their model processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2376d4b6-1690-45cf-bb93-4c25aa956710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '3a84daf3-5428-41ac-8c04-40928aa24046',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \"Sure thing! My name is LLaMA, I'm a large language model trained by a team of researcher at Meta AI. 🤓 I'm here to help answer any questions you may have, and I'm always happy to chat! 😊 I have been trained on a massive dataset of text from the internet, and I can provide information on a wide range of topics. I can also generate text in a variety of styles and formats\"},\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'completion_tokens': 100,\n",
       "  'prompt_tokens': 341,\n",
       "  'total_tokens': 441}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.client.last_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66d630c5-d640-4da9-b897-6d5c07b19910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.client.last_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5a7c0-e86e-4bad-84b3-8471fada61d9",
   "metadata": {},
   "source": [
    "The following is an example of checking the model specifications of the previously-instantiated llm's endpoint by manually retrieving it from the client's internal list of available services for advanced debugging purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23908a2b-ba34-4ff5-a328-b60eb48ed392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'e0bb7fb9-5333-4a27-8534-c6288f921d3f',\n",
       " 'ncaId': 'NVVppkDl81nJZ0GtZORXNeeOoY8h1p1Tfd8vuIPFV18',\n",
       " 'versionId': 'e83f1041-df20-4624-82f4-dffddadaa073',\n",
       " 'name': 'playground_llama2_13b',\n",
       " 'status': 'ACTIVE',\n",
       " 'inferenceUrl': 'infer',\n",
       " 'ownedByDifferentAccount': True,\n",
       " 'inferencePort': 8003,\n",
       " 'containerEnvironment': [{'key': 'CHECKPOINTS_DIR',\n",
       "   'value': '/config/models/llama2/'},\n",
       "  {'key': 'MODEL_NAME', 'value': 'llama2'}],\n",
       " 'models': [{'name': 'llama2',\n",
       "   'version': '0.9',\n",
       "   'uri': '/v2/org/whw3rcpsilnj/team/playground/models/llama2_13b_trt_l40g/0.9/files'}],\n",
       " 'containerImage': 'nvcr.io/whw3rcpsilnj/playground/llama2_server_optimized:0.16',\n",
       " 'apiBodyFormat': 'CUSTOM',\n",
       " 'healthUri': '/v2/health/ready',\n",
       " 'createdAt': '2023-11-15T07:34:20.700Z'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Rough Implementation Internals:\n",
    "# model_id = chat.client.last_inputs.get(\"url\").split(\"/\")[-1]\n",
    "# known_fns = chat.client.available_functions\n",
    "# fn_spec = [f for f in known_fns if f.get('id') == model_id][0]\n",
    "# fn_spec\n",
    "chat.get_model_details()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
