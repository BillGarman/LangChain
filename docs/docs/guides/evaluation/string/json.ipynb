{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465cfbef-5bba-4b3b-b02d-fe2eba39db17",
   "metadata": {},
   "source": [
    "# JSON Evaluators Guide\n",
    "\n",
    "Evaluating JSON outputs can be challenging given the structure and data types involved. This guide aims to assist in understanding and utilizing three evaluators designed for JSON string evaluation. These evaluators ensure the integrity, equality, and minimal difference between JSON strings.\n",
    "\n",
    "## JsonValidityEvaluator\n",
    "\n",
    "The `JsonValidityEvaluator` is designed to check the validity of a JSON string prediction.\n",
    "\n",
    "\n",
    "### Overview:\n",
    "- **Requires Input?**: No\n",
    "- **Requires Reference?**: No\n",
    "- **Evaluation Name**: \"json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e5f7dd-82fe-48f9-a251-b2052e17e61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import JsonValidityEvaluator\n",
    "\n",
    "evaluator = JsonValidityEvaluator()\n",
    "\n",
    "prediction = '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}'\n",
    "\n",
    "result = evaluator.evaluate_strings(prediction=prediction)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9607c6-edab-4c26-86c4-22b226e18aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0, 'reasoning': 'Expecting property name enclosed in double quotes: line 1 column 48 (char 47)'}\n"
     ]
    }
   ],
   "source": [
    "prediction = '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\",}'\n",
    "result = evaluator.evaluate_strings(prediction=prediction)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac18a83-30d8-4c11-abf2-7a36e4cb829f",
   "metadata": {},
   "source": [
    "## JsonEqualityEvaluator\n",
    "\n",
    "The `JsonEqualityEvaluator` assesses whether a JSON prediction matches a given reference after both are parsed.\n",
    "\n",
    "### Overview:\n",
    "- **Requires Input?**: No\n",
    "- **Requires Reference?**: Yes\n",
    "- **Evaluation Name**: \"parsed_equality\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab97111e-cba9-4273-825f-d5d4278a953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': True}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import JsonEqualityEvaluator\n",
    "\n",
    "evaluator = JsonEqualityEvaluator()\n",
    "result = evaluator.evaluate_strings(prediction='{\"a\": 1}', reference='{\"a\": 1}')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "655ba486-09b6-47ce-947d-b2bd8b6f6364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': False}\n"
     ]
    }
   ],
   "source": [
    "result = evaluator.evaluate_strings(prediction='{\"a\": 1}', reference='{\"a\": 2}')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7e541-b7fe-46b6-bc3a-e94fe316227e",
   "metadata": {},
   "source": [
    "The evaluator also by default lets you provide a dictionary directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e70ba3-4e62-483c-893a-5f328b7f303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': False}\n"
     ]
    }
   ],
   "source": [
    "result = evaluator.evaluate_strings(prediction={\"a\": 1}, reference={\"a\": 2})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921d33f0-b3c2-4e9e-820c-9ec30bc5bb20",
   "metadata": {},
   "source": [
    "## JsonEditDistanceEvaluator\n",
    "\n",
    "The `JsonEditDistanceEvaluator` computes a normalized Damerau-Levenshtein distance between two \"canonicalized\" JSON strings.\n",
    "\n",
    "### Overview:\n",
    "- **Requires Input?**: No\n",
    "- **Requires Reference?**: Yes\n",
    "- **Evaluation Name**: \"parsed_equality\"\n",
    "- **Distance Function**: Damerau-Levenshtein (by default)\n",
    "\n",
    "_Note: Ensure that `rapidfuzz` is installed or provide an alternative `string_distance` function to avoid an ImportError._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da9ec3a3-675f-4420-8ec7-cde48d8c2918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.07692307692307693}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import JsonEditDistanceEvaluator\n",
    "\n",
    "evaluator = JsonEditDistanceEvaluator()\n",
    "\n",
    "result = evaluator.evaluate_strings(\n",
    "    prediction='{\"a\": 1, \"b\": 2}', reference='{\"a\": 1, \"b\": 3}'\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "537ed58c-6a9c-402f-8f7f-07b1119a9ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# The values are canonicalized prior to comparison\n",
    "result = evaluator.evaluate_strings(\n",
    "    prediction=\"\"\"\n",
    "    {\n",
    "        \"b\": 3,\n",
    "        \"a\":   1\n",
    "    }\"\"\",\n",
    "    reference='{\"a\": 1, \"b\": 3}',\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52abec79-58ed-4ab6-9fb1-7deb1f5146cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.14285714285714285}\n"
     ]
    }
   ],
   "source": [
    "# You can also pass in objects directly\n",
    "result = evaluator.evaluate_strings(prediction={\"a\": 1}, reference={\"a\": 2})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e721a6-3625-426c-88f2-d1f686308913",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "These JSON evaluators offer a robust suite of tools for ensuring\n",
    "the accuracy and validity of model outputs in JSON format. By integrating these into your evaluation pipeline, you can maintain a high standard of data quality for your projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85afcf33-d2f4-406e-9d8f-15dc0a4772f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
