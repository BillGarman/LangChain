{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36d938c",
   "metadata": {},
   "source": [
    "# LLM Caching\n",
    "This notebook covers how to cache results of individual LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ad9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f0598",
   "metadata": {},
   "source": [
    "### In Memory Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "426ff912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f69f6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the caching really obvious, lets use a slower model.\n",
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64005d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.3 ms, sys: 18 ms, total: 52.3 ms\n",
      "Wall time: 990 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8a1cb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 76 µs, sys: 1e+03 ns, total: 77 µs\n",
      "Wall time: 81.8 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf59c12",
   "metadata": {},
   "source": [
    "### SQLite Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f036236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same thing with a SQLite cache\n",
    "from langchain.cache import SQLiteCache\n",
    "langchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa18e3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.46 ms, sys: 2.47 ms, total: 8.93 ms\n",
      "Wall time: 7.78 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bf2f6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.47 ms, sys: 1.08 ms, total: 3.55 ms\n",
      "Wall time: 2.96 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934943dc",
   "metadata": {},
   "source": [
    "### SQLAlchemy Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acccff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use SQLAlchemyCache to cache with any SQL database supported by SQLAlchemy.\n",
    "\n",
    "# from langchain.cache import SQLAlchemyCache\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/postgres\")\n",
    "# langchain.llm_cache = SQLAlchemyCache(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c69d84d",
   "metadata": {},
   "source": [
    "### Optional Caching\n",
    "You can also turn off caching for specific LLMs should you choose. In the example below, even though global caching is enabled, we turn it off for a specific LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6af46e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26c4fd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.27 ms, sys: 2.32 ms, total: 7.6 ms\n",
      "Wall time: 614 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46846b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.71 ms, sys: 3.53 ms, total: 10.2 ms\n",
      "Wall time: 1.56 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nA lady went to a store and saw a man with a huge head.\\n\\n\"Excuse me, sir,\" she said. \"I\\'ve never seen someone with a head that big. What did you do?\"\\n\\n\"I\\'m a genius,\" he said. \"I solved a Rubik\\'s Cube in less than 20 seconds.\"'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da41b77",
   "metadata": {},
   "source": [
    "### Optional Caching in Chains\n",
    "You can also turn off caching for particular nodes in chains. Note that because of certain interfaces, its often easier to construct the chain first, and then edit the LLM afterwards.\n",
    "\n",
    "As an example, we will load a summarizer map-reduce chain. We will cache results for the map-step, but then not freeze it for the combine step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9afa3f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98a78e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "\n",
    "text_splitter = CharacterTextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bfb099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../state_of_the_union.txt') as f:\n",
    "    state_of_the_union = f.read()\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f78b7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "docs = [Document(page_content=t) for t in texts[:3]]\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2a30822",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c97958e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets set the LLM on the combine document chain to not use a cache\n",
    "chain.combine_document_chain.llm_chain.llm = OpenAI(model_name=\"text-davinci-002\", cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a545b743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 487 ms, sys: 174 ms, total: 661 ms\n",
      "Wall time: 6.62 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\nIn response to Russia's aggression in Ukraine, the United States and its allies are imposing economic sanctions and taking action to hold Russian oligarchs accountable. The US Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs and seize their assets. The free world is standing together to show that freedom will always triumph over tyranny.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed85e9d",
   "metadata": {},
   "source": [
    "When we run it again, we see that it runs substantially faster but the final answer is different. This is due to caching at the map steps, but not at the reduce step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39cbb282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 ms, sys: 5.19 ms, total: 15.7 ms\n",
      "Wall time: 2.13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe United States and its allies are taking action to punish Russia for its invasion of Ukraine, including seizing assets, closing off airspace, and providing economic and military assistance to Ukraine. The US is also mobilizing forces to protect NATO countries and has released 30 million barrels of oil from its Strategic Petroleum Reserve to help blunt gas prices. The world is uniting in support of Ukraine and democracy, and the US stands with its Ukrainian American citizens.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0dab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
