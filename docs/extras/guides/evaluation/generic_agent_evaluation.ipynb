{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Agent Trajectories\n",
    "\n",
    "Good evaluation is key for quickly iterating on your agent's prompts and tools. Here we provide an example of how to use the TrajectoryEvalChain to evaluate the efficacy of the actions taken by your agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by defining our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import Wikipedia\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents.react.base import DocstoreExplorer\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import LLMMathChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain import SerpAPIWrapper\n",
    "\n",
    "docstore = DocstoreExplorer(Wikipedia())\n",
    "\n",
    "math_llm = OpenAI(temperature=0)\n",
    "\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=math_llm, verbose=True)\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=docstore.search,\n",
    "        description=\"useful for when you need to ask with search. Must call before lookup.\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Lookup\",\n",
    "        func=docstore.lookup,\n",
    "        description=\"useful for when you need to ask with lookup. Only call after a successfull 'Search'.\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for doing calculations. Expects strict numeric input.\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Search-the-Web-SerpAPI\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", return_messages=True, output_key=\"output\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-0613\")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    "    return_intermediate_steps=True,  # This is needed for the evaluation later\n",
    "    handle_parsing_errors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Agent\n",
    "\n",
    "Now let's try our agent out on some example queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_one = (\n",
    "    \"How many ping pong balls would it take to fill the entire Empire State Building?\"\n",
    ")\n",
    "\n",
    "test_outputs_one = agent({\"input\": query_one}, return_only_outputs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks alright.. Let's try it out on another query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Search` with `length of the US from coast to coast`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "== Watercraft ==\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Search` with `distance from coast to coast of the US`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe Oregon Coast is a coastal region of the U.S. state of Oregon. It is bordered by the Pacific Ocean to its west and the Oregon Coast Range to the east, and stretches approximately 362 miles (583 km) from the California state border in the south to the Columbia River in the north. The region is not a specific geological, environmental, or political entity, and includes the Columbia River Estuary.\n",
      "The Oregon Beach Bill of 1967 allows free beach access to everyone.  In return for a pedestrian easement and relief from construction, the bill eliminates property taxes on private beach land and allows its owners to retain certain beach land rights.Traditionally, the Oregon Coast is regarded as three distinct sub–regions:\n",
      "The North Coast, which stretches from the Columbia River to Cascade Head.\n",
      "The Central Coast, which stretches from Cascade Head to Reedsport.\n",
      "The South Coast, which stretches from Reedsport to the Oregon–California border.The largest city is Coos Bay, population 16,700 in Coos County on the South Coast. U.S. Route 101 is the primary highway from Brookings to Astoria and is known for its scenic overlooks of the Pacific Ocean. Over 80 state parks and recreation areas dot the Oregon Coast. However, only a few highways cross the Coast Range to the interior: US 30, US 26, OR 6, US 20, OR 18, OR 34, OR 126, OR 38, and OR 42.  OR 18 and US 20 are considered among the dangerous roads in the state.The Oregon Coast includes Clatsop County, Tillamook County, Lincoln County, western Lane County, western Douglas County, Coos County, and Curry County.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Calculator` with `362 miles`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "362 miles\u001b[32;1m\u001b[1;3m```text\n",
      "362 * 1609.344\n",
      "```\n",
      "...numexpr.evaluate(\"362 * 1609.344\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m582582.528\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[38;5;200m\u001b[1;3mAnswer: 582582.528\u001b[0m\u001b[32;1m\u001b[1;3mIf we assume that the distance from coast to coast of the US is approximately 582,582.528 miles, and the height of the Eiffel Tower is 1,063 feet, we can calculate the number of Eiffel Towers needed to cover the US from coast to coast.\n",
      "\n",
      "First, we need to convert the height of the Eiffel Tower from feet to miles. Since 1 mile is equal to 5,280 feet, the height of the Eiffel Tower in miles is approximately 0.201325 miles (1,063 feet / 5,280 feet per mile).\n",
      "\n",
      "Next, we divide the distance from coast to coast of the US by the height of the Eiffel Tower in miles:\n",
      "\n",
      "582,582.528 miles / 0.201325 miles = 2,894,736.842\n",
      "\n",
      "Therefore, you would need approximately 2,894,737 Eiffel Towers laid end to end to cover the US from coast to coast.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query_two = \"If you laid the Eiffel Tower end to end, how many would you need cover the US from coast to coast?\"\n",
    "\n",
    "test_outputs_two = agent({\"input\": query_two}, return_only_outputs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look so good. Let's try running some evaluation.\n",
    "\n",
    "## Evaluating the Agent\n",
    "\n",
    "Let's start by defining the TrajectoryEvalChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation.agents import TrajectoryEvalChain\n",
    "\n",
    "# Define chain\n",
    "eval_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "eval_chain = TrajectoryEvalChain.from_llm(\n",
    "    llm=eval_llm,  # Note: This must be a chat model\n",
    "    agent_tools=agent.tools,\n",
    "    return_reasoning=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try evaluating the first query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Chain.__call__() got an unexpected keyword argument 'reference'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m question, steps, answer \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     test_outputs_one[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      3\u001b[0m     test_outputs_one[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintermediate_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m     test_outputs_one[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m \u001b[43meval_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_trajectory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_outputs_one\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_outputs_one\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_trajectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_outputs_one\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mintermediate_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore from 1 to 5: \u001b[39m\u001b[38;5;124m\"\u001b[39m, evaluation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReasoning: \u001b[39m\u001b[38;5;124m\"\u001b[39m, evaluation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/code/lc/lckg/langchain/evaluation/agents/trajectory_eval_chain.py:257\u001b[0m, in \u001b[0;36mTrajectoryEvalChain.evaluate_trajectory\u001b[0;34m(self, input, agent_trajectory, output, reference)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate a trajectory.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    dict: The evaluation result.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_trajectory\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_agent_trajectory(agent_trajectory),\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: output,\n\u001b[1;32m    256\u001b[0m }\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Chain.__call__() got an unexpected keyword argument 'reference'"
     ]
    }
   ],
   "source": [
    "question, steps, answer = (\n",
    "    test_outputs_one[\"input\"],\n",
    "    test_outputs_one[\"intermediate_steps\"],\n",
    "    test_outputs_one[\"output\"],\n",
    ")\n",
    "\n",
    "evaluation = eval_chain.evaluate_intermediate_steps(\n",
    "    input=test_outputs_one[\"input\"],\n",
    "    output=test_outputs_one[\"output\"],\n",
    "    intermediate_steps=test_outputs_one[\"intermediate_steps\"],\n",
    ")\n",
    "\n",
    "print(\"Score from 1 to 5: \", evaluation[\"score\"])\n",
    "print(\"Reasoning: \", evaluation[\"reasoning\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems about right. Let's try the second query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, steps, answer = (\n",
    "    test_outputs_two[\"input\"],\n",
    "    test_outputs_two[\"intermediate_steps\"],\n",
    "    test_outputs_two[\"output\"],\n",
    ")\n",
    "\n",
    "evaluation = eval_chain(\n",
    "    inputs={\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"agent_trajectory\": eval_chain.get_agent_trajectory(steps),\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Score from 1 to 5: \", evaluation[\"score\"])\n",
    "print(\"Reasoning: \", evaluation[\"reasoning\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That also sounds about right. In conclusion, the TrajectoryEvalChain allows us to use GPT-4 to score both our agent's outputs and tool use in addition to giving us the reasoning behind the evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "06ba49dd587e86cdcfee66b9ffe769e1e94f0e368e54c2d6c866e38e33c0d9b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
