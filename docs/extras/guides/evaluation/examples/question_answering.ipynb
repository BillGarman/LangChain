{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480b7cf8",
   "metadata": {},
   "source": [
    "# Question Answering\n",
    "\n",
    "This notebook covers how to evaluate generic question answering problems. This is a situation where you have an example containing a question and its corresponding ground truth answer, and you want to measure how well the language model does at answering those questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e3023b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For demonstration purposes, we will just evaluate a simple question answering system that only evaluates the model's internal knowledge. Please see the [Data Augmented Question Answering](data_augmented_qa.ipynb) guide for an examples evaluating a Q&A system over data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96710d50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e33ccf00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"You are a helpful AI assistant.\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c584440",
   "metadata": {},
   "source": [
    "## Examples\n",
    "For this purpose, we will just use two simple hardcoded examples, but see other notebooks for tips on how to get and/or generate these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87de1d84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\",\n",
    "        \"answer\": \"11\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": 'Is the following sentence plausible? \"Joao Moutinho caught the screen pass in the NFC championship.\"',\n",
    "        \"answer\": \"No\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b1155",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "We can now make and inspect the predictions for these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7bd809c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roger initially has 5 tennis balls. He buys 2 cans of tennis balls, and each can has 3 tennis balls. So, he has 2 * 3 = <<2*3=6>>6 additional tennis balls.\n",
      "Therefore, Roger now has a total of 5 + 6 = <<5+6=11>>11 tennis balls.\n",
      "\n",
      "No, the sentence is not plausible. Joao Moutinho is not a football player, and the NFC championship is a game in American football, not soccer.\n"
     ]
    }
   ],
   "source": [
    "predictions = chain.apply(examples)\n",
    "print(\"\\n\\n\".join([pred['text'] for pred in predictions]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc2f9d",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We can see that if we tried to just do exact match on the answer answers (`11` and `No`) they would not match what the language model answered. However, semantically the language model is correct in both cases. In order to account for this, we can use a language model itself to evaluate the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cacc65a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aa6cd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_results = [\n",
    "    evaluator.evaluate_strings(\n",
    "        input=eg['question'],\n",
    "        prediction=pred['text'],\n",
    "        reference=eg['answer'],\n",
    "    )\n",
    "    for eg, pred in zip(examples, predictions)\n",
    "]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63780020",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "Real Answer: 11\n",
      "Predicted Answer: Roger initially has 5 tennis balls. He buys 2 cans of tennis balls, and each can has 3 tennis balls. So, he has 2 * 3 = <<2*3=6>>6 additional tennis balls.\n",
      "Therefore, Roger now has a total of 5 + 6 = <<5+6=11>>11 tennis balls.\n",
      "Predicted Result: CORRECT\n",
      "\n",
      "Example 1:\n",
      "Question: Is the following sentence plausible? \"Joao Moutinho caught the screen pass in the NFC championship.\"\n",
      "Real Answer: No\n",
      "Predicted Answer: No, the sentence is not plausible. Joao Moutinho is not a football player, and the NFC championship is a game in American football, not soccer.\n",
      "Predicted Result: CORRECT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (eval_res, eg, pred) in enumerate(zip(eval_results, examples, predictions)):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + eg[\"question\"])\n",
    "    print(\"Real Answer: \" + eg[\"answer\"])\n",
    "    print(\"Predicted Answer: \" + pred[\"text\"])\n",
    "    print(\"Predicted Result: \" + eval_res['value'])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "53f3bc57609c7a84333bb558594977aa5b4026b1d6070b93987956689e367341"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
