{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4596ea-a631-416d-a2a4-3577c140493d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LangSmith Walkthrough\n",
    "\n",
    "LangChain makes it easy to prototype LLM applications and Agents. However, delivering LLM applications to production can be deceptively difficult. You will likely have to heavily customize and iterate on your prompts, chains, and other components to create a high-quality product.\n",
    "\n",
    "To aid in this process, we've launched LangSmith, a unified platform for debugging, testing, and monitoring your LLM applications.\n",
    "\n",
    "When might this come in handy? You may find it useful when you want to:\n",
    "\n",
    "- Quickly debug a new chain, agent, or set of tools\n",
    "- Visualize how components (chains, llms, retrievers, etc.) relate and are used\n",
    "- Evaluate different prompts and LLMs for a single component\n",
    "- Run a given chain several times over a dataset to ensure it consistently meets a quality bar\n",
    "- Capture usage traces and using LLMs or analytics pipelines to generate insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138fbb8f-960d-4d26-9dd5-6d6acab3ee55",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**[Create a LangSmith account](https://smith.langchain.com/) and create an API key (see bottom left corner). Familiarize yourself with the platform by looking through the [docs](https://docs.smith.langchain.com/)**\n",
    "\n",
    "Note LangSmith is in closed beta; we're in the process of rolling it out to more users. However, you can fill out the form on the website for expedited access.\n",
    "\n",
    "Now, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d77d064-41b4-41fb-82e6-2d16461269ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Log runs to LangSmith\n",
    "\n",
    "First, configure your environment variables to tell LangChain to log traces. This is done by setting the `LANGCHAIN_TRACING_V2` environment variable to true.\n",
    "You can tell LangChain which project to log to by setting the `LANGCHAIN_PROJECT` environment variable (if this isn't set, runs will be logged to the `default` project). This will automatically create the project for you if it doesn't exist. You must also set the `LANGCHAIN_ENDPOINT` and `LANGCHAIN_API_KEY` environment variables.\n",
    "\n",
    "For more information on other ways to set up tracing, please reference the [LangSmith documentation](https://docs.smith.langchain.com/docs/).\n",
    "\n",
    "**NOTE:** You must also set your `OPENAI_API_KEY` and `SERPAPI_API_KEY` environment variables in order to run the following tutorial.\n",
    "\n",
    "**NOTE:** You can only access an API key when you first create it. Keep it somewhere safe.\n",
    "\n",
    "**NOTE:** You can also use a context manager in python to log traces using\n",
    "```python\n",
    "from langchain.callbacks.manager import tracing_v2_enabled\n",
    "\n",
    "with tracing_v2_enabled(project_name=\"My Project\"):\n",
    "    agent.run(\"How many people live in canada as of 2023?\")\n",
    "```\n",
    "\n",
    "However, in this example, we will use environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4780363-f05a-4649-8b1a-9b449f960ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -U langchain langsmith --quiet\n",
    "%pip install openai tiktoken pandas html2text faiss-cpu duckduckgo-search --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904db9a5-f387-4a57-914c-c8af8d39e249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "unique_id = uuid4().hex[0:8]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"\"  # Update to your API key\n",
    "\n",
    "# Used by the agent in this tutorial\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAI-API-KEY>\"\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = \"<YOUR-SERPAPI-API-KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee7f34b-b65c-4e09-ad52-e3ace78d0221",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create the langsmith client to interact with the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "510b5ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27fa11-ddce-4af0-971e-c5c37d5b92ef",
   "metadata": {},
   "source": [
    "Create a LangChain component and log runs to the platform. In this example, we will create a ReAct-style agent with access to a general search tool (DuckDuckGo) as well as a vector store retrieval tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2dd075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wfh/.pyenv/versions/3.11.2/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import RecursiveUrlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "api_loader = RecursiveUrlLoader(\"https://docs.smith.langchain.com\")\n",
    "text_splitter = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "doc_transformer = Html2TextTransformer()\n",
    "raw_documents = api_loader.load()\n",
    "transformed = doc_transformer.transform_documents(raw_documents)\n",
    "documents = text_splitter.split_documents(transformed)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c801853-8e96-404d-984c-51ace59cbbef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.agents.agent_toolkits.conversational_retrieval.tool import (\n",
    "    create_retriever_tool,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import DuckDuckGoSearchResults, StructuredTool\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    DuckDuckGoSearchResults(), # General internet search using DuckDuckGo\n",
    "    create_retriever_tool( # Search the Langsmith documentation\n",
    "        retriever,\n",
    "        name=\"query-langsmith-docs\",\n",
    "        description=\"Query the Langsmith documentation using semantic search.\",\n",
    "    )\n",
    "]\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab51e1e-8270-452c-ba22-22b5b5951899",
   "metadata": {},
   "source": [
    "We are running the agent concurrently on multiple inputs to reduce latency. Runs get logged to LangSmith in the background so execution latency is unaffected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19537902-b95c-4390-80a4-f6c9a937081e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"What is LangChain?\",\n",
    "    \"How might I query for all runs in a project?\",\n",
    "    \"What's a langsmith dataset?\",\n",
    "    \"How do I use a traceable decorator?\",\n",
    "    \"Can I trace my Llama V2 llm?\",\n",
    "    \"Why do I have to set environment variables?\",\n",
    "    \"How do I move my project between organizations?\",\n",
    "    \"How do I search for a run with metadata?\",\n",
    "]\n",
    "\n",
    "results = agent.batch(inputs, return_exceptions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6a764c-5d7a-4de7-a916-3ecc987d5bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'What is LangChain?',\n",
       "  'output': 'LangChain is a framework for developing applications powered by language models.'},\n",
       " InvalidRequestError(message=\"This model's maximum context length is 16385 tokens. However, your messages resulted in 16771 tokens. Please reduce the length of the messages.\", param='messages', code='context_length_exceeded', http_status=400, request_id=None),\n",
       " langchain.schema.output_parser.OutputParserException('Could not parse LLM output: `Based on the observation, a langsmith dataset is a curated dataset that can be used for testing and evaluating changes to a prompt or chain in LangSmith. It can be used to run the chain over the data points and visualize the outputs. The LangSmith client makes it easy to pull down a dataset and run a chain over them, logging the results to a new project associated with the dataset. The dataset can also be exported for use in other contexts.`')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This weaker agent returns a number of errors\n",
    "results[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9decb964-be07-4b6c-9802-9825c8be7b64",
   "metadata": {},
   "source": [
    "Assuming you've successfully set up your environment, your agent traces should show up in the `Projects` section in the [app](https://smith.langchain.com/). Congrats!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c43c311-4e09-4d57-9ef3-13afb96ff430",
   "metadata": {},
   "source": [
    "## Evaluate another agent implementation\n",
    "\n",
    "In addition to logging runs, LangSmith also allows you to test and evaluate your LLM applications.\n",
    "\n",
    "In this section, you will leverage LangSmith to create a benchmark dataset and run AI-assisted evaluators on an agent. You will do so in a few steps:\n",
    "\n",
    "1. Create a dataset from pre-existing run inputs and outputs\n",
    "2. Initialize a new agent to benchmark\n",
    "3. Configure evaluators to grade an agent's output\n",
    "4. Run the agent over the dataset and evaluate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab1a29-b79d-4a99-b5b1-0870c2d772b1",
   "metadata": {},
   "source": [
    "### 1. Create a LangSmith dataset\n",
    "\n",
    "Below, we use the LangSmith client to create a dataset from the input questions from above and a list labels. You will use these later to measure performance for a new agent. A dataset is a collection of examples, which are nothing more than input-output pairs you can use as test cases to your application.\n",
    "\n",
    "For more information on datasets, including how to create them from CSVs or other files or how to create them in the platform, please refer to the [LangSmith documentation](https://docs.smith.langchain.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43fd40b2-3f02-4e51-9343-705aafe90a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [\n",
    "    \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\",\n",
    "    \"client.list_runs(project_name='my-project-name'), or in TypeScript, client.ListRuns({projectName: 'my-project-name'})\",\n",
    "    \"A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.\",\n",
    "    \"\"\"The traceable decorator is available in the langsmith python SDK. To use, configure your environment with your API key,import the required function, decorate your function, and then call the function. Below is an example:\n",
    "```python\n",
    "from langsmith.run_helpers import traceable\n",
    "@traceable(run_type=\"chain\") # or \"llm\", etc.\n",
    "def my_function(input_param):\n",
    "    # Function logic goes here\n",
    "    return output\n",
    "result = my_function(input_param)\n",
    "```\"\"\",\n",
    "    \"So long as you are using one of LangChain's LLM implementations, all your calls can be traced\",\n",
    "    \"Environment variables can tell your LangChain application to perform tracing and contain the information necessary to authenticate to LangSmith. While there are other ways to connect, environment variables tend to be the simplest way to configure your application.\",\n",
    "    \"LangSmith doesn't directly support moving projects between organizations.\",\n",
    "    \"\"\"You can search for runs with specific metadata using the filter 'has(metadata, \"<json-search>\")'. An example in python is:\n",
    "```python\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "runs = list(client.list_runs(\n",
    "    project_name=\"<your_project>\",\n",
    "    filter='has(metadata, '{\"variant\": \"abc123\"}')',\n",
    "))```\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17580c4b-bd04-4dde-9d21-9d4edd25b00d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = f\"langsmith-docs-dataset-{unique_id}\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name, description=\"An example dataset of questions over the LangSmith documentation.\"\n",
    ")\n",
    "\n",
    "for query, answer in zip(inputs, outputs):\n",
    "    client.create_example(inputs={\"input\": query}, outputs={\"output\": answer}, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adfd29c-b258-49e5-94b4-74597a12ba16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Initialize a new agent to benchmark\n",
    "\n",
    "LangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn't shared between dataset runs, we will pass in a `chain_factory` (aka a `constructor`) function to initialize for each call.\n",
    "\n",
    "In this case, we will test an agent that uses OpenAI's function calling endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f42d8ecc-d46a-448b-a89c-04b0f6907f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wfh/code/lc/langchain/libs/langchain/langchain/__init__.py:24: UserWarning: Importing hub from langchain root module is no longer supported.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools, AgentExecutor\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "# Since chains can be stateful (e.g. they can have memory), we provide\n",
    "# a way to initialize a new chain for each row in the dataset. This is done\n",
    "# by passing in a factory function that returns a new chain for each row.\n",
    "def agent_factory():\n",
    "    prompt = hub.pull(\"wfh/langsmith-agent-prompt\")\n",
    "    \n",
    "    llm_with_tools = llm.bind(\n",
    "        functions=[format_tool_to_openai_function(t) for t in tools]\n",
    "    )\n",
    "    runnable_agent = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"agent_scratchpad\": lambda x: format_to_openai_functions(x['intermediate_steps'])\n",
    "            } \n",
    "             | prompt \n",
    "             | llm_with_tools \n",
    "             | OpenAIFunctionsAgentOutputParser()\n",
    "    )\n",
    "    return  AgentExecutor(agent=runnable_agent, tools=tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9ef53",
   "metadata": {},
   "source": [
    "### 3. Configure evaluation\n",
    "\n",
    "Manually comparing the results of chains in the UI is effective, but it can be time consuming.\n",
    "It can be helpful to use automated metrics and AI-assisted feedback to evaluate your component's performance.\n",
    "\n",
    "Below, we will create some pre-implemented run evaluators that do the following:\n",
    "- Compare results against ground truth labels.\n",
    "- Measure semantic (dis)similarity using embedding distance\n",
    "- Evaluate 'aspects' of the agent's response in a reference-free manner using custom criteria\n",
    "\n",
    "For a longer discussion of how to select an appropriate evaluator for your use case and how to create your own\n",
    "custom evaluators, please refer to the [LangSmith documentation](https://docs.smith.langchain.com/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a25dc281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator\n",
    "    evaluators=[\n",
    "        # Measures whether a QA response is \"Correct\", based on a reference answer\n",
    "        # You can also select via the raw string \"qa\"\n",
    "        EvaluatorType.QA,\n",
    "        # Measure the embedding distance between the output and the reference answer\n",
    "        # Equivalent to: EvalConfig.EmbeddingDistance(embeddings=OpenAIEmbeddings())\n",
    "        EvaluatorType.EMBEDDING_DISTANCE,\n",
    "        # Grade whether the output satisfies the stated criteria. You can select a default one such as \"helpfulness\" or provide your own.\n",
    "        RunEvalConfig.LabeledCriteria(\"helpfulness\"),\n",
    "        # Both the Criteria and LabeledCriteria evaluators can be configured with a dictionary of custom criteria.\n",
    "        RunEvalConfig.Criteria(\n",
    "            {\n",
    "                \"fifth-grader-score\": \"Do you have to be smarter than a fifth grader to answer this question?\"\n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    "    # You can add custom StringEvaluator or RunEvaluator objects here as well, which will automatically be\n",
    "    # applied to each prediction. Check out the docs for examples.\n",
    "    custom_evaluators=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07885b10",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Run the agent and evaluators\n",
    "\n",
    "Use the [run_on_dataset](https://api.python.langchain.com/en/latest/smith/langchain.smith.evaluation.runner_utils.run_on_dataset.html#langchain.smith.evaluation.runner_utils.run_on_dataset) (or asynchronous [arun_on_dataset](https://api.python.langchain.com/en/latest/smith/langchain.smith.evaluation.runner_utils.arun_on_dataset.html#langchain.smith.evaluation.runner_utils.arun_on_dataset)) function to evaluate your model. This will:\n",
    "1. Fetch example rows from the specified dataset.\n",
    "2. Run your agent (or any custom function) on each example.\n",
    "3. Apply evalutors to the resulting run traces and corresponding reference examples to generate automated feedback.\n",
    "\n",
    "The results will be visible in the LangSmith app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3733269b-8085-4644-9d5d-baedcff13a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'openai-functions-agent-test-e36e3f91' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/4219e049-472b-4dce-ac02-1ba390379fb5\n",
      "[------------------------------------------------->] 8/8\n",
      " Eval quantiles:\n",
      "                               0.25       0.5      0.75      mean      mode\n",
      "correctness                0.000000  0.000000  1.000000  0.375000  0.000000\n",
      "embedding_cosine_distance  0.139897  0.166687  0.184377  0.154261  0.072665\n",
      "helpfulness                0.000000  0.500000  1.000000  0.500000  0.000000\n",
      "fifth-grader-score         0.000000  0.000000  1.000000  0.375000  0.000000\n"
     ]
    }
   ],
   "source": [
    "from langchain.smith import (\n",
    "    arun_on_dataset,\n",
    "    run_on_dataset, \n",
    ")\n",
    "\n",
    "chain_results = run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=agent_factory,\n",
    "    evaluation=evaluation_config,\n",
    "    verbose=True,\n",
    "    client=client,\n",
    "    project_name=f\"runnable-agent-test-{unique_id}\",\n",
    "    tags=[\"testing-notebook\"],  # Optional, adds a tag to the resulting chain runs\n",
    ")\n",
    "\n",
    "# Sometimes, the agent will error due to parsing issues, incompatible tool inputs, etc.\n",
    "# These are logged as warnings here and captured as errors in the tracing UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdacd159-eb4d-49e9-bb2a-c55322c40ed4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Review the test results\n",
    "\n",
    "You can review the test results tracing UI below by clicking the URL in the output above or navigating to the \"Testing & Datasets\" page in LangSmith  **\"f\"langsmith-docs-dataset-{unique_id}\"*\"** dataset. \n",
    "\n",
    "This will show the new runs and the feedback logged from the selected evaluators. You can also explore a summary of the results in tabular format below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9da60638-5be8-4b5f-a721-2c6627aeaf0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correctness</th>\n",
       "      <th>embedding_cosine_distance</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>fifth-grader-score</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1e9fbacd-1655-43fd-88ea-5837ccb74394</th>\n",
       "      <td>0</td>\n",
       "      <td>0.155596</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'input': 'How do I search for a run with meta...</td>\n",
       "      <td>{'input': 'How do I search for a run with meta...</td>\n",
       "      <td>{'output': 'You can search for runs with speci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a0691ae7-2fe5-4f64-878f-a67f68f68416</th>\n",
       "      <td>0</td>\n",
       "      <td>0.182275</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'input': 'How do I move my project between or...</td>\n",
       "      <td>{'input': 'How do I move my project between or...</td>\n",
       "      <td>{'output': 'LangSmith doesn't directly support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4bdcd32-0ac4-4eac-b913-7c46ba37b879</th>\n",
       "      <td>1</td>\n",
       "      <td>0.154744</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>{'input': 'Why do I have to set environment va...</td>\n",
       "      <td>{'input': 'Why do I have to set environment va...</td>\n",
       "      <td>{'output': 'Environment variables can tell you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6429c1e0-11c4-4d0b-b277-b4ba7e43b7bf</th>\n",
       "      <td>0</td>\n",
       "      <td>0.204993</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'input': 'Can I trace my Llama V2 llm?'}</td>\n",
       "      <td>{'input': 'Can I trace my Llama V2 llm?', 'out...</td>\n",
       "      <td>{'output': 'So long as you are using one of La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64677fff-5c59-40f3-b7c4-95df1c2411a7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.072665</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'input': 'How do I use a traceable decorator?'}</td>\n",
       "      <td>{'input': 'How do I use a traceable decorator?...</td>\n",
       "      <td>{'output': 'The traceable decorator is availab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>015f3a00-0587-41ee-b6bd-d195c50a96d9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.095354</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>{'input': 'What's a langsmith dataset?'}</td>\n",
       "      <td>{'input': 'What's a langsmith dataset?', 'outp...</td>\n",
       "      <td>{'output': 'A LangSmith dataset is a collectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cd1c6874-ff6f-4e55-92ad-1f80d8037f09</th>\n",
       "      <td>0</td>\n",
       "      <td>0.190684</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>{'input': 'How might I query for all runs in a...</td>\n",
       "      <td>{'input': 'How might I query for all runs in a...</td>\n",
       "      <td>{'output': 'client.list_runs(project_name='my-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d436add4-3eb0-417c-9998-59dbc53b70a8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'input': 'What is LangChain?'}</td>\n",
       "      <td>{'input': 'What is LangChain?', 'output': 'I'm...</td>\n",
       "      <td>{'output': 'LangChain is an open-source framew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      correctness  embedding_cosine_distance  \\\n",
       "1e9fbacd-1655-43fd-88ea-5837ccb74394            0                   0.155596   \n",
       "a0691ae7-2fe5-4f64-878f-a67f68f68416            0                   0.182275   \n",
       "d4bdcd32-0ac4-4eac-b913-7c46ba37b879            1                   0.154744   \n",
       "6429c1e0-11c4-4d0b-b277-b4ba7e43b7bf            0                   0.204993   \n",
       "64677fff-5c59-40f3-b7c4-95df1c2411a7            1                   0.072665   \n",
       "015f3a00-0587-41ee-b6bd-d195c50a96d9            1                   0.095354   \n",
       "cd1c6874-ff6f-4e55-92ad-1f80d8037f09            0                   0.190684   \n",
       "d436add4-3eb0-417c-9998-59dbc53b70a8            0                   0.177778   \n",
       "\n",
       "                                      helpfulness  fifth-grader-score  \\\n",
       "1e9fbacd-1655-43fd-88ea-5837ccb74394            0                   0   \n",
       "a0691ae7-2fe5-4f64-878f-a67f68f68416            0                   0   \n",
       "d4bdcd32-0ac4-4eac-b913-7c46ba37b879            1                   0   \n",
       "6429c1e0-11c4-4d0b-b277-b4ba7e43b7bf            0                   1   \n",
       "64677fff-5c59-40f3-b7c4-95df1c2411a7            1                   1   \n",
       "015f3a00-0587-41ee-b6bd-d195c50a96d9            1                   0   \n",
       "cd1c6874-ff6f-4e55-92ad-1f80d8037f09            1                   0   \n",
       "d436add4-3eb0-417c-9998-59dbc53b70a8            0                   1   \n",
       "\n",
       "                                                                                  input  \\\n",
       "1e9fbacd-1655-43fd-88ea-5837ccb74394  {'input': 'How do I search for a run with meta...   \n",
       "a0691ae7-2fe5-4f64-878f-a67f68f68416  {'input': 'How do I move my project between or...   \n",
       "d4bdcd32-0ac4-4eac-b913-7c46ba37b879  {'input': 'Why do I have to set environment va...   \n",
       "6429c1e0-11c4-4d0b-b277-b4ba7e43b7bf          {'input': 'Can I trace my Llama V2 llm?'}   \n",
       "64677fff-5c59-40f3-b7c4-95df1c2411a7   {'input': 'How do I use a traceable decorator?'}   \n",
       "015f3a00-0587-41ee-b6bd-d195c50a96d9           {'input': 'What's a langsmith dataset?'}   \n",
       "cd1c6874-ff6f-4e55-92ad-1f80d8037f09  {'input': 'How might I query for all runs in a...   \n",
       "d436add4-3eb0-417c-9998-59dbc53b70a8                    {'input': 'What is LangChain?'}   \n",
       "\n",
       "                                                                                 output  \\\n",
       "1e9fbacd-1655-43fd-88ea-5837ccb74394  {'input': 'How do I search for a run with meta...   \n",
       "a0691ae7-2fe5-4f64-878f-a67f68f68416  {'input': 'How do I move my project between or...   \n",
       "d4bdcd32-0ac4-4eac-b913-7c46ba37b879  {'input': 'Why do I have to set environment va...   \n",
       "6429c1e0-11c4-4d0b-b277-b4ba7e43b7bf  {'input': 'Can I trace my Llama V2 llm?', 'out...   \n",
       "64677fff-5c59-40f3-b7c4-95df1c2411a7  {'input': 'How do I use a traceable decorator?...   \n",
       "015f3a00-0587-41ee-b6bd-d195c50a96d9  {'input': 'What's a langsmith dataset?', 'outp...   \n",
       "cd1c6874-ff6f-4e55-92ad-1f80d8037f09  {'input': 'How might I query for all runs in a...   \n",
       "d436add4-3eb0-417c-9998-59dbc53b70a8  {'input': 'What is LangChain?', 'output': 'I'm...   \n",
       "\n",
       "                                                                              reference  \n",
       "1e9fbacd-1655-43fd-88ea-5837ccb74394  {'output': 'You can search for runs with speci...  \n",
       "a0691ae7-2fe5-4f64-878f-a67f68f68416  {'output': 'LangSmith doesn't directly support...  \n",
       "d4bdcd32-0ac4-4eac-b913-7c46ba37b879  {'output': 'Environment variables can tell you...  \n",
       "6429c1e0-11c4-4d0b-b277-b4ba7e43b7bf  {'output': 'So long as you are using one of La...  \n",
       "64677fff-5c59-40f3-b7c4-95df1c2411a7  {'output': 'The traceable decorator is availab...  \n",
       "015f3a00-0587-41ee-b6bd-d195c50a96d9  {'output': 'A LangSmith dataset is a collectio...  \n",
       "cd1c6874-ff6f-4e55-92ad-1f80d8037f09  {'output': 'client.list_runs(project_name='my-...  \n",
       "d436add4-3eb0-417c-9998-59dbc53b70a8  {'output': 'LangChain is an open-source framew...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_results.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591c819e-9932-45cf-adab-63727dd49559",
   "metadata": {},
   "source": [
    "## Exporting datasets and runs\n",
    "\n",
    "LangSmith lets you export data to common formats such as CSV or JSONL directly in the web app. You can also use the client to fetch runs for further analysis, to store in your own database, or to share with others. Let's fetch the run traces from the evaluation run.\n",
    "\n",
    "**Note: It may be a few moments before all the runs are accessible.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33bfefde-d1bb-4f50-9f7a-fd572ee76820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs = client.list_runs(project_name=chain_results[\"project_name\"], execution_order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6595c888-1f5c-4ae3-9390-0a559f5575d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After some time, these will be populated.\n",
    "client.read_project(project_name=chain_results[\"project_name\"]).feedback_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646f0fb-81d4-43ce-8a9b-54b8e19841e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You have succesfully traced and evaluated an agent using LangSmith!\n",
    "\n",
    "This was a quick guide to get started, but there are many more ways to use LangSmith to speed up your developer flow and produce better results.\n",
    "\n",
    "For more information on how you can get the most out of LangSmith, check out [LangSmith documentation](https://docs.smith.langchain.com/), and please reach out with questions, feature requests, or feedback at [support@langchain.dev](mailto:support@langchain.dev)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
