{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9acd2e",
   "metadata": {},
   "source": [
    "# Single Call Protocol\n",
    "\n",
    "In an effort to make it as easy as possible to create custom chains, we've implemented a \"Runnable\" protocol that most components implement. This is a standard interface with a few different methods, which makes it easy to define custom chains as well as making it possible to invoke them in a standard way. The standard interface exposed includes:\n",
    "\n",
    "- `stream`: stream back chunks of the response\n",
    "- `invoke`: call the chain on an input\n",
    "- `batch`: call the chain on a list of inputs\n",
    "\n",
    "These also have corresponding async methods:\n",
    "\n",
    "- `astream`: stream back chunks of the response async\n",
    "- `ainvoke`: call the chain on an input async\n",
    "- `abatch`: call the chain on a list of inputs async\n",
    "\n",
    "The type of the input varies by component - eg for a prompt it is a dictionary, for a retriever it is a single string.\n",
    "\n",
    "Let's take a look at a few different ways to do things!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa2c87",
   "metadata": {},
   "source": [
    "## PromptTemplate + LLM\n",
    "\n",
    "A PromptTemplate -> LLM is a core chain that is used in most other larger chains/systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "466b65b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.13) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c634ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1850a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {foo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d0669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bea9639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't bears wear shoes?\n",
      "\n",
      "Because they have bear feet!"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"foo\": \"bears\"}):\n",
    "    print(s.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9685de67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"foo\": \"bears\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3d0a6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why don't bears use cell phones?\\n\\nBecause they can't bear to have their calls dropped!\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"foo\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9098c5ed",
   "metadata": {},
   "source": [
    "## PromptTemplate + LLM + OutputParser\n",
    "\n",
    "We can also add in an output parser to easily trasform the raw LLM/ChatModel output into a more workable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f799664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import NoOpOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc194c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | NoOpOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77acf448",
   "metadata": {},
   "source": [
    "Notice that this now returns a string - a much more workable format for downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3d69a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here's a bear joke for you:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they already have bear feet!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"foo\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed58136",
   "metadata": {},
   "source": [
    "## Passthroughs and itemgetter\n",
    "\n",
    "Often times when constructing a chain you may want to pass along original input variables to future steps in the chain. How exactly you do this depends on what exactly the input is:\n",
    "\n",
    "- If the original input was a string, then you likely just want to pass along the string. This can be done with `RunnablePassthrough`. For an example of this, see `LLMChain + Retriever`\n",
    "- If the original input was a dictionary, then you likely want to pass along specific keys. This can be done with `itemgetter`. For an example of this see `Multiple LLM Chains`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d3d8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c5ef3d",
   "metadata": {},
   "source": [
    "## LLMChain + Retriever\n",
    "\n",
    "Let's now look at adding in a retrieval step, which adds up to a \"retrieval-augmented generation\" chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33be32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df3f3fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the retriever\n",
    "vectorstore = Chroma.from_texts([\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfc47ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eae31755",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | model | NoOpOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3040b0c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1d20c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = {\n",
    "    \"context\": itemgetter(\"question\") | retriever, \n",
    "    \"question\": itemgetter(\"question\"), \n",
    "    \"language\": itemgetter(\"language\")\n",
    "} | prompt | model | NoOpOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ee8b2d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Harrison ha lavorato a Kensho.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2bf8d3",
   "metadata": {},
   "source": [
    "## Multiple LLM Chains\n",
    "\n",
    "This can also be used to string together multiple LLMChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d65d4e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El pa√≠s en el que se encuentra la ciudad de Honolulu, en el estado de Haw√°i, es Estados Unidos.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"what country is the city {city} in? respond in {language}\")\n",
    "\n",
    "chain1 = prompt1 | model | NoOpOutputParser()\n",
    "\n",
    "chain2 = {\"city\": chain1, \"language\": itemgetter(\"language\")} | prompt2 | model | NoOpOutputParser()\n",
    "\n",
    "chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "878f8176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: no caching here, so it's bad!\n",
    "prompt1 = ChatPromptTemplate.from_template(\"generate a random color\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"what is a fruit of color: {color}\")\n",
    "prompt3 = ChatPromptTemplate.from_template(\"what is countries flag that has the color: {color}\")\n",
    "prompt4 = ChatPromptTemplate.from_template(\"What is the color of {fruit} and {country}\")\n",
    "chain1 = prompt1 | model | NoOpOutputParser()\n",
    "chain2 = {\n",
    "    \"fruit\": {\"color\": chain1} | prompt2 | model | NoOpOutputParser(),\n",
    "    \"country\": {\"color\": chain1} | prompt3 | model | NoOpOutputParser(),\n",
    "} | prompt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d621a870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='What is the color of A fruit that closely matches the color #FF7F50 is the persimmon. Persimmons are orange-red in color, and some varieties have a shade similar to #FF7F50. and The flag of the country with the color #9F51D9 is not identifiable as a specific country. Flags are unique to each country and have different colors, symbols, and designs.', additional_kwargs={}, example=False)])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc4bf6e",
   "metadata": {},
   "source": [
    "## Arbitrary Functions\n",
    "\n",
    "You can use arbitrary functions in the pipeline by using the `RunnableLambda`\n",
    "\n",
    "Note that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single input and unpacks it into multiple argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bb221b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "\n",
    "chain1 = prompt | model\n",
    "\n",
    "chain = {\n",
    "    \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
    "    \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")} | RunnableLambda(multiple_length_function)\n",
    "} | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5488ec85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='3 + 9 equals 12.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e9636",
   "metadata": {},
   "source": [
    "## SQL Database\n",
    "\n",
    "We can also try to replicate our SQLDatabaseChain using this style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a927516",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f51f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import SQLDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ccca6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(\"sqlite:///../../../notebooks/Chinook.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05ba88ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(_):\n",
    "    return db.get_table_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4eda902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "    return db.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5046cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_response = {\n",
    "    \"schema\": RunnableLambda(get_schema),\n",
    "    \"question\": itemgetter(\"question\")\n",
    "} | prompt | model | NoOpOutputParser() | RunnableLambda(run_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5552039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[(8,)]'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_response.invoke({\"question\": \"How many employees are there?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6fee130",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Response: {response}\"\"\"\n",
    "prompt_response = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923aa634",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"schema\": RunnableLambda(get_schema),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
