{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c9cbd6",
   "metadata": {},
   "source": [
    "# Fallbacks\n",
    "\n",
    "When working with language models, you may often encounter issues from the underlying APIs, whether these be rate limiting or downtime. Therefor, as you go to move your LLM applications into production it becomes more and more important to safe guard against these. That's why we've introduced the concept of fallbacks.\n",
    "\n",
    "Crucially, fallbacks can be applied not only on the LLM level but on the whole runnable level. This is important because often times different models require different prompts. So if your call to OpenAI fails, you don't just want to send the same prompt to Anthropic - you probably want want to use a different prompt template and send a different version there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8960c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "bad_llm = ChatOpenAI(model_name=\"gpt-fake\")\n",
    "good_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "llm = bad_llm.with_fallbacks([good_llm])\n",
    "\n",
    "result = llm.invoke(\"Why did the the chicken cross the road?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "566521cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To get to the other side.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb340e3e",
   "metadata": {},
   "source": [
    "We can add multiple fallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73700501",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = bad_llm.with_fallbacks([bad_llm, bad_llm, good_llm])\n",
    "\n",
    "result = llm.invoke(\"Why did the the chicken cross the road?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47398773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To get to the other side.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00bea25",
   "metadata": {},
   "source": [
    "We can use our \"LLM with Fallbacks\" as we would a normal LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f8eaaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's a great question! Kangaroos are known for their incredible hopping abilities, so maybe this particular kangaroo wanted to show off its impressive skills while crossing the road. I must say, kangaroos are truly fascinating creatures!\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You're a nice assistant who always includes a compliment in your response\"),\n",
    "        (\"human\", \"Why did the {animal} cross the road\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"animal\": \"kangaroo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10ac5e2",
   "metadata": {},
   "source": [
    "We can also create fallbacks for sequences, that are sequences themselves. Here we do that with two different models: ChatOpenAI and then normal OpenAI (which does not use a chat model). Because OpenAI is NOT a chat model, you likely want a different prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's create a chain with a ChatModel\n",
    "# We add in a string output parser here so the outputs between the two are the same type\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You're a nice assistant who always includes a compliment in your response\"),\n",
    "        (\"human\", \"Why did the {animal} cross the road\"),\n",
    "    ]\n",
    ")\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-fake\")\n",
    "bad_chain = chat_prompt | chat_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d1fc2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create a chain with the normal OpenAI model\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Instructions: You should always include a compliment in your response.\n",
    "\n",
    "Question: Why did the {animal} cross the road?\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "llm = OpenAI()\n",
    "good_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "283bfa44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nAnswer: He must have been in a hurry to get somewhere, but I'm sure he was careful while crossing. Good for him!\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now create a final chain which combines the two\n",
    "chain = bad_chain.with_fallbacks([good_chain])\n",
    "chain.invoke({\"animal\": \"turtle\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b0fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
