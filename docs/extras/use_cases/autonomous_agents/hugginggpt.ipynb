{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of [HuggingGPT](https://github.com/microsoft/JARVIS). HuggingGPT is a system to connect LLMs (ChatGPT) with ML community (Hugging Face).\n",
    "\n",
    "+ ðŸ”¥ Paper: https://arxiv.org/abs/2303.17580\n",
    "+ ðŸš€ Project: https://github.com/microsoft/JARVIS\n",
    "+ ðŸ¤— Space: https://huggingface.co/spaces/microsoft/HuggingGPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up tools\n",
    "\n",
    "We set up the tools available from [Transformers Agent](https://huggingface.co/docs/transformers/transformers_agents#tools). It includes a library of tools supported by Transformers and some customized tools such as image generator, video generator, text downloader and other tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenyl/miniconda/envs/langchain/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import load_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ImageTransformationTool implements a different description in its configuration and class. Using the tool configuration description.\n"
     ]
    }
   ],
   "source": [
    "hf_tools = [load_tool(tool_name) for tool_name in [\n",
    "    \"document-question-answering\", \n",
    "    \"image-captioning\", \n",
    "    \"image-question-answering\", \n",
    "    \"image-segmentation\", \n",
    "    \"speech-to-text\", \n",
    "    \"summarization\", \n",
    "    \"text-classification\", \n",
    "    \"text-question-answering\", \n",
    "    \"translation\", \n",
    "    \"huggingface-tools/text-to-image\", \n",
    "    \"huggingface-tools/text-to-video\", \n",
    "    \"text-to-speech\", \n",
    "    \"huggingface-tools/text-download\", \n",
    "    \"huggingface-tools/image-transformation\"\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model and HuggingGPT\n",
    "\n",
    "We create an instance of HuggingGPT and use ChatGPT as the controller to rule the above tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.experimental import HuggingGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_BASE=http://localhost:8000/v1\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "agent = HuggingGPT(llm, hf_tools)\n",
    "\n",
    "%env OPENAI_API_BASE=http://localhost:8000/v1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an example\n",
    "\n",
    "Given a text, show a related image and video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new TaskPlaningChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: #1 Task Planning Stage: The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\": task_id, \"dep\": dependency_task_id, \"args\": {\"input name\": text may contain <resource-dep_id>}}]. The special tag \"dep_id\" refer to the one generated text/image/audio in the dependency task (Please consider whether the dependency task generates resources of this type.) and \"dep_id\" must be in \"dep\" list. The \"dep\" field denotes the ids of the previous prerequisite tasks which generate a new resource that the current task relies on. The task MUST be selected from the following tools (along with tool description, input name and output type): ['document_qa: This is a tool that answers a question about an document (pdf). It takes an input named `document` which should be the document containing the information, as well as a `question` that is the question about the document. It returns a text that contains the answer to the question.', 'image_captioner: This is a tool that generates a description of an image. It takes an input named `image` which should be the image to caption, and returns a text that contains the description in English.', 'image_qa: This is a tool that answers a question about an image. It takes an input named `image` which should be the image containing the information, as well as a `question` which should be the question in English. It returns a text that is the answer to the question.', 'image_segmenter: This is a tool that creates a segmentation mask of an image according to a label. It cannot create an image.It takes two arguments named `image` which should be the original image, and `label` which should be a text describing the elements what should be identified in the segmentation mask. The tool returns the mask.', 'transcriber: This is a tool that transcribes an audio into text. It takes an input named `audio` and returns the transcribed text.', 'summarizer: This is a tool that summarizes an English text. It takes an input `text` containing the text to summarize, and returns a summary of the text.', 'text_classifier: This is a tool that classifies an English text using provided labels. It takes two inputs: `text`, which should be the text to classify, and `labels`, which should be the list of labels to use for classification. It returns the most likely label in the list of provided `labels` for the input text.', 'text_qa: This is a tool that answers questions related to a text. It takes two arguments named `text`, which is the text where to find the answer, and `question`, which is the question, and returns the answer to the question.', \"translator: This is a tool that translates text from a language to another. It takes three inputs: `text`, which should be the text to translate, `src_lang`, which should be the language of the text to translate and `tgt_lang`, which should be the language for the desired ouput language. Both `src_lang` and `tgt_lang` are written in plain English, such as 'Romanian', or 'Albanian'. It returns the text translated in `tgt_lang`.\", 'image_generator: This is a tool that creates an image according to a prompt, which is a text description. It takes an input named `prompt` which contains the image description and outputs an image.', 'video_generator: This is a tool that creates a video according to a text description. It takes an input named `prompt` which contains the image description, as well as an optional input `seconds` which will be the duration of the video. The default is of two seconds. The tool outputs a video object.', 'text_reader: This is a tool that reads an English text out loud. It takes an input named `text` which should contain the text to read (in English) and returns a waveform object containing the sound.', 'text_downloader: This is a tool that downloads a file from a `url`. It takes the `url` as input, and returns the text contained in the file.', 'image_transformer: This is a tool that transforms an image according to a prompt. It takes two inputs: `image`, which should be the image to transform, and `prompt`, which should be the prompt to use to change it. The prompt should only contain descriptive adjectives, as if completing the prompt of the original image. It returns the modified image.']. There may be multiple tasks of the same type. Think step by step about all the tasks needed to resolve the user's request. Parse out as few tasks as possible while ensuring that the user request can be resolved. Pay attention to the dependencies and order among tasks. If the user input can't be parsed, you need to reply empty JSON [].\n",
      "Human: please show me a video and an image of (based on the text) 'a boy is running' and dub it\n",
      "AI: [{\"task\": \"video_generator\", \"id\": 0, \"dep\": [-1], \"args\": {\"prompt\": \"a boy is running\" }}, {\"task\": \"text_reader\", \"id\": 1, \"dep\": [-1], \"args\": {\"text\": \"a boy is running\" }}, {\"task\": \"image_generator\", \"id\": 2, \"dep\": [-1], \"args\": {\"prompt\": \"a boy is running\" }}]\n",
      "Human: Give you some pictures e1.jpg, e2.png, e3.jpg, help me count the number of sheep?\n",
      "AI: [ {\"task\": \"image_qa\", \"id\": 0, \"dep\": [-1], \"args\": {\"image\": \"e1.jpg\", \"question\": \"How many sheep in the picture\"}}, {\"task\": \"image_qa\", \"id\": 1, \"dep\": [-1], \"args\": {\"image\": \"e2.jpg\", \"question\": \"How many sheep in the picture\"}}, {\"task\": \"image_qa\", \"id\": 2, \"dep\": [-1], \"args\": {\"image\": \"e3.jpg\", \"question\": \"How many sheep in the picture\"}}]\n",
      "Human: Now I input: please show me a video and an image of (based on the text) 'a boy is running'.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "running video_generator({'prompt': 'a boy is running'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:30<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running image_generator({'prompt': 'a boy is running'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:01<00:00, 14.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ResponseGenerationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe AI assistant has parsed the user input into several tasks and executed them. The results are as follows:\n",
      "video_generator({'prompt': 'a boy is running'})\n",
      "status: completed\n",
      "result: d39969.mp4\n",
      "image_generator({'prompt': 'a boy is running'})\n",
      "status: completed\n",
      "result: 6bb46f.png\n",
      "\n",
      "Please summarize the results and generate a response.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The AI assistant successfully executed multiple tasks based on the user input of \"a boy is running.\" A video generator created a video with the filename d39969.mp4 and an image generator created a PNG image with the filename 6bb46f.png. Both tasks were completed successfully.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"please show me a video and an image of (based on the text) 'a boy is running'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
