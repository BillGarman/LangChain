{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf13f702",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/extras/use_cases/summarization.ipynb)\n",
    "\n",
    "## Use case\n",
    "\n",
    "--- \n",
    "\n",
    "Suppose you have a set of documents (PDFs, Notion pages, customer questions, etc.) and you want to summarize the content. \n",
    "\n",
    "LLMs are a great tool for this given their proficiency in understanding and synthesizing text.\n",
    "\n",
    "In this walkthrough we'll go over how to perform document summarization using LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e233997",
   "metadata": {},
   "source": [
    "![Image description](/img/summarization_use_case_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4715b4ff",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "--- \n",
    "\n",
    "A central question is whether the documents fit into the LLM context window. Two common approaches for are:\n",
    "\n",
    "1. `Stuff`: This is the simplest form of the summarization that will attempt to fit all the documents into the prompt (see [here](https://langchain-fzn9vbdgo-langchain.vercel.app/docs/modules/chains/document/stuff)).\n",
    "\n",
    "2. `Map reduce`: Summarize sub-sets in a \"map\" step and then \"reduce\" the summaries into a final summary (see [here](https://langchain-fzn9vbdgo-langchain.vercel.app/docs/modules/chains/document/map_reduce))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec66bc",
   "metadata": {},
   "source": [
    "![Image description](/img/summarization_use_case_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea785ac",
   "metadata": {},
   "source": [
    "## Quickstart\n",
    "\n",
    "--- \n",
    "\n",
    "To give you a sneak preview, either pipeline can be wrapped in a single object: `load_summarize_chain`. \n",
    "\n",
    "Suppose we want to summarize a blog post. We can create this in a few lines of code.\n",
    "\n",
    "First set environment variables and install packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai \n",
    "! pip install tiktoken\n",
    "! pip install chromadb\n",
    "! pip install langchain\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36138740",
   "metadata": {},
   "source": [
    "We can use `chain_type=\"stuff\"`, especially if using larger context window models such as:\n",
    "\n",
    "* 16k token OpenAI `gpt-3.5-turbo-16k` \n",
    "* 100k token Anthropic [Claude-2](https://www.anthropic.com/index/claude-2)\n",
    "\n",
    "We can also supply `chain_type=\"map_reduce\"` or `chain_type=\"refine\"` (read more [here](https://langchain-fzn9vbdgo-langchain.vercel.app/docs/modules/chains/document/refine))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd271681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and proof-of-concept examples of LLM-powered agents in various domains. It also highlights the challenges and limitations of using LLMs in agent systems.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "chain.run(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b36e1",
   "metadata": {},
   "source": [
    "## Option 1. Stuff\n",
    "\n",
    "--- \n",
    "\n",
    "When we use `load_summarize_chain` with `chain_type=\"stuff\"`, we will use the [StuffDocumentsChain](https://python.langchain.com/docs/modules/chains/document/stuff).\n",
    "\n",
    "The chain will take a list of documents, inserts them all into a prompt, and passes that prompt to an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef45585d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and examples of proof-of-concept demos, highlighting the challenges and limitations of LLM-powered agents. It also includes references to related research papers and provides a citation for the article.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "# Define LLM chain\n",
    "document_variable_name = \"text\"\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=PROMPT)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain, document_variable_name=document_variable_name\n",
    ")\n",
    "\n",
    "input_doc = loader.load()\n",
    "stuff_chain.run(input_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e4a43",
   "metadata": {},
   "source": [
    "Great! We can see that we reproduce the earlier result using the `load_summarize_chain`.\n",
    "\n",
    "### Go deeper\n",
    "\n",
    "* You can easily customize the prompt. \n",
    "* You can easily try different LLMs, (e.g., [Claude](https://python.langchain.com/docs/integrations/chat/anthropic)) via the `llm` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6cabee",
   "metadata": {},
   "source": [
    "## Option 2. Map Reduce\n",
    "\n",
    "---\n",
    "\n",
    "Let's unpack the map reduce chain.\n",
    " \n",
    "First, we specfy a few things:\n",
    "\n",
    "* LLM, prompt, and text splitter for map stage\n",
    "* LLM and prompt for reduce stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1e6773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "\n",
    "# Map\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")\n",
    "map_template_string = \"\"\"The following is set of documents\n",
    "{docs}\n",
    "Based on this list docs, please identify the main themes \n",
    "Helpful Answer:\"\"\"\n",
    "MAP_PROMPT = PromptTemplate(input_variables=[\"docs\"], template=map_template_string)\n",
    "map_llm_chain = LLMChain(llm=ChatOpenAI(temperature=0), prompt=MAP_PROMPT)\n",
    "\n",
    "# Reduce\n",
    "reduce_template_string = template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Helpful Answer:\"\"\"\n",
    "REDUCE_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"doc_summaries\"], template=reduce_template_string\n",
    ")\n",
    "reduce_llm_chain = LLMChain(llm=ChatOpenAI(temperature=0), prompt=REDUCE_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3c331",
   "metadata": {},
   "source": [
    "Now, we specify each part of the map-reduce chain.\n",
    "\n",
    "`Reduce`\n",
    "\n",
    "* `StuffDocumentsChain`: Pack all the summaries from map into the prompt of `reduce_llm_chain`\n",
    "* `ReduceDocumentsChain`: Handle cases where the summaries exceed the context window of `reduce_llm_chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1edb1b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a list of documents and combines them into a single string\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_llm_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb5ae1a",
   "metadata": {},
   "source": [
    "`Map`\n",
    "\n",
    "* `MapReduceChain`: Apply `text_splitter` to input of `MapReduceDocumentsChain`\n",
    "* `MapReduceDocumentsChain`: Apply `map_llm_chain` on `text_splitter` output, then `reduce_documents_chain` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22f1cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "combine_documents = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_llm_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=Falsea,\n",
    ")\n",
    "\n",
    "map_reduce = MapReduceChain(\n",
    "    # Chain to combine documents\n",
    "    combine_documents_chain=combine_documents,\n",
    "    # Splitter to use for initial split\n",
    "    text_splitter=text_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7afb8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The main themes identified in the provided set of documents revolve around the concept of LLM-powered autonomous agents. These agents utilize large language models as their core controllers and have the potential to generate well-written copies, stories, essays, and programs. The documents discuss the key components of such agent systems, including planning, memory, and tool use, and highlight their roles in enhancing the agent's capabilities. Task decomposition techniques, self-reflection frameworks, and memory enhancement techniques are explored. Additionally, the documents touch upon the challenges and limitations associated with finite context length, long-term planning, and the reliability of natural language interfaces. Overall, the main themes encompass LLM-powered autonomous agents, their components, and the techniques used to enhance their planning, memory, and tool use capabilities.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce.run(input_text=input_doc[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c21cf",
   "metadata": {},
   "source": [
    "### Go deeper\n",
    " \n",
    "**Customization** \n",
    "\n",
    "* As shown above, you can customize the LLMs and prompts for map and reduce stages.\n",
    "\n",
    "**Real-world use-case**\n",
    "\n",
    "* See [this blog post](https://blog.langchain.dev/llms-to-improve-documentation/) case-study on analyzing user interactions (questions about LangChain documentation)!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ff365",
   "metadata": {},
   "source": [
    "## Option 3. Refine\n",
    "\n",
    "--- \n",
    " \n",
    "[Refine](https://langchain-fzn9vbdgo-langchain.vercel.app/docs/modules/chains/document/refine) is similar to map-reduce:\n",
    "\n",
    "> The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer.\n",
    "\n",
    "This can be easily run with the `chain_type=\"refine\"` specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de1dc10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and proof-of-concept examples of LLM-powered agents in various domains. It also highlights the challenges and limitations of using LLMs in agent systems.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "chain.run(input_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b46f44d",
   "metadata": {},
   "source": [
    "It's also possible to supply a prompt and return intermediate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f86c8072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intermediate_steps': ['The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and examples of proof-of-concept demos, highlighting the challenges and limitations of LLM-powered agents. It also includes references to related research papers and benchmarks.'],\n",
       " 'output_text': 'The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and examples of proof-of-concept demos, highlighting the challenges and limitations of LLM-powered agents. It also includes references to related research papers and benchmarks.'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary in Italian\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    return_intermediate_steps=True,\n",
    "    question_prompt=PROMPT,\n",
    "    refine_prompt=refine_prompt,\n",
    ")\n",
    "chain({\"input_documents\": input_doc}, return_only_outputs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
