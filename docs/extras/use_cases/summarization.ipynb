{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf13f702",
   "metadata": {},
   "source": [
    "# Суммаризация\n",
    "\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ai-forever/gigachain/blob/master/docs/extras/use_cases/summarization.ipynb)\n",
    "\n",
    "## Варианты использования\n",
    "\n",
    "Предположим, у вас есть набор документов (PDF-файлы, страницы Notion, вопросы клиентов и т. д.), и вы хотите обобщить их содержимое.\n",
    "\n",
    "LLM являются отличным инструментом для этого, учитывая их навыки понимания и синтеза текста.\n",
    "\n",
    "В этом пошаговом руководстве мы рассмотрим, как выполнить обобщение документов с помощью LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e233997",
   "metadata": {},
   "source": [
    "![Use case 1](../../docs_skeleton/static/img/summarization_use_case_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4715b4ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Обзор\n",
    "\n",
    "Основной вопрос при создании сумматора — как передать ваши документы в контекстное окно LLM. Два распространенных подхода к этому:\n",
    "\n",
    "1.  `Stuff`: просто \"сложите\" все свои документы в один промпт. Это самый простой подход (см. [здесь](/docs/modules/chains/document/stuff) для получения дополнительной информации о `StuffDocumentsChains`, который используется для этого метода).\n",
    "\n",
    "2. `Map-reduce`: Суммаризирует каждый документ по-отдельности на шаге map, затем суммаризирует все результаты на шаге reduce для получения финального саммари (см. [here](/docs/modules/chains/document/map_reduce) для получения дополнительной информации о `MapReduceDocumentsChain`, который используется для этого метода)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec66bc",
   "metadata": {},
   "source": [
    "![Use case 2](../../docs_skeleton/static/img/summarization_use_case_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea785ac",
   "metadata": {},
   "source": [
    "## Quickstart\n",
    "\n",
    "Для наглядности любой конвейер можно обернуть в один объект: `load_summarize_chain`.\n",
    "\n",
    "Предположим, мы хотим подвести итог записи в блоге. Мы можем создать это с помощью нескольких строк кода.\n",
    "\n",
    "Сначала установите переменные среды и установите пакеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "578d6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai tiktoken chromadb gigachain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36138740",
   "metadata": {},
   "source": [
    " При использоваии параметра `chain_type=\"stuff\"` следует выбирать LLM с большим размером контекстного окна, например:\n",
    "\n",
    "* 16k token OpenAI `gpt-3.5-turbo-16k` \n",
    "* 100k token Anthropic [Claude-2](https://www.anthropic.com/index/claude-2)\n",
    "\n",
    "Также можно использовать другие типы суммаризации `chain_type=\"map_reduce\"` или `chain_type=\"refine\"` (подробнее см. [здесь](/docs/modules/chains/document/refine))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd271681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'В данном тексте рассматривается использование технологии машинного обучения на основе нейронных сетей для создания и управления автономными агентами. Описываются проблемы, связанные с планированием и выполнением задач на длительные периоды времени, а также с устойчивостью моделей к ошибкам. Обсуждаются преимущества использования технологии нейронных сетей для планирования и решения задач, а также ее потенциал для создания более эффективных и надежных систем искусственного интеллекта.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chat_models.gigachat import GigaChat\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "giga = GigaChat(user=<user_name>, password=<password>)\n",
    "chain = load_summarize_chain(giga, chain_type=\"stuff\")\n",
    "\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b36e1",
   "metadata": {},
   "source": [
    "## Подход 1. Stuff\n",
    "\n",
    "When we use `load_summarize_chain` with `chain_type=\"stuff\"`, we will use the [StuffDocumentsChain](/docs/modules/chains/document/stuff).\n",
    "\n",
    "The chain will take a list of documents, inserts them all into a prompt, and passes that prompt to an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef45585d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and examples of proof-of-concept demos, highlighting the challenges and limitations of LLM-powered agents. It also includes references to related research papers and provides a citation for the article.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define LLM chain\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain, document_variable_name=\"text\"\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(stuff_chain.run(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e4a43",
   "metadata": {},
   "source": [
    "Great! We can see that we reproduce the earlier result using the `load_summarize_chain`.\n",
    "\n",
    "### Go deeper\n",
    "\n",
    "* You can easily customize the prompt. \n",
    "* You can easily try different LLMs, (e.g., [Claude](/docs/integrations/chat/anthropic)) via the `llm` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6cabee",
   "metadata": {},
   "source": [
    "## Подход 2. Map-Reduce\n",
    "\n",
    "Рассмотрим пример суммаризации на основании подхода map-reduce для статей, загружаемых из Википедии.\n",
    "\n",
    "Статья из Википедии загружается целиком с помощью `WikipediaLoader`, после чего с помощью `CharacterTextSplitter` делится на части размером около 5000 символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1e6773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts count: 12\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.chat_models.gigachat import GigaChat\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "docs = WikipediaLoader(query=\"Винни-пух\", lang=\"ru\", load_max_docs=1, doc_content_chars_max=1000000).load()\n",
    "split_docs = CharacterTextSplitter(chunk_size=5000, chunk_overlap=500).split_documents(docs)\n",
    "print(f\"Parts count: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3c331",
   "metadata": {},
   "source": [
    "Далее с помощью `GigaChat`` выполняется суммаризация каждой части (фаза map). В конце все суммаризированные части объединяются в одну, после чего выполняется суммарная суммаризация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1edb1b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Винни-Пух — любимый герой детской литературы XX века, созданный Аланом Милном. Цикл о Винни-Пухе состоит из двух книг: «Винни-Пух» и «Дом на Пуховой опушке». Пересказ Бориса Заходера сохраняет языковую игру и юмор оригинала, соблюдает английскую ментальность, является популярным детским чтением и среди взрослых, особенно научной интеллигенции. Некоторые переводы были выполнены с английского оригинала, а некоторые — с сокращенной версии пересказа Заходера. Переводы Вебера и Вегушина и Лисицкой были опубликованы в 1996 году в издательстве «Моимпекс» с параллельным английским текстом, чтобы облегчить изучение языков. Однако, по мнению критиков, эти переводы не являются точными или научными, а скорее представляют собой игру с традицией или деконструкцию текста. Фильм \"Винни-Пух\" был создан на студии \"Союзмультфильм\" под руководством Федора Хитрука."
     ]
    }
   ],
   "source": [
    "giga = GigaChat(profanity=False, user=\"<username>\", password=\"<pass>\")\n",
    "chain = load_summarize_chain(giga, chain_type=\"map_reduce\")\n",
    "res = chain.run(split_docs)\n",
    "\n",
    "print(\"\\n\\n===\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c21cf",
   "metadata": {},
   "source": [
    "### Погружаемся глубже\n",
    " \n",
    "**Кастомизация** \n",
    "\n",
    "* Вы можете самостоятельно настроить промпты для каждой фазы map-reduce.\n",
    "\n",
    "**Реальные примеры использования**\n",
    "\n",
    "* См. [this blog post](https://blog.langchain.dev/llms-to-improve-documentation/) кейс по анализу взаимодействия с пользователем (вопросы по документации LangChain)!  \n",
    "* Данном посте [repo](https://github.com/mendableai/QA_clustering) представлена кластеризация как средство суммаризации.\n",
    "* Это открывает третий путь, помимо подходов «stuff» или «map-reduce», который стоит рассмотреть.\n",
    "\n",
    "![Image description](/img/summarization_use_case_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ff365",
   "metadata": {},
   "source": [
    "## Подход 3. Refine\n",
    " \n",
    "[Refine](/docs/modules/chains/document/refine) подход похож на mad-reduce:\n",
    "\n",
    "> Цепочка refine создает ответ, перебирая входные документы и итеративно обновляя свой ответ. В цикле происходит суммаризация каждого документа с добавлением полученной на предыдущем шаге суммаризации предыдущих документов.\n",
    "\n",
    "Вы можете легко запустить эту цепочку, использовав параметр `chain_type=\"refine\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de1dc10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The GPT-Engineer project aims to create a repository of code for specific tasks specified in natural language. It involves breaking down tasks into smaller components and seeking clarification from the user when needed. The project emphasizes the importance of implementing every detail of the architecture as code and provides guidelines for file organization, code structure, and dependencies. However, there are challenges in long-term planning and task decomposition, as well as the reliability of the natural language interface. The system has limited communication bandwidth and struggles to adjust plans when faced with unexpected errors. The reliability of model outputs is questionable, as formatting errors and rebellious behavior can occur. The conversation also includes instructions for writing the code, including laying out the core classes, functions, and methods, and providing the code in a markdown code block format. The user is reminded to ensure that the code is fully functional and follows best practices for file naming, imports, and types. The project is powered by LLM (Large Language Models) and incorporates prompting techniques from various research papers.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "chain.run(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b46f44d",
   "metadata": {},
   "source": [
    "It's also possible to supply a prompt and return intermediate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f86c8072",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary in Italian\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    ")\n",
    "result = chain({\"input_documents\": split_docs}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9600b67-79d4-4f85-aba2-9fe81fa29f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'articolo discute il concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. Esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l'uso di strumenti. Dimostrazioni di concetto come AutoGPT mostrano la possibilità di creare agenti autonomi con LLM come controller principale. Approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente. Tuttavia, ci sono sfide legate alla lunghezza del contesto, alla pianificazione a lungo termine e alla decomposizione delle attività. Inoltre, l'affidabilità dell'interfaccia di linguaggio naturale tra LLM e componenti esterni come la memoria e gli strumenti è incerta. Nonostante ciò, l'uso di LLM come router per indirizzare le richieste ai moduli esperti più adatti è stato proposto come architettura neuro-simbolica per agenti autonomi nel sistema MRKL. L'articolo fa riferimento a diverse pubblicazioni che approfondiscono l'argomento, tra cui Chain of Thought, Tree of Thoughts, LLM+P, ReAct, Reflexion, e MRKL Systems.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f91a8eb-daa5-4191-ace4-01765801db3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article discusses the concept of building autonomous agents using LLM (large language model) as the core controller. The article explores the different components of an LLM-powered agent system, including planning, memory, and tool use. It also provides examples of proof-of-concept demos and highlights the potential of LLM as a general problem solver.\n",
      "\n",
      "Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l'uso degli strumenti. Vengono anche forniti esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente.\n",
      "\n",
      "Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l'uso degli strumenti. Vengono anche forniti esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente. Il nuovo contesto riguarda l'approccio Chain of Hindsight (CoH) che permette al modello di migliorare autonomamente i propri output attraverso un processo di apprendimento supervisionato. Viene anche presentato l'approccio Algorithm Distillation (AD) che applica lo stesso concetto alle traiettorie di apprendimento per compiti di reinforcement learning.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join(result[\"intermediate_steps\"][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddd522e-30dc-4f6a-b993-c4f97e656c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
