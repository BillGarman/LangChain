{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea857b1",
   "metadata": {},
   "source": [
    "# Running LLMs locally\n",
    "\n",
    "The popularity of [PrivateGPT](https://github.com/imartinez/privateGPT) and [GPT4All](https://github.com/nomic-ai/gpt4all) underscore the importance of running LLMs locally.\n",
    "\n",
    "LangChain has integrations with many open source LLMs that can be run locally.\n",
    "\n",
    "For example, here we show how to run GPT4All locally using both gpt4all embeddings and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11514b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7543fa",
   "metadata": {},
   "source": [
    "Load and split an example docucment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8cf5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d5059",
   "metadata": {},
   "source": [
    "This will download the GPT4AllEmbeddings locally if you don't already have them.\n",
    "\n",
    "For example, mine are here:\n",
    " \n",
    "```\n",
    "Model downloaded at:  /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdce8923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[45921]: Class GGMLMetalClass is implemented in both /Users/rlm/anaconda3/envs/lcn2/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x2a8e50208) and /Users/rlm/anaconda3/envs/lcn2/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x2a927c208). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29137915",
   "metadata": {},
   "source": [
    "Test similarity search is working with our local embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0c55e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32b43339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9579a7",
   "metadata": {},
   "source": [
    "[Download the GPT4All model binary]((https://python.langchain.com/docs/modules/model_io/models/llms/integrations/gpt4all)).\n",
    "\n",
    "Then, specify the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a24eef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: using Metal\n",
      "llama.cpp: loading model from /Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 9031.71 MB (+ 1608.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1600.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/rlm/anaconda3/envs/lcn2/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1027b9810\n",
      "ggml_metal_init: loaded kernel_mul                            0x1027bad60\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x1027bb500\n",
      "ggml_metal_init: loaded kernel_scale                          0x1027bbb90\n",
      "ggml_metal_init: loaded kernel_silu                           0x1027bc310\n",
      "ggml_metal_init: loaded kernel_relu                           0x1027bc950\n",
      "ggml_metal_init: loaded kernel_gelu                           0x2ac672f80\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x2a9d5b750\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x2a9d5cfe0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x2a9d5c0c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x2a9d5d240\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x2a9d5de70\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x2a9d5d6e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x2a9d5ee40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x2a9d5f560\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x1027b9b60\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x1027bd270\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1027bddd0\n",
      "ggml_metal_init: loaded kernel_norm                           0x1027be4f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x2a9d60030\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x2a9d60290\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x2a9d606b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x2a9d614e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x2ac673cd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x2ac6745f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x2ac675280\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x2a9d61f20\n",
      "ggml_metal_init: loaded kernel_rope                           0x2a9d62550\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x2a9d627b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x2a9d64200\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x2a9d64af0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x2a9d65460\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, ( 6984.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1024.00 MB, ( 8008.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1602.00 MB, ( 9610.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB, (10122.45 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB, (10634.45 / 21845.34)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=\"/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin\",\n",
    "    max_tokens=2048,\n",
    ")\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030225a",
   "metadata": {},
   "source": [
    "Run it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "112ca227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are the approaches to Task Decomposition?',\n",
       " 'result': 'There are three main approaches to Task Decomposition: 1) Language Model (LM) prompting with simple instructions like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\" or task-specific instructions; e.g., \"Write a story outline.\" for writing a novel, 2) using human inputs to guide LMs in decomposing tasks, and 3) Expert models executing on specific tasks and logging results while judges determine the correctness of the outcomes.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
