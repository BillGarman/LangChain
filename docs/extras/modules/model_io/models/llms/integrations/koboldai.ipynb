{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPF4vhdZyJ7S"
   },
   "source": [
    "# KoboldAI API\n",
    "\n",
    "[KoboldAI](https://github.com/KoboldAI/KoboldAI-Client) is a \"a browser-based front-end for AI-assisted writing with multiple local & remote AI models...\". It has a public and local API that is able to be used in langchain.\n",
    "\n",
    "This example goes over how to use LangChain with that API.\n",
    "\n",
    "Documentation on parameters can be found [here](https://api.python.langchain.com/en/latest/llms/langchain.llms.koboldai.KoboldApiLLM.html)\n",
    "\n",
    "Windows: run 'remoteplay.bat' for Public\n",
    "\n",
    "windows or linux users: play.sh --remote for a public url or ./play.sh --host XXX.XXX.XXX.0/24 --port 5000  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lyzOsRRTf_Vr"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import KoboldApiLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a_H7mvfy51O"
   },
   "source": [
    "Replace the endpoint seen below with the one shown in the output after starting it with the correct arguments. \n",
    "\n",
    "Optionally, you can pass in parameters like temperature or max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "g3vGebq8f_Vr"
   },
   "outputs": [],
   "source": [
    "llm = KoboldApiLLM(endpoint=\"http://127.0.0.1:5000\", max_length=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Kobold API LLM Wraper \n",
    "In the next two cells, a DuckDuckGo search is performed based on the question to gather information, then the language model is utilized to generate a response based on this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When was llamav2, from meta, released?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "sPxNGGiDf_Vr",
    "outputId": "024a1d62-3cd7-49a8-c6a8-5278224d02ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When was llamav2, from meta, released?\n",
      "\n",
      "Answer: Llamav2, from meta, was released on Wed 19 Jul 2023 11.11 EDT.\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "langchain.debug = False\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Input: {search_result}\n",
    "\n",
    "Answer: Based on the input above, the answer to the question is as follows:\"\"\"\n",
    "\n",
    "\n",
    "search_result = search(question)\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"search_result\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "response = llm_chain.run({'question': question, 'search_result': search_result})\n",
    "\n",
    "print(f'Question: {question}\\n\\n{search_result}\\n\\nAnswer: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
