{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xorbits Inference (Xinference)\n",
    "\n",
    "[Xinference](https://github.com/xorbitsai/inference) is a powerful and versatile library designed to serve LLMs, \n",
    "speech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as chatglm, baichuan, whisper, vicuna, orca, and many others. This notebook demonstrates how to use Xinference with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install `Xinference` through PyPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"xinference[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Xinference in a Distributed Cluster\n",
    "\n",
    "Firstly, start an Xinference supervisor using the `xinference-supervisor`. You can also use the option -p to specify the port and -H to specify the host. The default port is 9997.\n",
    "\n",
    "Then, start the Xinference workers using `xinference-worker` on each server you want to run them on. \n",
    "\n",
    "## Wrapper\n",
    "\n",
    "To use Xinference with LangChain, you need to first launch a model. You can use command line interface (CLI) to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uid: c2dc8072-277e-11ee-8b88-d29396a3f064\n"
     ]
    }
   ],
   "source": [
    "!xinference launch -n orca -s 3 -q q4_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model uid is returned for you to use. Now you can use Xinference embeddings within LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The Eiffel Tower is a famous landmark in Paris, France. It is located on the Champ de Mars and is one of the most visited places in the world. Additionally, you can visit the Louvre Museum, Notre-Dame Cathedral, or the Palace of Versailles.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import Xinference\n",
    "\n",
    "llm = Xinference(\n",
    "    server_url=\"http://0.0.0.0:9997\",\n",
    "    model_uid = \"c2dc8072-277e-11ee-8b88-d29396a3f064\"\n",
    ")\n",
    "\n",
    "llm(\n",
    "    prompt=\"Q: where can we visit in the capital of France? A:\",\n",
    "    generate_config={\"max_tokens\": 1024, \"stream\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate with a LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We can visit the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral and the Palace of Versailles.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"Where can we visit in the capital of {country}?\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"country\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "generated = llm_chain.run(country=\"France\")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, terminate the model when you do not need to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!xinference terminate --model-uid \"c2dc8072-277e-11ee-8b88-d29396a3f064\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
