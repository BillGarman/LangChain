{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "328ef72d",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "Test that we can both write to and read from an index across several production vectorstores.\n",
    "\n",
    "In particular, the existing documentation often involoves simmply writing to an `index` (e.g., `from_documents`).\n",
    "\n",
    "But, there are two gaps:\n",
    "\n",
    "1) We don't confirm that we can also read an existing index, which appears to be a problem w/ Weviate today.\n",
    "\n",
    "2) We also don't confirm integration w/ hosted instances in all cases (e.g., many are just local). \n",
    "\n",
    "\n",
    "## Example Text\n",
    "\n",
    "Example from Karpathy-GPT app [here](https://github.com/rlancemartin/karpathy-gpt/tree/main/eval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e79437d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z9/m84x374516sbvv3h5bfsvnbh0000gn/T/ipykernel_13302/285374083.py:2: ResourceWarning: unclosed file <_io.TextIOWrapper name='example_data/karpathy_course_all.txt' mode='r' encoding='UTF-8'>\n",
      "  course_txt = open('example_data/karpathy_course_all.txt').read()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "course_txt = open('example_data/karpathy_course_all.txt').read()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "splits = text_splitter.split_text(course_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5934097c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1360"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full course\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bfa86f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "splits = splits[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaac9ed",
   "metadata": {},
   "source": [
    "## Pinecone\n",
    "\n",
    "* Use Pinecone console to create a new index with `index_name`\n",
    "* [Write and read supported](https://python.langchain.com/en/latest/reference/modules/vectorstores.html#langchain.vectorstores.Pinecone) to `index_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f151140",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60eb879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Auth\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),  \n",
    "    environment=\"us-east1-gcp\"  \n",
    ")\n",
    "\n",
    "# Create new index\n",
    "embeddings = OpenAIEmbeddings()\n",
    "index_name = \"karpathy-gpt\"\n",
    "vectorstore_new = Pinecone.from_texts(splits, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b35506e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from index\n",
    "vectorstore_pinecone = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bafe2136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(lc_kwargs={'page_content': \"Hello, my name is Andrej and I've been training deep neural networks for a bit more than a decade. And in this lecture I'd like to show you what neural network training looks like under the hood. So in particular we are going to start with a blank Jupyter notebook and by the end of this lecture we will define and train a neural net and you'll get to see everything that goes on under the hood and exactly sort of how that works on an intuitive level. Now specifically what I would like to do is I would like to take you through building of micrograd. Now micrograd is this library that I released on GitHub about two years ago but at the time I only uploaded the source code and you'd have to go in by yourself and really figure out how it works. So in this lecture I will take you through it step by step and kind of comment on all the pieces of it. So what is micrograd and why is it interesting? Thank you. Micrograd is basically an autograd engine. Autograd is short for automatic gradient and really what it does is it implements back propagation. Now back propagation is this algorithm that allows you to efficiently evaluate the gradient of some kind of a loss function with respect to the weights of a neural network and what that allows us to do then is we can iteratively tune the weights of that neural network to minimize the loss function and therefore improve the accuracy of the network. So back propagation would be at the mathematical core of any modern deep neural network library\", 'metadata': {'id': '01', 'link': 'https://youtu.be/VMj-3S1tku0', 'source': 'The spelled-out intro to neural networks and backpropagation： building micrograd 01', 'title': 'The spelled-out intro to neural networks and backpropagation： building micrograd'}}, page_content=\"Hello, my name is Andrej and I've been training deep neural networks for a bit more than a decade. And in this lecture I'd like to show you what neural network training looks like under the hood. So in particular we are going to start with a blank Jupyter notebook and by the end of this lecture we will define and train a neural net and you'll get to see everything that goes on under the hood and exactly sort of how that works on an intuitive level. Now specifically what I would like to do is I would like to take you through building of micrograd. Now micrograd is this library that I released on GitHub about two years ago but at the time I only uploaded the source code and you'd have to go in by yourself and really figure out how it works. So in this lecture I will take you through it step by step and kind of comment on all the pieces of it. So what is micrograd and why is it interesting? Thank you. Micrograd is basically an autograd engine. Autograd is short for automatic gradient and really what it does is it implements back propagation. Now back propagation is this algorithm that allows you to efficiently evaluate the gradient of some kind of a loss function with respect to the weights of a neural network and what that allows us to do then is we can iteratively tune the weights of that neural network to minimize the loss function and therefore improve the accuracy of the network. So back propagation would be at the mathematical core of any modern deep neural network library\", metadata={'id': '01', 'link': 'https://youtu.be/VMj-3S1tku0', 'source': 'The spelled-out intro to neural networks and backpropagation： building micrograd 01', 'title': 'The spelled-out intro to neural networks and backpropagation： building micrograd'})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query\n",
    "query = \"What is micrograd?\"\n",
    "matched_docs = vectorstore_pinecone.similarity_search(query)\n",
    "matched_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc0c3b",
   "metadata": {},
   "source": [
    "## Supabase\n",
    "\n",
    "\n",
    "* Create a new project in [Supabase dashboard](https://supabase.com/dashboard/project/xhbejgrankzufmczyqil).\n",
    "* In the project, go to the SQL editor on the left.\n",
    "* We need to create a table to store our embeddings.\n",
    "* We will use `pgvector`, an extension for PostgreSQL that allows you to both store and query vector embeddings.\n",
    "* Create the table in the SQL editor with [this code](https://supabase.com/docs/guides/ai/langchain), modified below for our table name `karpathy_gpt`:\n",
    "\n",
    "```\n",
    "-- Enable the pgvector extension to work with embedding vectors\n",
    "-- create extension vector;\n",
    "\n",
    "-- Create a table to store your documents\n",
    "create table karpathy_gpt (\n",
    "  id bigserial primary key,\n",
    "  content text, -- corresponds to Document.pageContent\n",
    "  metadata jsonb, -- corresponds to Document.metadata\n",
    "  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed\n",
    ");\n",
    "\n",
    "-- Create a function to search for documents\n",
    "CREATE OR REPLACE function match_documents (\n",
    "  query_embedding vector(1536),\n",
    "  match_count int default null,\n",
    "  filter jsonb DEFAULT '{}'\n",
    ") returns table (\n",
    "  id bigint,\n",
    "  content text,\n",
    "  metadata jsonb,\n",
    "  similarity float\n",
    ")\n",
    "language plpgsql\n",
    "as $$\n",
    "#variable_conflict use_column\n",
    "begin\n",
    "  return query\n",
    "  select\n",
    "    id,\n",
    "    content,\n",
    "    metadata,\n",
    "    1 - (karpathy_gpt.embedding <=> query_embedding) as similarity\n",
    "  from karpathy_gpt\n",
    "  where metadata @> filter\n",
    "  order by karpathy_gpt.embedding <=> query_embedding\n",
    "  limit match_count;\n",
    "end;\n",
    "$$;\n",
    "```\n",
    "\n",
    "* Now, the table is created!\n",
    "* In the project, you can find `SUPABASE_URL` and `SUPABASE_SERVICE_KEY`, which we will use to connect to this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e251d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07c12532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import SupabaseVectorStore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from supabase.client import Client, create_client\n",
    "# Auth\n",
    "supabase_url = os.environ.get('supabase_url')\n",
    "supabase_key = os.environ.get('supabase_key')\n",
    "supabase: Client = create_client(supabase_url, supabase_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75b991ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new index\n",
    "table_name=\"karpathy_gpt\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore_new = SupabaseVectorStore.from_texts(splits,embeddings,client=supabase,table_name=table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b645bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from index\n",
    "vectorstore_supabase = SupabaseVectorStore(client=supabase,embedding=embeddings,table_name=table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c148d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(lc_kwargs={'metadata': {}, 'page_content': \"would like to take you through building of micrograd. Now micrograd is this library that I released on GitHub about two years ago but at the time I only uploaded the source code and you'd have to go in by yourself and really figure out how it works. So in this lecture I will take you through it step by step and kind of comment on all the pieces of it. So what is micrograd and why is it interesting? Thank you. Micrograd is basically an autograd engine. Autograd is short for automatic gradient\"}, page_content=\"would like to take you through building of micrograd. Now micrograd is this library that I released on GitHub about two years ago but at the time I only uploaded the source code and you'd have to go in by yourself and really figure out how it works. So in this lecture I will take you through it step by step and kind of comment on all the pieces of it. So what is micrograd and why is it interesting? Thank you. Micrograd is basically an autograd engine. Autograd is short for automatic gradient\", metadata={})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query\n",
    "query = \"What is micrograd?\"\n",
    "matched_docs = vectorstore_supabase.similarity_search(query,k=1)\n",
    "matched_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ffa7b",
   "metadata": {},
   "source": [
    "# Elastic\n",
    "\n",
    "* Log into Elastic Cloud console at https://cloud.elastic.co\n",
    "* Create deployment\n",
    "* Go to the deployment page and `copy endpoint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e15479bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auth\n",
    "elastic_endpoint = \"langchain-test.es.us-central1.gcp.cloud.es.io\"\n",
    "elasticsearch_url = f\"https://elastic:cYo6rjQMesQbwqcGHblf7P0K@{elastic_endpoint}:9243\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67bb8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import ElasticVectorSearch\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "index_name = \"karpathy-gpt\"\n",
    "\n",
    "# Create new index\n",
    "vectorstore_new = ElasticVectorSearch.from_texts(splits, embeddings, \n",
    "                                                 elasticsearch_url=elasticsearch_url,\n",
    "                                                 index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af7ab74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(lc_kwargs={'page_content': \"would like to take you through building of micrograd. Now micrograd is this library that I released on GitHub about two years ago but at the time I only uploaded the source code and you'd have to go in by yourself and really figure out how it works. So in this lecture I will take you through it step by step and kind of comment on all the pieces of it. So what is micrograd and why is it interesting? Thank you. Micrograd is basically an autograd engine. Autograd is short for automatic gradient\", 'metadata': {}}, page_content=\"would like to take you through building of micrograd. Now micrograd is this library that I released on GitHub about two years ago but at the time I only uploaded the source code and you'd have to go in by yourself and really figure out how it works. So in this lecture I will take you through it step by step and kind of comment on all the pieces of it. So what is micrograd and why is it interesting? Thank you. Micrograd is basically an autograd engine. Autograd is short for automatic gradient\", metadata={})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "query = \"What is micrograd?\"\n",
    "matched_docs = vectorstore_new.similarity_search(query,k=1)\n",
    "matched_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe76eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from index\n",
    "vectorstore_estc = ElasticVectorSearch(elasticsearch_url=elasticsearch_url, index_name=index_name, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c053c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(lc_kwargs={'page_content': \"would like to take you through building of micrograd. Now micrograd is this library that I released on GitHub about two years ago but at the time I only uploaded the source code and you'd have to go in by yourself and really figure out how it works. So in this lecture I will take you through it step by step and kind of comment on all the pieces of it. So what is micrograd and why is it interesting? Thank you. Micrograd is basically an autograd engine. Autograd is short for automatic gradient\", 'metadata': {}}, page_content=\"would like to take you through building of micrograd. Now micrograd is this library that I released on GitHub about two years ago but at the time I only uploaded the source code and you'd have to go in by yourself and really figure out how it works. So in this lecture I will take you through it step by step and kind of comment on all the pieces of it. So what is micrograd and why is it interesting? Thank you. Micrograd is basically an autograd engine. Autograd is short for automatic gradient\", metadata={})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query\n",
    "matched_docs = vectorstore_estc.similarity_search(query,k=1)\n",
    "matched_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5125056e",
   "metadata": {},
   "source": [
    "## Redis\n",
    "\n",
    "Cloud -\n",
    "\n",
    "* Create database in Redis public cloud, which has endpoint: `redis-16792.c302.asia-northeast1-1.gce.cloud.redislabs.com:16792`\n",
    "* **Need**: Documentation on how to [connect](https://docs.redis.com/latest/rs/references/client_references/client_python/) to this because we still get auth errors.\n",
    "\n",
    "--- \n",
    "\n",
    "Local - \n",
    "  \n",
    "```\n",
    "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n",
    "brew install redis\n",
    "brew services start redis\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e6a01a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: redis in /Users/31treehaus/miniconda3/envs/langchain-new/lib/python3.9/site-packages (4.5.5)\r\n",
      "Requirement already satisfied: async-timeout>=4.0.2 in /Users/31treehaus/miniconda3/envs/langchain-new/lib/python3.9/site-packages (from redis) (4.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install redis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e7298b",
   "metadata": {},
   "source": [
    "**Need to sort out auth errors and get crisp documentation here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98005a03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redis://:LangChainTest01!@redis-18547.c1.us-central1-2.gce.cloud.redislabs.com:18547\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "invalid username-password pair",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m redis_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mredis://:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpublic_endpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(redis_url)\n\u001b[0;32m---> 16\u001b[0m vectorstore_new \u001b[38;5;241m=\u001b[39m \u001b[43mRedis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43mredis_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mredis_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlink\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AI/langchain-fork/langchain/vectorstores/redis.py:449\u001b[0m, in \u001b[0;36mRedis.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, index_name, content_key, metadata_key, vector_key, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Redis],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    431\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Redis:\n\u001b[1;32m    432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Redis vectorstore from raw documents.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    This is a user-friendly interface that:\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m        1. Embeds documents.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     instance, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts_return_keys\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvector_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m instance\n",
      "File \u001b[0;32m~/Desktop/AI/langchain-fork/langchain/vectorstores/redis.py:400\u001b[0m, in \u001b[0;36mRedis.from_texts_return_keys\u001b[0;34m(cls, texts, embedding, metadatas, index_name, content_key, metadata_key, vector_key, distance_metric, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m     index_name \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# Create instance\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mredis_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Create embeddings over documents\u001b[39;00m\n\u001b[1;32m    411\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n",
      "File \u001b[0;32m~/Desktop/AI/langchain-fork/langchain/vectorstores/redis.py:139\u001b[0m, in \u001b[0;36mRedis.__init__\u001b[0;34m(self, redis_url, index_name, embedding_function, content_key, metadata_key, vector_key, relevance_score_fn, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     redis_client \u001b[38;5;241m=\u001b[39m redis\u001b[38;5;241m.\u001b[39mfrom_url(redis_url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# check if redis has redisearch module installed\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     \u001b[43m_check_redis_module_exist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mredis_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mREDIS_REQUIRED_MODULES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRedis failed to connect: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/AI/langchain-fork/langchain/vectorstores/redis.py:48\u001b[0m, in \u001b[0;36m_check_redis_module_exist\u001b[0;34m(client, required_modules)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_redis_module_exist\u001b[39m(client: RedisType, required_modules: List[\u001b[38;5;28mdict\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check if the correct Redis modules are installed.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     installed_modules \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     installed_modules \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     50\u001b[0m         module[\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m): module \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m installed_modules\n\u001b[1;32m     51\u001b[0m     }\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m required_modules:\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain-new/lib/python3.9/site-packages/redis/commands/core.py:5781\u001b[0m, in \u001b[0;36mModuleCommands.module_list\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodule_list\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT:\n\u001b[1;32m   5775\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5776\u001b[0m \u001b[38;5;124;03m    Returns a list of dictionaries containing the name and version of\u001b[39;00m\n\u001b[1;32m   5777\u001b[0m \u001b[38;5;124;03m    all loaded modules.\u001b[39;00m\n\u001b[1;32m   5778\u001b[0m \n\u001b[1;32m   5779\u001b[0m \u001b[38;5;124;03m    For more information see https://redis.io/commands/module-list\u001b[39;00m\n\u001b[1;32m   5780\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5781\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMODULE LIST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain-new/lib/python3.9/site-packages/redis/client.py:1266\u001b[0m, in \u001b[0;36mRedis.execute_command\u001b[0;34m(self, *args, **options)\u001b[0m\n\u001b[1;32m   1264\u001b[0m pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_pool\n\u001b[1;32m   1265\u001b[0m command_name \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1266\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mretry\u001b[38;5;241m.\u001b[39mcall_with_retry(\n\u001b[1;32m   1270\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_command_parse_response(\n\u001b[1;32m   1271\u001b[0m             conn, command_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1272\u001b[0m         ),\n\u001b[1;32m   1273\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m error: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disconnect_raise(conn, error),\n\u001b[1;32m   1274\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain-new/lib/python3.9/site-packages/redis/connection.py:1457\u001b[0m, in \u001b[0;36mConnectionPool.get_connection\u001b[0;34m(self, command_name, *keys, **options)\u001b[0m\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_use_connections\u001b[38;5;241m.\u001b[39madd(connection)\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1456\u001b[0m     \u001b[38;5;66;03m# ensure this connection is connected to Redis\u001b[39;00m\n\u001b[0;32m-> 1457\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;66;03m# connections that the pool provides should be ready to send\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;66;03m# a command. if not, the connection was either returned to the\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;66;03m# pool before all data has been read or the socket has been\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;66;03m# closed. either way, reconnect and verify everything is good.\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain-new/lib/python3.9/site-packages/redis/connection.py:711\u001b[0m, in \u001b[0;36mAbstractConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mredis_connect_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;66;03m# Use the default on_connect function\u001b[39;00m\n\u001b[0;32m--> 711\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;66;03m# Use the passed function redis_connect_func\u001b[39;00m\n\u001b[1;32m    714\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mredis_connect_func(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain-new/lib/python3.9/site-packages/redis/connection.py:755\u001b[0m, in \u001b[0;36mAbstractConnection.on_connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend_command(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUTH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mauth_args, check_health\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 755\u001b[0m     auth_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AuthenticationWrongNumberOfArgsError:\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;66;03m# a username and password were specified but the Redis\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;66;03m# server seems to be < 6.0.0 which expects a single password\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;66;03m# arg. retry auth with just the password.\u001b[39;00m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;66;03m# https://github.com/andymccurdy/redis-py/issues/1274\u001b[39;00m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend_command(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUTH\u001b[39m\u001b[38;5;124m\"\u001b[39m, auth_args[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], check_health\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain-new/lib/python3.9/site-packages/redis/connection.py:874\u001b[0m, in \u001b[0;36mAbstractConnection.read_response\u001b[0;34m(self, disable_decoding, disconnect_on_error)\u001b[0m\n\u001b[1;32m    871\u001b[0m host_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_error()\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 874\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisable_decoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_decoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mtimeout:\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m disconnect_on_error:\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain-new/lib/python3.9/site-packages/redis/connection.py:347\u001b[0m, in \u001b[0;36mPythonParser.read_response\u001b[0;34m(self, disable_decoding)\u001b[0m\n\u001b[1;32m    345\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mget_pos() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisable_decoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_decoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer:\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain-new/lib/python3.9/site-packages/redis/connection.py:370\u001b[0m, in \u001b[0;36mPythonParser._read_response\u001b[0;34m(self, disable_decoding)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# if the error is a ConnectionError, raise immediately so the user\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# is notified\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mConnectionError\u001b[39;00m):\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# otherwise, we're dealing with a ResponseError that might belong\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# inside a pipeline response. the connection's read_response()\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# and/or the pipeline's execute() will raise this error if\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# necessary, so just return the exception instance here.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: invalid username-password pair"
     ]
    }
   ],
   "source": [
    "### AUTH ERROR\n",
    "import urllib.parse\n",
    "from langchain.vectorstores.redis import Redis\n",
    "password = 'm.fN%A#A8vEwVK6'\n",
    "redis_url=\"redis://redis-16792.c302.asia-northeast1-1.gce.cloud.redislabs.com:16792\"\n",
    "password_encoded = urllib.parse.quote(password)\n",
    "redis_url = f'redis://:{password_encoded}@redis-16792.c302.asia-northeast1-1.gce.cloud.redislabs.com:16792'\n",
    "\n",
    "### STILL AUTH ERROR\n",
    "password = \"LangChainTest01!\"\n",
    "public_endpoint = \"redis-18547.c1.us-central1-2.gce.cloud.redislabs.com:18547\"\n",
    "redis_url = f'redis://:{password}@{public_endpoint}'\n",
    "print(redis_url)\n",
    "vectorstore_new = Redis.from_texts(splits,embeddings,redis_url=redis_url,index_name='link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088eea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_url=\"redis://redis-16792.c302.asia-northeast1-1.gce.cloud.redislabs.com:16792\"\n",
    "vectorstore_new = Redis.from_texts(splits, embeddings, redis_url=redis_url, index_name='link')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05d923c",
   "metadata": {},
   "source": [
    "## Weviate\n",
    "\n",
    "* Create a new cluser in [Weviate dashboard](https://console.weaviate.cloud/dashboard).\n",
    "* This gives you a url: https://langchain-test-l73n8vle.weaviate.network\n",
    "* `text_key` is the name of the text property in your Weaviate schema where the text of your documents is stored. \n",
    "* It's used to find documents that are similar to a text query.\n",
    "\n",
    "Bug: \n",
    "\n",
    "* https://github.com/hwchase17/langchain/issues/6121\n",
    "* https://github.com/weaviate/weaviate/issues/3132\n",
    "* https://github.com/weaviate/weaviate/issues/3142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a0769e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: weaviate-client in /Users/31treehaus/miniconda3/envs/langchain-new/lib/python3.9/site-packages (3.19.2)\n",
      "Requirement already satisfied: requests<2.29.0,>=2.28.0 in /Users/31treehaus/miniconda3/envs/langchain-new/lib/python3.9/site-packages (from weaviate-client) (2.28.2)\n",
      "Requirement already satisfied: validators<=0.21.0,>=0.18.2 in /Users/31treehaus/miniconda3/envs/langchain-new/lib/python3.9/site-packages (from weaviate-client) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.59.0 in /Users/31treehaus/miniconda3/envs/langchain-new/lib/python3.9/site-packages (from weaviate-client) (4.65.0)\n",
      "Requirement already satisfied: authlib>=1.1.0 in /Users/31treehaus/miniconda3/envs/langchain-new/lib/python3.9/site-packages (from weaviate-client) (1.2.0)\n",
      "Requirement already satisfied: cryptography>=3.2 in /Users/31treehaus/miniconda3/envs/langchain-new/lib/python3.9/site-packages (from authlib>=1.1.0->weaviate-client) (41.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/31treehaus/miniconda3/envs/langchain-new/lib/python3.9/site-packages (from requests<2.29.0,>=2.28.0->weaviate-client) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/31treehaus/miniconda3/envs/langchain-new/lib/python3.9/site-packages (from requests<2.29.0,>=2.28.0->weaviate-client) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/31treehaus/miniconda3/envs/langchain-new/lib/python3.9/site-packages (from requests<2.29.0,>=2.28.0->weaviate-client) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/31treehaus/miniconda3/envs/langchain-new/lib/python3.9/site-packages (from requests<2.29.0,>=2.28.0->weaviate-client) (2023.5.7)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /Users/31treehaus/miniconda3/envs/langchain-new/lib/python3.9/site-packages (from validators<=0.21.0,>=0.18.2->weaviate-client) (5.1.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/31treehaus/miniconda3/envs/langchain-new/lib/python3.9/site-packages (from cryptography>=3.2->authlib>=1.1.0->weaviate-client) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/31treehaus/miniconda3/envs/langchain-new/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.2->authlib>=1.1.0->weaviate-client) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install weaviate-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "050c62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from weaviate import Client, auth\n",
    "from langchain.vectorstores import Weaviate\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Auth\n",
    "weaviate_url = \"https://langchain-test-l73n8vle.weaviate.network\"\n",
    "client = Client(url=weaviate_url, auth_client_secret=auth.AuthClientPassword(\"lance@langchain.dev\", \"j!ZEFs6pFd.SWH.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7de36ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index name in from_texts:\n",
      "LangChain_609ee8f62aad4db0910cb2e1ba510244\n"
     ]
    }
   ],
   "source": [
    "# Create and add texts\n",
    "### Added logging to show the auto-generated index_name and text_key ###\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore_new = Weaviate.from_texts(splits,embeddings,client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c06580c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index name in similarity search\n",
      "LangChain_609ee8f62aad4db0910cb2e1ba510244\n",
      "query_obj\n",
      "<weaviate.gql.get.GetBuilder object at 0x119425460>\n",
      "result\n",
      "{'data': {'Get': {'LangChain_609ee8f62aad4db0910cb2e1ba510244': [{'text': \"would like to take you through building of micrograd. Now micrograd is this library that I released on GitHub about two years ago but at the time I only uploaded the source code and you'd have to go in by yourself and really figure out how it works. So in this lecture I will take you through it step by step and kind of comment on all the pieces of it. So what is micrograd and why is it interesting? Thank you. Micrograd is basically an autograd engine. Autograd is short for automatic gradient\"}]}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(lc_kwargs={'page_content': \"would like to take you through building of micrograd. Now micrograd is this library that I released on GitHub about two years ago but at the time I only uploaded the source code and you'd have to go in by yourself and really figure out how it works. So in this lecture I will take you through it step by step and kind of comment on all the pieces of it. So what is micrograd and why is it interesting? Thank you. Micrograd is basically an autograd engine. Autograd is short for automatic gradient\", 'metadata': {}}, page_content=\"would like to take you through building of micrograd. Now micrograd is this library that I released on GitHub about two years ago but at the time I only uploaded the source code and you'd have to go in by yourself and really figure out how it works. So in this lecture I will take you through it step by step and kind of comment on all the pieces of it. So what is micrograd and why is it interesting? Thank you. Micrograd is basically an autograd engine. Autograd is short for automatic gradient\", metadata={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is micrograd?\"\n",
    "matched_docs = vectorstore_new.similarity_search(query,k=1)\n",
    "matched_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28cf27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to our index using the index_name and text_key we obtained from logging\n",
    "index_name = \"LangChain_0b289ac1f5dd4543b76b9f9bdf047ef6\"\n",
    "text_key = \"text\"\n",
    "vectorstore_weviate = Weaviate(client=client,index_name=index_name,text_key=text_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86f25750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index name in from_texts:\n",
      "Karpathy_gpt\n"
     ]
    }
   ],
   "source": [
    "# Let's try to supply an index name on initialization\n",
    "index_name = \"Karpathy_gpt\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore_new = Weaviate.from_texts(splits, embeddings, client=client, index_name=index_name, text_key = \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07f1127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index name in similarity search\n",
      "Karpathy_gpt\n",
      "query_obj\n",
      "<weaviate.gql.get.GetBuilder object at 0x119176b20>\n",
      "result\n",
      "{'data': {'Get': {'Karpathy_gpt': [{'text': \"would like to take you through building of micrograd. Now micrograd is this library that I released on GitHub about two years ago but at the time I only uploaded the source code and you'd have to go in by yourself and really figure out how it works. So in this lecture I will take you through it step by step and kind of comment on all the pieces of it. So what is micrograd and why is it interesting? Thank you. Micrograd is basically an autograd engine. Autograd is short for automatic gradient\"}]}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(lc_kwargs={'page_content': \"would like to take you through building of micrograd. Now micrograd is this library that I released on GitHub about two years ago but at the time I only uploaded the source code and you'd have to go in by yourself and really figure out how it works. So in this lecture I will take you through it step by step and kind of comment on all the pieces of it. So what is micrograd and why is it interesting? Thank you. Micrograd is basically an autograd engine. Autograd is short for automatic gradient\", 'metadata': {}}, page_content=\"would like to take you through building of micrograd. Now micrograd is this library that I released on GitHub about two years ago but at the time I only uploaded the source code and you'd have to go in by yourself and really figure out how it works. So in this lecture I will take you through it step by step and kind of comment on all the pieces of it. So what is micrograd and why is it interesting? Thank you. Micrograd is basically an autograd engine. Autograd is short for automatic gradient\", metadata={})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is micrograd?\"\n",
    "matched_docs = vectorstore_new.similarity_search(query,k=1)\n",
    "matched_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9e39713",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_weviate = Weaviate(embedding=embeddings, client=client, index_name=index_name, text_key = \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac41486b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error during query: [{'locations': [{'column': 28, 'line': 1}], 'message': 'Unknown argument \"nearText\" on field \"Karpathy_gpt\" of type \"GetObjectsObj\". Did you mean \"nearVector\" or \"nearObject\"?', 'path': None}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is micrograd?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m matched_docs \u001b[38;5;241m=\u001b[39m \u001b[43mvectorstore_weviate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m matched_docs\n",
      "File \u001b[0;32m~/Desktop/AI/langchain-fork/langchain/vectorstores/weaviate.py:170\u001b[0m, in \u001b[0;36mWeaviate.similarity_search\u001b[0;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    List of Documents most similar to the query.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_by_text:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_by_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/AI/langchain-fork/langchain/vectorstores/weaviate.py:202\u001b[0m, in \u001b[0;36mWeaviate.similarity_search_by_text\u001b[0;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m result \u001b[38;5;241m=\u001b[39m query_obj\u001b[38;5;241m.\u001b[39mwith_near_text(content)\u001b[38;5;241m.\u001b[39mwith_limit(k)\u001b[38;5;241m.\u001b[39mdo()\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    203\u001b[0m docs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGet\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_name]:\n",
      "\u001b[0;31mValueError\u001b[0m: Error during query: [{'locations': [{'column': 28, 'line': 1}], 'message': 'Unknown argument \"nearText\" on field \"Karpathy_gpt\" of type \"GetObjectsObj\". Did you mean \"nearVector\" or \"nearObject\"?', 'path': None}]"
     ]
    }
   ],
   "source": [
    "query = \"What is micrograd?\"\n",
    "matched_docs = vectorstore_weviate.similarity_search(query,k=1)\n",
    "matched_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fbd2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
