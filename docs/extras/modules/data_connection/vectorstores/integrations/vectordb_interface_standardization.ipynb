{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "328ef72d",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "We want support 3 common methods across vectorDB interfaces:\n",
    "\n",
    "(1) `delete by ids`: Delete documents by their IDs.\n",
    "\n",
    "(2) `update by ids`: Update documents by their IDs.\n",
    "\n",
    "(3) `add by ids`: Add document with their IDs.\n",
    "\n",
    "\n",
    "## Example Text\n",
    "\n",
    "Example from Karpathy-GPT app [here](https://github.com/rlancemartin/karpathy-gpt/tree/main/eval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e79437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "course_txt = open('/Users/31treehaus/Desktop/AI/karpathy-gpt/eval/karpathy_course_all.txt').read()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "splits = text_splitter.split_text(course_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5934097c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1360"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full course\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bfa86f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "splits = splits[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaac9ed",
   "metadata": {},
   "source": [
    "## Pinecone\n",
    "\n",
    "**Create index** \n",
    "\n",
    "* Use Pinecone console to create a new index with `index_name`\n",
    " \n",
    " ---\n",
    " \n",
    "**Pinecone python client:**\n",
    "\n",
    "(1) `delete by ids`: Delete documents by their IDs.\n",
    "\n",
    "* [`Delete`](https://docs.pinecone.io/reference/delete_post) by ID:\n",
    "```\n",
    "pinecone.Index(index_name).delete(ids=ids_to_delete)\n",
    "\n",
    "```\n",
    "\n",
    "(2) `update by ids`: Update documents by their IDs.\n",
    "\n",
    "* [`Insert`](https://docs.pinecone.io/reference/upsert) by ID:\n",
    "\n",
    "```\n",
    "pinecone.Index(index_name).upsert(vectors=vectors, ids=ids)\n",
    "```\n",
    "\n",
    "(3) `add by ids`: Add document with their IDs.\n",
    "\n",
    "* [`Insert`](https://docs.pinecone.io/reference/upsert) by ID:\n",
    "\n",
    "```\n",
    "pinecone.Index(index_name).upsert(vectors=vectors, ids=ids)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Langchain:**\n",
    "\n",
    "(1) `delete by ids`: Delete documents by their IDs.\n",
    "\n",
    "* Create new method\n",
    "\n",
    "(2) `update by ids`: Update documents by their IDs.\n",
    "\n",
    "* `add_texts` is using `upsert` with IDs optionally supplied\n",
    "* Create a new method that calls `add_texts`\n",
    "\n",
    "(3) `add by ids`: Add document with their IDs.\n",
    "\n",
    "* `add_texts` is using `upsert` with IDs optionally supplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f151140",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153e679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Auth\n",
    "pinecone.init(\n",
    "    api_key=\"xxx\",  \n",
    "    environment=\"us-east1-gcp\"  \n",
    ")\n",
    "\n",
    "# Create index\n",
    "embeddings = OpenAIEmbeddings()\n",
    "index_name = \"karpathy-gpt\"\n",
    "# vectorstore_new = Pinecone.from_texts(splits, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6265ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read index\n",
    "vectorstore_pinecone = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "251fe1ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc96aef91acd4f578030dbb58dae02a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='baz', metadata={'source': '/docs'}),\n",
       " Document(page_content='foo', metadata={'source': '/docs'}),\n",
       " Document(page_content='bar', metadata={'source': '/docs'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add by IDs\n",
    "docs=[\n",
    "Document(page_content='foo',metadata={\"source\":'/docs'}),\n",
    "Document(page_content='bar',metadata={\"source\":'/docs'}),\n",
    "Document(page_content='baz',metadata={\"source\":'/docs'}),\n",
    "]\n",
    "vectorstore_pinecone.add_documents_by_id(documents=docs,ids=[\"1\",\"2\",\"3\"])\n",
    "vectorstore_pinecone.similarity_search(\"Foo Bar Baz\",k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b8cf3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208dbcf0270a47a78130dc2e78f03d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='foo', metadata={'source': '/docs'}),\n",
       " Document(page_content='bar', metadata={'source': '/docs'}),\n",
       " Document(page_content='biz', metadata={'source': '/docs'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update by IDs \n",
    "docs=[\n",
    "Document(page_content='foo',metadata={\"source\":'/docs'}),\n",
    "Document(page_content='bar',metadata={\"source\":'/docs'}),\n",
    "Document(page_content='biz',metadata={\"source\":'/docs'}),\n",
    "]\n",
    "vectorstore_pinecone.update_documents_by_id(documents=docs,ids=[\"1\",\"2\",\"3\"])\n",
    "vectorstore_pinecone.similarity_search(\"Foo Bar Baz\",k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db8afdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"but what I did also is I printed 10,000 characters, so a lot more, and I wrote them to a file. And so here we see some of the outputs. So it's a lot more recognizable as the input text file. So the input text file, just for reference, looked like this. So there's always someone speaking in this manner. And our predictions now take on that form. Except, of course, they're nonsensical when you actually read them. So it is, every crimpty be house. Oh, those prepation. We give heed. You know. Oh, ho, sent me you mighty lord. Anyway, so you can read through this. It's nonsensical, of course, but this is just a transformer trained on the character level for 1 million characters that come from Shakespeare. So there's sort of like blabbers on in Shakespeare-like manner, but it doesn't, of course, make sense at this scale. But I think still a pretty good demonstration of what's possible. So now I think that kind of concludes the programming section of this video. We basically kind of did a pretty good job of implementing this transformer. But the picture doesn't exactly match up to what we've done. So what's going on with all these additional parts here? So let me finish explaining this architecture and why it looks so funky. Basically what's happening here is what we implemented here is a decoder only transformer. So there's no component here. This part is called the encoder. And there's no cross attention block here. Our block only has a self attention and the feed forward. So it\", metadata={'id': '07', 'link': 'https://youtu.be/kCc8FmEb1nY', 'source': \"Let's build GPT： from scratch, in code, spelled out. 07\", 'title': \"Let's build GPT： from scratch, in code, spelled out.\"}),\n",
       " Document(page_content=\"are just not possible. And so we're basically wasting space, and not only that, but the s and the e are getting very crowded here. I was using these brackets because there's convention in natural language processing to use these kinds of brackets to denote special tokens, but we're going to use something else. So let's fix all this and make it prettier. We're not actually going to have two special tokens, we're only going to have one special token. So we're going to have nxn array of 27x27 instead. Instead of having two, we will just have one, and I will call it a dot. Let me swing this over here. Now one more thing that I would like to do is I would actually like to make this special character have position 0, and I would like to offset all the other letters off. I find that a little bit more pleasing. So we need a plus one here so that the first character, which is a, will start at 1. So s to i will now be a starts at 1 and dot is 0. And i to s, of course, we're not changing this because i to s just creates a reverse mapping and this will work fine. So 1 is a, 2 is b, 0 is dot. So we reverse that here, we have a dot and a dot. This should work fine. Make sure I start at 0s. Count. And then here we don't go up to 28, we go up to 27. And this should just work. So we see that dot dot never happened, it's at 0 because we don't have empty words. And this row here now is just very simply the counts for all the first letters. So j starts a word, h starts a word, i starts a word,\", metadata={'id': '02', 'link': 'https://youtu.be/PaCmpygFfXo', 'source': 'The spelled-out intro to language modeling： building makemore 02', 'title': 'The spelled-out intro to language modeling： building makemore'}),\n",
       " Document(page_content=\"answer or someone else can answer your questions. And I may also do a follow-up video that answers some of the most common questions. But for now, that's it. I hope you enjoyed it. If you did, then please like and subscribe so that YouTube knows to feature this video to more people. And that's it for now. I'll see you later. Now here's the problem. We know DL by... Wait, what is the problem? And that's everything I wanted to cover in this lecture. So I hope you enjoyed us building out micrograd... micrograb? Okay, now let's do the exact same thing for multiply because we can't do something like A times 2. Oops. I know what happened there.\", metadata={'id': '01', 'link': 'https://youtu.be/VMj-3S1tku0', 'source': 'The spelled-out intro to neural networks and backpropagation： building micrograd 01', 'title': 'The spelled-out intro to neural networks and backpropagation： building micrograd'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete by IDs\n",
    "vectorstore_pinecone.delete_by_id(ids=[\"1\",\"2\",\"3\"])\n",
    "vectorstore_pinecone.similarity_search(\"Foo Bar Baz\",k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc0c3b",
   "metadata": {},
   "source": [
    "## Supabase\n",
    "\n",
    "**Create index** \n",
    "\n",
    "* Create a new project in [Supabase dashboard](https://supabase.com/dashboard/project/xhbejgrankzufmczyqil).\n",
    "* In the project, go to the SQL editor on the left.\n",
    "* We need to create a table to store our embeddings.\n",
    "* We will use `pgvector`, an extension for PostgreSQL that allows you to both store and query vector embeddings.\n",
    "* Create the table in the SQL editor with [this code](https://supabase.com/docs/guides/ai/langchain), modified below for our table name `karpathy_gpt`:\n",
    "\n",
    "```\n",
    "-- Enable the pgvector extension to work with embedding vectors\n",
    "-- create extension vector;\n",
    "\n",
    "-- Create a table to store your documents\n",
    "create table karpathy_gpt (\n",
    "  id bigserial primary key,\n",
    "  content text, -- corresponds to Document.pageContent\n",
    "  metadata jsonb, -- corresponds to Document.metadata\n",
    "  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed\n",
    ");\n",
    "\n",
    "-- Create a function to search for documents\n",
    "CREATE OR REPLACE function match_documents (\n",
    "  query_embedding vector(1536),\n",
    "  match_count int default null,\n",
    "  filter jsonb DEFAULT '{}'\n",
    ") returns table (\n",
    "  id bigint,\n",
    "  content text,\n",
    "  metadata jsonb,\n",
    "  similarity float\n",
    ")\n",
    "language plpgsql\n",
    "as $$\n",
    "#variable_conflict use_column\n",
    "begin\n",
    "  return query\n",
    "  select\n",
    "    id,\n",
    "    content,\n",
    "    metadata,\n",
    "    1 - (karpathy_gpt.embedding <=> query_embedding) as similarity\n",
    "  from karpathy_gpt\n",
    "  where metadata @> filter\n",
    "  order by karpathy_gpt.embedding <=> query_embedding\n",
    "  limit match_count;\n",
    "end;\n",
    "$$;\n",
    "```\n",
    "\n",
    "* Now, the table is created!\n",
    "* In the project, you can find `SUPABASE_URL` and `SUPABASE_SERVICE_KEY`, which we will use to connect to this table.\n",
    "\n",
    "---\n",
    "\n",
    "**Python client:**\n",
    "\n",
    "(1) `delete by ids`: Delete documents by their IDs.\n",
    "\n",
    "```\n",
    "condition = {'id': 'your_id'}\n",
    "response = client.table(table).delete(condition)\n",
    "```\n",
    "\n",
    "(2) `update by ids`: Update documents by their IDs.\n",
    "\n",
    "```\n",
    "client = create_client(supabase_url, supabase_key)\n",
    "condition = {'id': 'your_id'}\n",
    "response = client.table(table).update(data, condition)\n",
    "```\n",
    "\n",
    "(3) `add by ids`: Add document with their IDs.\n",
    "\n",
    "* [`Insert`](https://supabase.com/docs/reference/python/insert) by ID:\n",
    "\n",
    "```\n",
    "client = create_client(supabase_url, supabase_key)\n",
    "data = {'id': 'custom_id', 'name': 'John Doe', 'age': 30}\n",
    "response = client.table(table).insert(data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Langchain:**\n",
    "\n",
    "(1) `delete by ids`: Delete documents by their IDs.\n",
    "\n",
    "* Create new method\n",
    "\n",
    "(2) `update by ids`: Update documents by their IDs.\n",
    "\n",
    "* Create new method using `update`\n",
    "\n",
    "(3) `add by ids`: Add document with their IDs.\n",
    "\n",
    "* `add_texts` is using `insert`, but does not support IDs (AFAICT).\n",
    "\n",
    "```\n",
    "result = client.from_(table_name).insert(chunk).execute()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97395e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c12532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import SupabaseVectorStore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from supabase.client import Client, create_client\n",
    "\n",
    "# Auth\n",
    "supabase_url = \"https://xhbejgrankzufmczyqil.supabase.co\"\n",
    "supabase_key = \"xxx\"\n",
    "supabase: Client = create_client(supabase_url, supabase_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b991ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new index\n",
    "table_name=\"karpathy_gpt\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# vectorstore_new = SupabaseVectorStore.from_texts(splits,embeddings,client=supabase,table_name=table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b645bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from index\n",
    "vectorstore_supabase = SupabaseVectorStore(client=supabase,embedding=embeddings,table_name=table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a30ba927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='baz', metadata={'source': '/docs'}),\n",
       " Document(page_content='foo', metadata={'source': '/docs'}),\n",
       " Document(page_content='bar', metadata={'source': '/docs'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add by IDs\n",
    "docs=[\n",
    "Document(page_content='foo',metadata={\"source\":'/docs'}),\n",
    "Document(page_content='bar',metadata={\"source\":'/docs'}),\n",
    "Document(page_content='baz',metadata={\"source\":'/docs'}),\n",
    "]\n",
    "vectorstore_supabase.add_documents_by_id(documents=docs,ids=[\"1000\",\"1002\",\"1003\"])\n",
    "vectorstore_supabase.similarity_search(\"Foo Bar Baz\",k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4647bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='foo', metadata={'source': '/docs'}),\n",
       " Document(page_content='bar', metadata={'source': '/docs'}),\n",
       " Document(page_content='biz', metadata={'source': '/docs'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update by IDs \n",
    "docs=[\n",
    "Document(page_content='foo',metadata={\"source\":'/docs'}),\n",
    "Document(page_content='bar',metadata={\"source\":'/docs'}),\n",
    "Document(page_content='biz',metadata={\"source\":'/docs'}),\n",
    "]\n",
    "vectorstore_supabase.update_documents_by_id(documents=docs,ids=[\"1000\",\"1002\",\"1003\"])\n",
    "vectorstore_supabase.similarity_search(\"Foo Bar Baz\",k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81352010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"the left here. So this is A, B creating E. And then E plus C creates D, just like we have it here. And finally, let's make this expression just one layer deeper. So D will not be the final output node. Instead, after D, we are going to create a new value object called F. We're going to start running out of variables soon. F will be negative 2.0. And its label will, of course, just be F. And then L, capital L, will be the output of our graph. And L will be T times F. OK. So L will be negative 8,\", metadata={}),\n",
       " Document(page_content=\"save it in each node. And then here, we're going to do label as A, label as B, label as C. And then let's create a special E equals A times B. And E dot label will be E. It's kind of naughty. And E will be E plus C. And a D dot label will be B. OK. So nothing really changes. I just added this new E function, new E variable. And then here, when we are printing this, I'm going to print the label here. So this will be a percent S bar. And this will be N dot label. And so now, we have the label on\", metadata={}),\n",
       " Document(page_content=\"is the output. So now, we don't just draw a D, we draw L. OK. And somehow, the label of L was undefined. Oops. L dot label has to be explicitly given to it. There we go. So L is the output. So let's quickly recap what we've done so far. We are able to build out mathematical expressions using only plus and times so far. They are scalar valued along the way. And we can do this forward pass and build out a mathematical expression. So we have multiple inputs here, A, B, C, and F, going into a\", metadata={})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete by IDs\n",
    "vectorstore_supabase.delete_by_id(ids=[\"1000\",\"1002\",\"1003\"])\n",
    "vectorstore_supabase.similarity_search(\"Foo Bar Baz\",k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69273f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
