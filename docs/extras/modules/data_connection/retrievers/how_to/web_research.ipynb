{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0ffe42",
   "metadata": {},
   "source": [
    "# WebResearchRetriever\n",
    "\n",
    "Given a query, this retriever will: \n",
    "\n",
    "* Formulate a set of relate Google searches\n",
    "* Search for each \n",
    "* Load all the resulting URLs\n",
    "* Then embed and perform similarity search with the query on the consolidate page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4abea0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.retrievers.web_research import WebResearchRetriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e57bb",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "Pass the desired model and vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d84ea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "# Set input\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "vectorstore = Chroma(embedding_function=OpenAIEmbeddings())\n",
    "GOOGLE_CSE_ID = \"xxx\"\n",
    "GOOGLE_API_KEY = \"xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f135e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "web_research_retriever = WebResearchRetriever(\n",
    "    vectorstore=vectorstore, \n",
    "    llm=llm, \n",
    "    GOOGLE_CSE_ID=GOOGLE_CSE_ID, \n",
    "    GOOGLE_API_KEY=GOOGLE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c958adc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.web_research:Generating questions for Google Search ...\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'How do LLM Powered Autonomous Agents work?', 'text': LineList(lines=['1. What is the definition of LLM Powered Autonomous Agents?', '2. What are the key features of LLM Powered Autonomous Agents?', '3. How do LLM Powered Autonomous Agents differ from traditional autonomous agents?', '4. What are the applications of LLM Powered Autonomous Agents?', '5. Are there any case studies or examples of successful implementations of LLM Powered Autonomous Agents?'])}\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. What is the definition of LLM Powered Autonomous Agents?', '2. What are the key features of LLM Powered Autonomous Agents?', '3. How do LLM Powered Autonomous Agents differ from traditional autonomous agents?', '4. What are the applications of LLM Powered Autonomous Agents?', '5. Are there any case studies or examples of successful implementations of LLM Powered Autonomous Agents?']\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:URLs to load: {'https://towardsdatascience.com/autonomous-agents-and-multi-agent-systems-101-agents-and-deception-f4da3401f92a', 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ...\n",
      "Fetching pages: 100%|##############################################################################################################################################| 2/2 [00:02<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.INFO)\n",
    "user_input = \"How do LLM Powered Autonomous Agents work?\"\n",
    "docs = web_research_retriever.get_relevant_documents(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52c07edd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663b2ba",
   "metadata": {},
   "source": [
    "`Local -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62e36e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 9132.71 MB (+ 1608.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 3200.00 MB\n",
      "ggml_metal_init: allocating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n",
      "llama_new_context_with_model: max tensor size =    87.89 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x295facb60\n",
      "ggml_metal_init: loaded kernel_mul                            0x295fadef0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x295faf260\n",
      "ggml_metal_init: loaded kernel_scale                          0x295fae150\n",
      "ggml_metal_init: loaded kernel_silu                           0x295fae3b0\n",
      "ggml_metal_init: loaded kernel_relu                           0x295fb00c0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x295fb0e50\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x295fb1330\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x295fb1d20\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x295fb23c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x295fb1f80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x295fb2ba0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x295fb3360\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x295fb3a90\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x295fb41b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x295fb48d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x295fb4ff0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x295fb5710\n",
      "ggml_metal_init: loaded kernel_norm                           0x295fb65c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x295fb6e70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x295fb7620\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x295fb7dc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x295fb8580\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x295fb8ee0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x295fb9620\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x295fb9de0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x295fba5f0\n",
      "ggml_metal_init: loaded kernel_rope                           0x295fbad00\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x295fbb6f0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x295fbc310\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x295fbcc80\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x295fbd5f0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, ( 6984.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1040.00 MB, ( 8024.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  3202.00 MB, (11226.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   597.00 MB, (11823.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB, (12335.52 / 21845.34)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "objc[77641]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2964a8208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x29acf8208). One of the two will be used. Which one is undefined.\n",
      "objc[77641]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2964a8208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x29b124208). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llama = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=4096,  # Context window\n",
    "    max_tokens=1000,  # Max tokens to generate\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")\n",
    "vectorstore_llama = Chroma(embedding_function=GPT4AllEmbeddings())\n",
    "GOOGLE_CSE_ID = \"b5e84267513eb4dcf\"\n",
    "GOOGLE_API_KEY = \"AIzaSyDUKwJCpdU6nNwANyA7NC2cXnMfvXD6YcM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39ff3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WebResearchRetriever\n",
    "web_research_retriever = WebResearchRetriever(\n",
    "    vectorstore=vectorstore_llama, \n",
    "    llm=llama, \n",
    "    GOOGLE_CSE_ID=GOOGLE_CSE_ID, \n",
    "    GOOGLE_API_KEY=GOOGLE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0898e34c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.web_research:Generating questions for Google Search ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Based on the user input search query \"How do LLM Powered Autonomous Agents work?\", here are five search queries that could help answer the question:\n",
      "\n",
      "1. What is an LLM (LLM Powered Autonomous Agents)?\n",
      "2. How do LLMs differ from traditional AI systems?\n",
      "3. What are some real-world applications of LLM Powered Autonomous Agents?\n",
      "4. How do LLMs enable Autonomous Agents to make decisions in real-time?\n",
      "5. What are the current challenges and limitations facing LLM Powered Autonomous Agents?\n",
      "\n",
      "These search queries could lead to a wealth of information about LLM Powered Autonomous Agents, such as their definition, capabilities, applications, and potential drawbacks. This information can help users better understand how these systems work and their potential uses in various industries."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  5144.38 ms\n",
      "llama_print_timings:      sample time =   139.14 ms /   198 runs   (    0.70 ms per token,  1423.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5143.89 ms /    99 tokens (   51.96 ms per token,    19.25 tokens per second)\n",
      "llama_print_timings:        eval time =  6624.20 ms /   197 runs   (   33.63 ms per token,    29.74 tokens per second)\n",
      "llama_print_timings:       total time = 12190.60 ms\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'How do LLM Powered Autonomous Agents work?', 'text': LineList(lines=['1. What is an LLM (LLM Powered Autonomous Agents)?\\n', '2. How do LLMs differ from traditional AI systems?\\n', '3. What are some real-world applications of LLM Powered Autonomous Agents?\\n', '4. How do LLMs enable Autonomous Agents to make decisions in real-time?\\n', '5. What are the current challenges and limitations facing LLM Powered Autonomous Agents?\\n'])}\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. What is an LLM (LLM Powered Autonomous Agents)?\\n', '2. How do LLMs differ from traditional AI systems?\\n', '3. What are some real-world applications of LLM Powered Autonomous Agents?\\n', '4. How do LLMs enable Autonomous Agents to make decisions in real-time?\\n', '5. What are the current challenges and limitations facing LLM Powered Autonomous Agents?\\n']\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT,\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'What is Generative AI? Everything You Need to Know', 'link': 'https://www.techtarget.com/searchenterpriseai/definition/generative-AI', 'snippet': 'Generative AI is a type of artificial intelligence technology that can ... of Bard built on its most advanced LLM, PaLM 2, which allows Bard to be more\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Agent System Overview In a LLM-powered autonomous agent system, ... It plays a crucial role in real-world tasks where trial and error are\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'Wireless Multi-Agent Generative AI: From Connected Intelligence to ...', 'link': 'https://arxiv.org/pdf/2307.02757', 'snippet': 'Jul 6, 2023 ... intelligent decision-making happens right at the edge. This article ... scene for realizing on-device LLMs, where multi-agent LLMs are.'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:URLs to load: {'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://www.techtarget.com/searchenterpriseai/definition/generative-AI', 'https://arxiv.org/pdf/2307.02757'}\n",
      "INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ...\n",
      "Fetching pages: 100%|##############################################################################################################################################| 3/3 [00:00<00:00,  4.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.INFO)\n",
    "user_input = \"How do LLM Powered Autonomous Agents work?\"\n",
    "docs = web_research_retriever.get_relevant_documents(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e06adad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
