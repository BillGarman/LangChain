{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0ffe42",
   "metadata": {},
   "source": [
    "# WebResearchRetriever\n",
    "\n",
    "Given a query, this retriever will: \n",
    "\n",
    "* Formulate a set of relate Google searches\n",
    "* Search for each \n",
    "* Load all the resulting URLs\n",
    "* Then embed and perform similarity search with the query on the consolidate page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4abea0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.retrievers.web_research import WebResearchRetriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e57bb",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "Pass the desired model and vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d84ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "# Set input\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "vectorstore = Chroma(embedding_function=OpenAIEmbeddings(),persist_directory=\"./chroma_db_oai\")\n",
    "GOOGLE_CSE_ID = \"xxx\"\n",
    "GOOGLE_API_KEY = \"xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f135e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "web_research_retriever = WebResearchRetriever(\n",
    "    vectorstore=vectorstore, \n",
    "    llm=llm, \n",
    "    GOOGLE_CSE_ID=GOOGLE_CSE_ID, \n",
    "    GOOGLE_API_KEY=GOOGLE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c958adc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.web_research:Generating questions for Google Search ...\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'How do LLM Powered Autonomous Agents work?', 'text': LineList(lines=['1. What is the definition of LLM Powered Autonomous Agents?\\n', '2. What are the key features of LLM Powered Autonomous Agents?\\n', '3. How do LLM Powered Autonomous Agents use machine learning algorithms?\\n', '4. What are some real-world applications of LLM Powered Autonomous Agents?\\n'])}\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. What is the definition of LLM Powered Autonomous Agents?\\n', '2. What are the key features of LLM Powered Autonomous Agents?\\n', '3. How do LLM Powered Autonomous Agents use machine learning algorithms?\\n', '4. What are some real-world applications of LLM Powered Autonomous Agents?\\n']\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Fig. 1. Overview of a LLM-powered autonomous agent system. ... This approach utilizes the Planning Domain Definition Language (PDDL) as an\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': \"Jun 23, 2023 ... In a LLM-powered autonomous agent system, LLM functions as the agent's brain, complemented by several key components: Planning.\"}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Agent System Overview In a LLM-powered autonomous agent system, ... It plays a crucial role in real-world tasks where trial and error are\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:URLs to load: {'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ...\n",
      "Fetching pages: 100%|##############| 1/1 [00:00<00:00,  4.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.INFO)\n",
    "user_input = \"How do LLM Powered Autonomous Agents work?\"\n",
    "docs = web_research_retriever.get_relevant_documents(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a23b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LLM-powered autonomous agents work by using a large language model (LLM) as their core controller. These agents have several key components that complement the LLM:\\n\\n1. Planning: Complex tasks are broken down into simpler steps through task decomposition. This can be done by prompting the LLM, providing task-specific instructions, or using human input. Self-reflection techniques help the agents learn from experience and improve their reasoning.\\n\\n2. Memory: Autonomous agents have different types of memory, including working memory and long-term memory. They can use contextual embeddings to understand the user's intent and context by incorporating entire conversation histories. This allows them to respond based on collective knowledge gained throughout the interaction with a user.\\n\\n3. Tool Use: Autonomous agents have the capacity to use tools such as browsing the internet, accessing live data, or running code. They can define goals and tasks, identify the right actions to take, and generate and execute commands with the help of external tools like search engines or APIs.\\n\\nThese components work together to enable autonomous agents to perform tasks, solve problems, and interact with users in a personalized and context-aware manner. However, there are still challenges to overcome, such as the finite context length, long-term planning, and reliability of natural language interfaces. Ongoing research is focused on addressing these challenges and improving the capabilities of LLM-powered autonomous agents.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "output = chain({\"input_documents\": docs, \"question\": user_input}, return_only_outputs=True)\n",
    "output['output_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663b2ba",
   "metadata": {},
   "source": [
    "`Local -`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e62e36e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 9132.71 MB (+ 1608.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 3200.00 MB\n",
      "ggml_metal_init: allocating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n",
      "llama_new_context_with_model: max tensor size =    87.89 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x2996f3910\n",
      "ggml_metal_init: loaded kernel_mul                            0x2996f4e40\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x2996f5660\n",
      "ggml_metal_init: loaded kernel_scale                          0x2996f5cf0\n",
      "ggml_metal_init: loaded kernel_silu                           0x2996f6460\n",
      "ggml_metal_init: loaded kernel_relu                           0x2996f3c60\n",
      "ggml_metal_init: loaded kernel_gelu                           0x2996f3ec0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x2996f77e0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x2996f7c90\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x2996f85b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x2996f8de0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x2996f9570\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x2996f9de0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x2996fa4d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x2996fabd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x2996fb2b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x2996fb9b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x2996fc0b0\n",
      "ggml_metal_init: loaded kernel_norm                           0x2996fc7e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x2996fd7b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x2996fdfa0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x2996fe760\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x2996fef40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x2996ff8a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x2b8804080\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x2b8804840\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x2b8805000\n",
      "ggml_metal_init: loaded kernel_rope                           0x2b8805710\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x2b8806100\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x2b8806cd0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x2b8807640\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x2b8807fb0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, ( 6984.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1040.00 MB, ( 8024.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  3202.00 MB, (11226.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   597.00 MB, (11823.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB, (12335.52 / 21845.34)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "objc[24358]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2b82ac208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x5de898208). One of the two will be used. Which one is undefined.\n",
      "objc[24358]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2b82ac208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x5decc4208). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llama = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=4096,  # Context window\n",
    "    max_tokens=1000,  # Max tokens to generate\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")\n",
    "vectorstore_llama = Chroma(embedding_function=GPT4AllEmbeddings(),persist_directory=\"./chroma_db_llama\")\n",
    "GOOGLE_CSE_ID = \"b5e84267513eb4dcf\"\n",
    "GOOGLE_API_KEY = \"AIzaSyDUKwJCpdU6nNwANyA7NC2cXnMfvXD6YcM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39ff3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WebResearchRetriever\n",
    "web_research_retriever = WebResearchRetriever(\n",
    "    vectorstore=vectorstore_llama, \n",
    "    llm=llama, \n",
    "    GOOGLE_CSE_ID=GOOGLE_CSE_ID, \n",
    "    GOOGLE_API_KEY=GOOGLE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0898e34c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.web_research:Generating questions for Google Search ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Based on the user input search query \"How do LLM Powered Autonomous Agents work?\", here are five search queries that could help answer their question:\n",
      "\n",
      "1. What are LLM Powered Autonomous Agents and how do they differ from traditional AI agents?\n",
      "2. How do LLM Powered Autonomous Agents learn and improve their decision-making abilities over time?\n",
      "3. Can you provide examples of real-world applications of LLM Powered Autonomous Agents, such as self-driving cars or personal assistants?\n",
      "4. How does the choice of LLM (Long Short-Term Memory) algorithm affect the performance and capabilities of an LLM Powered Autonomous Agent?\n",
      "5. What are some common challenges and limitations of LLM Powered Autonomous Agents, such as dealing with unexpected events or handling conflicting goals?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  7308.57 ms\n",
      "llama_print_timings:      sample time =   136.15 ms /   194 runs   (    0.70 ms per token,  1424.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7308.44 ms /    99 tokens (   73.82 ms per token,    13.55 tokens per second)\n",
      "llama_print_timings:        eval time =  6384.79 ms /   193 runs   (   33.08 ms per token,    30.23 tokens per second)\n",
      "llama_print_timings:       total time = 14072.87 ms\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'How do LLM Powered Autonomous Agents work?', 'text': LineList(lines=['1. What are LLM Powered Autonomous Agents and how do they differ from traditional AI agents?\\n', '2. How do LLM Powered Autonomous Agents learn and improve their decision-making abilities over time?\\n', '3. Can you provide examples of real-world applications of LLM Powered Autonomous Agents, such as self-driving cars or personal assistants?\\n', '4. How does the choice of LLM (Long Short-Term Memory) algorithm affect the performance and capabilities of an LLM Powered Autonomous Agent?\\n'])}\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. What are LLM Powered Autonomous Agents and how do they differ from traditional AI agents?\\n', '2. How do LLM Powered Autonomous Agents learn and improve their decision-making abilities over time?\\n', '3. Can you provide examples of real-world applications of LLM Powered Autonomous Agents, such as self-driving cars or personal assistants?\\n', '4. How does the choice of LLM (Long Short-Term Memory) algorithm affect the performance and capabilities of an LLM Powered Autonomous Agent?\\n']\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'The State of Autonomous AI Agents', 'link': 'https://www.linkedin.com/pulse/state-autonomous-ai-agents-dean-meyer?utm_source=share&utm_medium=member_ios&utm_campaign=share_via', 'snippet': 'Jul 6, 2023 ... To optimize LLMs and in-context learning, a three-tiered infrastructure is taking shape (more on these tiers by Matt at a16z). The LLM Stack: (1)\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... In a LLM-powered autonomous agent system, LLM functions as the ... agents to improve iteratively by refining past action decisions and\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'SQ2. What are the most important advances in AI? | One Hundred ...', 'link': 'https://ai100.stanford.edu/gathering-strength-gathering-storms-one-hundred-year-study-artificial-intelligence-ai100-2021-1/sq2', 'snippet': 'One of the practical applications can be seen in GAN-based medical-image ... of crowds and are important for mobile robots including self-driving cars.'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'Reflexion: Language Agents with Verbal Reinforcement Learning', 'link': 'https://arxiv.org/pdf/2303.11366', 'snippet': \"Jun 10, 2023 ... policy as an agent's memory encoding paired with a choice of LLM ... of the Reflexion process are the notion of short-term and long-term.\"}]\n",
      "INFO:langchain.retrievers.web_research:URLs to load: {'https://www.linkedin.com/pulse/state-autonomous-ai-agents-dean-meyer?utm_source=share&utm_medium=member_ios&utm_campaign=share_via', 'https://arxiv.org/pdf/2303.11366', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://ai100.stanford.edu/gathering-strength-gathering-storms-one-hundred-year-study-artificial-intelligence-ai100-2021-1/sq2'}\n",
      "INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ...\n",
      "Fetching pages: 100%|##############| 4/4 [00:01<00:00,  3.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.INFO)\n",
    "user_input = \"How do LLM Powered Autonomous Agents work?\"\n",
    "docs = web_research_retriever.get_relevant_documents(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e06adad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc053593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LLM Powered Autonomous Agents use a combination of planning, memory, and tool use to perform tasks. The agent breaks down large tasks into smaller subgoals, reflects on past actions, and learns from mistakes. It also uses external tools to access proprietary information sources and more. The agent utilizes short-term and long-term memory to retain and recall information over extended periods."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  7308.57 ms\n",
      "llama_print_timings:      sample time =    63.39 ms /    85 runs   (    0.75 ms per token,  1340.88 tokens per second)\n",
      "llama_print_timings: prompt eval time = 103653.11 ms /  2117 tokens (   48.96 ms per token,    20.42 tokens per second)\n",
      "llama_print_timings:        eval time =  3880.31 ms /    84 runs   (   46.19 ms per token,    21.65 tokens per second)\n",
      "llama_print_timings:       total time = 107710.45 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  LLM Powered Autonomous Agents use a combination of planning, memory, and tool use to perform tasks. The agent breaks down large tasks into smaller subgoals, reflects on past actions, and learns from mistakes. It also uses external tools to access proprietary information sources and more. The agent utilizes short-term and long-term memory to retain and recall information over extended periods.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"<<SYS>> \\n You are a QA assistant. Use the following pieces of context to answer the \n",
    "question at the end. Keep the answer as concise as possible. \\n <</SYS>> \\n\\n  [INST] Context: \n",
    "{context} \\n\n",
    "Question: {question} [/INST]\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "chain = load_qa_chain(llama, chain_type=\"stuff\", prompt=QA_CHAIN_PROMPT)\n",
    "output = chain({\"input_documents\": docs, \"question\": user_input}, return_only_outputs=True)\n",
    "output['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa99b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
