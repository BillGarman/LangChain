{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0ffe42",
   "metadata": {},
   "source": [
    "# WebResearchRetriever\n",
    "\n",
    "Given a query, this retriever will: \n",
    "\n",
    "* Formulate a set of relate Google searches\n",
    "* Search for each \n",
    "* Load all the resulting URLs\n",
    "* Then embed and perform similarity search with the query on the consolidate page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4abea0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.retrievers.web_research import WebResearchRetriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b1dcbd",
   "metadata": {},
   "source": [
    "`Simple usage`\n",
    "\n",
    "Specify the LLM to use for search query generation, and the retriver will do the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e63d1c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "# Vectorstore\n",
    "vectorstore = Chroma(embedding_function=OpenAIEmbeddings(),persist_directory=\"./chroma_db_oai\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Search \n",
    "os.environ[\"GOOGLE_CSE_ID\"] = \"xxx\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"xxx\"\n",
    "search = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c4e8ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.web_research:Generating questions for Google Search ...\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'How do LLM Powered Autonomous Agents work?', 'text': LineList(lines=['1. What is the definition of LLM Powered Autonomous Agents?\\n', '2. What are the key features of LLM Powered Autonomous Agents?\\n', '3. How do LLM Powered Autonomous Agents differ from traditional autonomous agents?\\n', '4. What are the applications of LLM Powered Autonomous Agents?\\n'])}\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. What is the definition of LLM Powered Autonomous Agents?\\n', '2. What are the key features of LLM Powered Autonomous Agents?\\n', '3. How do LLM Powered Autonomous Agents differ from traditional autonomous agents?\\n', '4. What are the applications of LLM Powered Autonomous Agents?\\n']\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Fig. 1. Overview of a LLM-powered autonomous agent system. ... This approach utilizes the Planning Domain Definition Language (PDDL) as an\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': \"Jun 23, 2023 ... In a LLM-powered autonomous agent system, LLM functions as the agent's brain, complemented by several key components: Planning.\"}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'Autonomous Agents & Agent Simulations', 'link': 'https://blog.langchain.dev/agents-round/', 'snippet': 'Apr 18, 2023 ... The main differences between the AutoGPT project and traditional LangChain agents can be attributed to different objectives. In AutoGPT, the\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Agent System Overview In a LLM-powered autonomous agent system, LLM functions ... simulacra of human behavior for interactive applications.'}]\n",
      "INFO:langchain.retrievers.web_research:URLs to load: {'https://blog.langchain.dev/agents-round/', 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ...\n",
      "Fetching pages: 100%|######################################################################################################################################################################################################| 2/2 [00:00<00:00,  2.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "web_research_retriever = WebResearchRetriever.from_llm(\n",
    "    vectorstore=vectorstore,\n",
    "    llm=llm, \n",
    "    search=search, \n",
    ")\n",
    "\n",
    "# Run\n",
    "user_input = \"How do LLM Powered Autonomous Agents work?\"\n",
    "docs = web_research_retriever.get_relevant_documents(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d39a90a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e57bb",
   "metadata": {},
   "source": [
    "`Added flexibility`\n",
    "\n",
    "Pass an LLM chain with custom prompt and output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d84ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers.pydantic import PydanticOutputParser\n",
    "\n",
    "# LLMChain\n",
    "search_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"<<SYS>> \\n You are a web research assistant to help users\n",
    "    answer questions. Answer using a numeric list. Do not include any extra\n",
    "    test. \\n <</SYS>> \\n\\n [INST] Given a user input search query, \n",
    "    generate a numbered list of five search queries to run to help answer their \n",
    "    question: \\n\\n {question} [/INST]\"\"\",\n",
    ")\n",
    "\n",
    "class LineList(BaseModel):\n",
    "    \"\"\"List of questions.\"\"\"\n",
    "\n",
    "    lines: List[str] = Field(description=\"Questions\")\n",
    "\n",
    "class QuestionListOutputParser(PydanticOutputParser):\n",
    "    \"\"\"Output parser for a list of numbered questions.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = re.findall(r\"\\d+\\..*?\\n\", text)\n",
    "        return LineList(lines=lines)\n",
    "    \n",
    "llm_chain = LLMChain(\n",
    "            llm=llm,\n",
    "            prompt=search_prompt,\n",
    "            output_parser=QuestionListOutputParser(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "851b0471",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.web_research:Generating questions for Google Search ...\n",
      "ERROR:langchain.callbacks.tracers.langchain:Failed to post https://api.langchain.plus/runs in LangSmith API. {\"detail\":\"Internal server error\"}\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'How do LLM Powered Autonomous Agents work?', 'text': LineList(lines=['1. What is the definition of LLM Powered Autonomous Agents?\\n', '2. What are the key features of LLM Powered Autonomous Agents?\\n', '3. How do LLM Powered Autonomous Agents use machine learning algorithms?\\n', '4. What are the applications of LLM Powered Autonomous Agents?\\n'])}\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. What is the definition of LLM Powered Autonomous Agents?\\n', '2. What are the key features of LLM Powered Autonomous Agents?\\n', '3. How do LLM Powered Autonomous Agents use machine learning algorithms?\\n', '4. What are the applications of LLM Powered Autonomous Agents?\\n']\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Fig. 1. Overview of a LLM-powered autonomous agent system. ... This approach utilizes the Planning Domain Definition Language (PDDL) as an\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': \"Jun 23, 2023 ... In a LLM-powered autonomous agent system, LLM functions as the agent's brain, complemented by several key components: Planning.\"}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Agent System Overview In a LLM-powered autonomous agent system, LLM functions ... simulacra of human behavior for interactive applications.'}]\n",
      "INFO:langchain.retrievers.web_research:URLs to load: {'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ...\n",
      "Fetching pages: 100%|######################################################################################################################################################################################################| 1/1 [00:00<00:00,  5.13it/s]\n",
      "ERROR:langchain.callbacks.tracers.langchain:Failed to patch https://api.langchain.plus/runs/f347c1aa-c27a-4855-9fa5-c3cd772c62f6 in LangSmith API. {\"detail\":\"Internal server error\"}\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "web_research_retriever_llm_chain = WebResearchRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    llm_chain=llm_chain, \n",
    "    search=search, \n",
    ")\n",
    "\n",
    "# Run\n",
    "docs = web_research_retriever_llm_chain.get_relevant_documents(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ee52163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9530c0",
   "metadata": {},
   "source": [
    "`Run locally`\n",
    "\n",
    "Specify LLM and embeddings that will run locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cf0d155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 9132.71 MB (+ 1608.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 3200.00 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n",
      "llama_new_context_with_model: max tensor size =    87.89 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x2cdaed120\n",
      "ggml_metal_init: loaded kernel_mul                            0x2cdaee650\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x2cdaeede0\n",
      "ggml_metal_init: loaded kernel_scale                          0x2cdaef460\n",
      "ggml_metal_init: loaded kernel_silu                           0x2cdaefbe0\n",
      "ggml_metal_init: loaded kernel_relu                           0x2cdaed470\n",
      "ggml_metal_init: loaded kernel_gelu                           0x2cdaed6d0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x2cdaf0f20\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x2cdaf13b0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x2cdaf1e70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x2cdaf2540\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x2cdaf2cd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x28c9914a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x28c991700\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x28c991960\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x28c991bc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x28c991e20\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x28c992210\n",
      "ggml_metal_init: loaded kernel_norm                           0x28c992470\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x28c992a10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x28c992c70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x28c992ed0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x28c993130\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x10f7b2be0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x10f7b2ef0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x10f7b3150\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x10f7b33b0\n",
      "ggml_metal_init: loaded kernel_rope                           0x10f7b3610\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x10f7b3c10\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x10f7b41e0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10f7b47b0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x10f7b4d80\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, ( 6984.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1040.00 MB, ( 8024.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  3202.00 MB, (11226.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   597.00 MB, (11823.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB, (12335.52 / 21845.34)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "objc[39697]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2cd900208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x5f2400208). One of the two will be used. Which one is undefined.\n",
      "objc[39697]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2cd900208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x5f282c208). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llama = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=4096,  # Context window\n",
    "    max_tokens=1000,  # Max tokens to generate\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "vectorstore_llama = Chroma(embedding_function=GPT4AllEmbeddings(),persist_directory=\"./chroma_db_llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e0561ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.web_research:Generating questions for Google Search ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Based on the user input search query \"How do LLM Powered Autonomous Agents work?\", here are five search queries that could be used to help answer this question:\n",
      "\n",
      "1. What is an LLM (Large Language Model) and how does it differ from other machine learning models?\n",
      "2. How do autonomous agents use LLMs to make decisions and take actions?\n",
      "3. Can you provide examples of real-world applications of LLM Powered Autonomous Agents, such as self-driving cars or virtual assistants?\n",
      "4. What are some potential risks or limitations associated with using LLM Powered Autonomous Agents in various industries or contexts?\n",
      "5. How do experts predict the future of LLM Powered Autonomous Agents will evolve as technology advances and becomes more integrated into our daily lives?\n",
      "These search queries could help provide a comprehensive overview of how LLM Powered Autonomous Agents work, their potential applications, risks and limitations, as well as the future outlook of this technology."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12546.80 ms\n",
      "llama_print_timings:      sample time =   166.15 ms /   236 runs   (    0.70 ms per token,  1420.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12546.65 ms /    99 tokens (  126.73 ms per token,     7.89 tokens per second)\n",
      "llama_print_timings:        eval time =  9499.58 ms /   235 runs   (   40.42 ms per token,    24.74 tokens per second)\n",
      "llama_print_timings:       total time = 22535.38 ms\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'How do LLM Powered Autonomous Agents work?', 'text': LineList(lines=['1. What is an LLM (Large Language Model) and how does it differ from other machine learning models?\\n', '2. How do autonomous agents use LLMs to make decisions and take actions?\\n', '3. Can you provide examples of real-world applications of LLM Powered Autonomous Agents, such as self-driving cars or virtual assistants?\\n', '4. What are some potential risks or limitations associated with using LLM Powered Autonomous Agents in various industries or contexts?\\n', '5. How do experts predict the future of LLM Powered Autonomous Agents will evolve as technology advances and becomes more integrated into our daily lives?\\n'])}\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. What is an LLM (Large Language Model) and how does it differ from other machine learning models?\\n', '2. How do autonomous agents use LLMs to make decisions and take actions?\\n', '3. Can you provide examples of real-world applications of LLM Powered Autonomous Agents, such as self-driving cars or virtual assistants?\\n', '4. What are some potential risks or limitations associated with using LLM Powered Autonomous Agents in various industries or contexts?\\n', '5. How do experts predict the future of LLM Powered Autonomous Agents will evolve as technology advances and becomes more integrated into our daily lives?\\n']\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'What Are Large Language Models and Why Are They Important ...', 'link': 'https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/', 'snippet': 'Jan 26, 2023 ... A large language model, or LLM, is a deep learning algorithm that ... languages or scenarios in which communication of different types is\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'Demystifying LLMs (Part 2): Autonomous Agents ... - The Agency Fund', 'link': 'https://www.agency.fund/post/demystifying-llms-part-2', 'snippet': 'Jun 23, 2023 ... Autonomous LLM agents can aid in data analysis tasks, ... they use LLMs to define goals and tasks, identify the right actions to take,\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'What are the 3 types of AI? A guide to narrow, general, and super ...', 'link': 'https://codebots.com/artificial-intelligence/the-3-types-of-ai-is-the-third-even-possible', 'snippet': 'Oct 24, 2017 ... There are 3 types of artificial intelligence (AI): narrow or weak AI, general or strong AI, and artificial superintelligence. We have\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'Economic potential of generative AI | McKinsey', 'link': 'https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier', 'snippet': 'Jun 14, 2023 ... Deep learning has powered many of the recent advances in AI, but the ... Notably, the potential value of using generative AI for several\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'Predictions for the State of AI and Robotics in 2025 | Pew Research ...', 'link': 'https://www.pewresearch.org/internet/2014/08/06/predictions-for-the-state-of-ai-and-robotics-in-2025/', 'snippet': 'Aug 6, 2014 ... As computer intelligence becomes increasingly integrated in daily life, a number of experts expect major changes in the way people manage their\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:URLs to load: {'https://codebots.com/artificial-intelligence/the-3-types-of-ai-is-the-third-even-possible', 'https://www.pewresearch.org/internet/2014/08/06/predictions-for-the-state-of-ai-and-robotics-in-2025/', 'https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier', 'https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/', 'https://www.agency.fund/post/demystifying-llms-part-2'}\n",
      "INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ...\n",
      "Fetching pages: 100%|######################################################################################################################################################################################################| 5/5 [00:02<00:00,  1.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "web_research_retriever = WebResearchRetriever.from_llm(\n",
    "    vectorstore=vectorstore_llama,\n",
    "    llm=llama, \n",
    "    search=search, \n",
    ")\n",
    "\n",
    "# Run\n",
    "user_input = \"How do LLM Powered Autonomous Agents work?\"\n",
    "docs = web_research_retriever.get_relevant_documents(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c6304d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f135e81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LLM powered autonomous agents use large language models as their core controller to perform various tasks. These agents consist of three components: planning, memory, and tools. The LLM is responsible for generating actions based on the current state of the agent and its goals. The planning component breaks down complex tasks into smaller ones, while the memory component stores knowledge gained throughout the interaction with a user. Finally, the tool use component allows the agent to fetch current information, access live data, or perform dynamic computations using external tools such as search engines or APIs."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12546.80 ms\n",
      "llama_print_timings:      sample time =    83.38 ms /   114 runs   (    0.73 ms per token,  1367.27 tokens per second)\n",
      "llama_print_timings: prompt eval time = 176621.27 ms /  3027 tokens (   58.35 ms per token,    17.14 tokens per second)\n",
      "llama_print_timings:        eval time =  6285.97 ms /   113 runs   (   55.63 ms per token,    17.98 tokens per second)\n",
      "llama_print_timings:       total time = 183147.12 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  LLM powered autonomous agents use large language models as their core controller to perform various tasks. These agents consist of three components: planning, memory, and tools. The LLM is responsible for generating actions based on the current state of the agent and its goals. The planning component breaks down complex tasks into smaller ones, while the memory component stores knowledge gained throughout the interaction with a user. Finally, the tool use component allows the agent to fetch current information, access live data, or perform dynamic computations using external tools such as search engines or APIs.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gengerate answer\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"<<SYS>> \\n You are a QA assistant. Use the following pieces of context to answer the \n",
    "question at the end. Keep the answer as concise as possible. \\n <</SYS>> \\n\\n  [INST] Context: \n",
    "{context} \\n\n",
    "Question: {question} [/INST]\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "chain = load_qa_chain(llama, chain_type=\"stuff\", prompt=QA_CHAIN_PROMPT)\n",
    "output = chain({\"input_documents\": docs, \"question\": user_input}, return_only_outputs=True)\n",
    "output['output_text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
