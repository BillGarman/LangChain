{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0ffe42",
   "metadata": {},
   "source": [
    "# WebResearchRetriever\n",
    "\n",
    "Given a query, this retriever will: \n",
    "\n",
    "* Formulate a set of relate Google searches\n",
    "* Search for each \n",
    "* Load all the resulting URLs\n",
    "* Then embed and perform similarity search with the query on the consolidate page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4abea0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.web_research import WebResearchRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b1dcbd",
   "metadata": {},
   "source": [
    "### Simple usage\n",
    "\n",
    "Specify the LLM to use for Google search query generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e63d1c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "# Vectorstore\n",
    "vectorstore = Chroma(embedding_function=OpenAIEmbeddings(),persist_directory=\"./chroma_db_oai\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Search \n",
    "os.environ[\"GOOGLE_CSE_ID\"] = \"xxx\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"xxx\"\n",
    "search = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "118b50aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "web_research_retriever = WebResearchRetriever.from_llm(\n",
    "    vectorstore=vectorstore,\n",
    "    llm=llm, \n",
    "    search=search, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39114da4",
   "metadata": {},
   "source": [
    "`Run with citations`\n",
    "\n",
    "We can use `RetrievalQAWithSourcesChain` to retrieve docs and provide citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b330acd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|######################################################################################################################################################################################################| 1/1 [00:00<00:00,  3.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'How do LLM Powered Autonomous Agents work?',\n",
       " 'answer': 'LLM Powered Autonomous Agents work by utilizing a large language model (LLM) as the core controller of the agent. The agent is complemented by several key components, including planning, memory, and tool use. In terms of planning, the agent breaks down tasks into smaller subgoals and can reflect on past actions to improve future results. The memory component includes both short-term and long-term memory, allowing the agent to learn in-context and retain and recall information over extended periods. Tool use involves the agent calling external APIs for additional information. There are also challenges associated with LLM-powered autonomous agents, such as finite context length, long-term planning and task decomposition, and the reliability of natural language interfaces. \\n\\n',\n",
       " 'sources': '\\n- https://lilianweng.github.io/posts/2023-06-23-agent/'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "user_input = \"How do LLM Powered Autonomous Agents work?\"\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm,retriever=web_research_retriever)\n",
    "result = qa_chain({\"question\": user_input})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357559fd",
   "metadata": {},
   "source": [
    "`Run with logging`\n",
    "\n",
    "Here, we use `get_relevant_documents` method to return docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c4e8ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.web_research:Generating questions for Google Search ...\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'What is Task Decomposition in LLM Powered Autonomous Agents?', 'text': LineList(lines=['1. How do LLM powered autonomous agents utilize task decomposition?\\n', '2. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n', '3. What role does task decomposition play in the functioning of LLM powered autonomous agents?\\n', '4. Why is task decomposition important for LLM powered autonomous agents?\\n'])}\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. How do LLM powered autonomous agents utilize task decomposition?\\n', '2. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n', '3. What role does task decomposition play in the functioning of LLM powered autonomous agents?\\n', '4. Why is task decomposition important for LLM powered autonomous agents?\\n']\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\" , \"What are the subgoals for achieving XYZ?'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\" , \"What are the subgoals for achieving XYZ?\" , (2)\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... In a LLM-powered autonomous agent system, LLM functions as the ... Task decomposition can be done (1) by LLM with simple prompting like\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Agent System Overview In a LLM-powered autonomous agent system, ... Task decomposition can be done (1) by LLM with simple prompting like\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:New URLs to load: []\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.INFO)\n",
    "user_input = \"What is Task Decomposition in LLM Powered Autonomous Agents?\"\n",
    "docs = web_research_retriever.get_relevant_documents(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d30c39",
   "metadata": {},
   "source": [
    "`Look at the URLs loaded`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51621ebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://lilianweng.github.io/posts/2023-06-23-agent/']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_research_retriever.get_urls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b681a846",
   "metadata": {},
   "source": [
    "`Generate answer using retrieved docs`\n",
    "\n",
    "We can use `load_qa_chain` for QA using the retrieved docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ceca5681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition in LLM-powered autonomous agents refers to the process of breaking down a complex task into smaller, more manageable subgoals. This allows the agent to efficiently handle and solve complex tasks by tackling them step by step. By decomposing a task, the agent can plan ahead and determine the sequence of actions required to achieve the overall goal. Task decomposition is an important component of planning in LLM-powered agents.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "output = chain({\"input_documents\": docs, \"question\": user_input},return_only_outputs=True)\n",
    "output['output_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e57bb",
   "metadata": {},
   "source": [
    "### More flexibility\n",
    "\n",
    "Pass an LLM chain with custom prompt and output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d84ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers.pydantic import PydanticOutputParser\n",
    "\n",
    "# LLMChain\n",
    "search_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an assistant tasked with improving Google search \n",
    "    results. Generate FIVE Google search queries that are similar to\n",
    "    this question. The output should be a numbered list of questions and each\n",
    "    should have a question mark at the end: {question}\"\"\",\n",
    ")\n",
    "\n",
    "class LineList(BaseModel):\n",
    "    \"\"\"List of questions.\"\"\"\n",
    "\n",
    "    lines: List[str] = Field(description=\"Questions\")\n",
    "\n",
    "class QuestionListOutputParser(PydanticOutputParser):\n",
    "    \"\"\"Output parser for a list of numbered questions.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = re.findall(r\"\\d+\\..*?\\n\", text)\n",
    "        return LineList(lines=lines)\n",
    "    \n",
    "llm_chain = LLMChain(\n",
    "            llm=llm,\n",
    "            prompt=search_prompt,\n",
    "            output_parser=QuestionListOutputParser(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "851b0471",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.web_research:Generating questions for Google Search ...\n",
      "ERROR:langchain.callbacks.tracers.langchain:Failed to post https://api.langchain.plus/runs in LangSmith API. {\"detail\":\"Internal server error\"}\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'How do LLM Powered Autonomous Agents work?', 'text': LineList(lines=['1. What is the definition of LLM Powered Autonomous Agents?\\n', '2. What are the key features of LLM Powered Autonomous Agents?\\n', '3. How do LLM Powered Autonomous Agents use machine learning algorithms?\\n', '4. What are the applications of LLM Powered Autonomous Agents?\\n'])}\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. What is the definition of LLM Powered Autonomous Agents?\\n', '2. What are the key features of LLM Powered Autonomous Agents?\\n', '3. How do LLM Powered Autonomous Agents use machine learning algorithms?\\n', '4. What are the applications of LLM Powered Autonomous Agents?\\n']\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Fig. 1. Overview of a LLM-powered autonomous agent system. ... This approach utilizes the Planning Domain Definition Language (PDDL) as an\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': \"Jun 23, 2023 ... In a LLM-powered autonomous agent system, LLM functions as the agent's brain, complemented by several key components: Planning.\"}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevat urls ...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Agent System Overview In a LLM-powered autonomous agent system, LLM functions ... simulacra of human behavior for interactive applications.'}]\n",
      "INFO:langchain.retrievers.web_research:URLs to load: {'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ...\n",
      "Fetching pages: 100%|######################################################################################################################################################################################################| 1/1 [00:00<00:00,  5.13it/s]\n",
      "ERROR:langchain.callbacks.tracers.langchain:Failed to patch https://api.langchain.plus/runs/f347c1aa-c27a-4855-9fa5-c3cd772c62f6 in LangSmith API. {\"detail\":\"Internal server error\"}\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "web_research_retriever_llm_chain = WebResearchRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    llm_chain=llm_chain, \n",
    "    search=search, \n",
    ")\n",
    "\n",
    "# Run\n",
    "docs = web_research_retriever_llm_chain.get_relevant_documents(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ee52163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9530c0",
   "metadata": {},
   "source": [
    "### Run locally\n",
    "\n",
    "Specify LLM and embeddings that will run locally (e.g., on your laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cf0d155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 9132.71 MB (+ 1608.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 3200.00 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n",
      "llama_new_context_with_model: max tensor size =    87.89 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x2c147aa30\n",
      "ggml_metal_init: loaded kernel_mul                            0x2c147bfd0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x2c147c820\n",
      "ggml_metal_init: loaded kernel_scale                          0x16c293d10\n",
      "ggml_metal_init: loaded kernel_silu                           0x16c294770\n",
      "ggml_metal_init: loaded kernel_relu                           0x16c2954f0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x16c295b90\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x16c296210\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x16c296960\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x16c2970e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x16c297810\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x16c297f90\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x2c147adc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x2c147d350\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x2c147ded0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x2c147e5f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x2c147ece0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x2c147f3e0\n",
      "ggml_metal_init: loaded kernel_norm                           0x2c147fb50\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x14afce5a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x14af8b5e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x109306050\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x14af89f80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x16c298b50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x16c298db0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x16c299780\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x2c147d740\n",
      "ggml_metal_init: loaded kernel_rope                           0x2c1480280\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x2c1481000\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x2c14819d0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x2c1482680\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x2c14828e0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, ( 6984.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1040.00 MB, ( 8024.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  3202.00 MB, (11226.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   597.00 MB, (11823.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB, (12335.52 / 21845.34)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "objc[77075]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2c1268208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x5e671c208). One of the two will be used. Which one is undefined.\n",
      "objc[77075]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2c1268208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x5e6b48208). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llama = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=4096,  # Context window\n",
    "    max_tokens=1000,  # Max tokens to generate\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "vectorstore_llama = Chroma(embedding_function=GPT4AllEmbeddings(),persist_directory=\"./chroma_db_llama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f93dd4",
   "metadata": {},
   "source": [
    "We supplied `StreamingStdOutCallbackHandler()`, so model outputs are streamed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e0561ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LlamaCpp\n",
      "  Sure, here are five Google search queries that are similar to \"How do LLM Powered Autonomous Agents work?\":\n",
      "\n",
      "1. What are the key components of an LLM-powered autonomous agent?\n",
      "2. How do LLMs enable autonomous agents to make decisions?\n",
      "3. Can you explain the training process for an LLM-powered autonomous agent?\n",
      "4. What are some real-world applications of LLM-powered autonomous agents?\n",
      "5. How do LLM-powered autonomous agents handle unexpected events or situations?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  8929.09 ms\n",
      "llama_print_timings:      sample time =    88.29 ms /   125 runs   (    0.71 ms per token,  1415.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8928.89 ms /    97 tokens (   92.05 ms per token,    10.86 tokens per second)\n",
      "llama_print_timings:        eval time =  8130.48 ms /   124 runs   (   65.57 ms per token,    15.25 tokens per second)\n",
      "llama_print_timings:       total time = 17310.78 ms\n",
      "Fetching pages: 100%|######################################################################################################################################################################################################| 1/1 [00:00<00:00,  3.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'How do LLM Powered Autonomous Agents work?',\n",
       " 'answer': 'LLM Powered Autonomous Agents work by utilizing a large language model (LLM) as the core controller of the agent. The agent system consists of several key components, including planning, memory, and tool use. In terms of planning, the agent breaks down complex tasks into smaller subgoals and can reflect on past actions to improve future results. The memory component includes both short-term and long-term memory, allowing the agent to learn in-context and retain and recall information over extended periods. The tool use component involves the agent calling external APIs for additional information. There are also case studies, such as scientific discovery agents and generative agents simulations, that demonstrate the capabilities of LLM-powered autonomous agents. However, there are challenges, such as the limited context length, difficulties in long-term planning and task decomposition, and the reliability of natural language interfaces. Self-reflection is also an important aspect of LLM-powered autonomous agents, allowing them to refine past actions and learn from mistakes. There are different approaches to implementing LLM-powered agents, including LLM+P, which involves using an external classical planner for long-horizon planning, and ReAct, which integrates reasoning and acting within LLM. \\n\\n',\n",
       " 'sources': '\\n- https://lilianweng.github.io/posts/2023-06-23-agent/'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize\n",
    "web_research_retriever = WebResearchRetriever.from_llm(\n",
    "    vectorstore=vectorstore_llama,\n",
    "    llm=llama, \n",
    "    search=search, \n",
    ")\n",
    "\n",
    "# Run\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm,retriever=web_research_retriever)\n",
    "result = qa_chain({\"question\": user_input})\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
