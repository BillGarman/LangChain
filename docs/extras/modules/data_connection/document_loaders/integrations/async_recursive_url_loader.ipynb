{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AsyncRecursiveURLLoader\n",
    "\n",
    "This notebook demonstrates how to use the AsyncRecursiveURLLoader to load a document from a URL.\n",
    "\n",
    "This loader will fetch the document from the URL, then find every accessible in the webpage. It will then fetch each of those URLs, and repeat the process until all URLs have been fetched, or the depth limit has been reached.\n",
    "\n",
    "Please note that this loader is an asynchronous one, thus, lazy_loading won't work as excepted. If you want to use lazy_loading, you should use the RecursiveURLLoader instead. Also, this loader will be obviously slower than other loaders as it has exponentially more requests to make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "To use this loader, it's necessary to install the `aiohttp` and `asyncio` package:\n",
    "\n",
    "```bash\n",
    "pip install aiohttp asyncio\n",
    "```\n",
    "\n",
    "It is also highly recommended to use beautifulsoup4 or goose3 to extract the text from the HTML document. But if you would like to write a custom filter, you can use any other package or write your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "AsyncRecursiveURLLoader takes the following parameters:\n",
    "- url, the URL string to load the document from.\n",
    "- exclude_dirs, optional, a list of directories to exclude from the recursive search.\n",
    "- raw_webpage_to_text_converter, optional, a function that takes a raw webpage and returns the text from it. It should be a function from string to string.\n",
    "- max_depth, optional, the maximum depth to search for URLs. If not specified, by default, it will be 2.\n",
    "- prevent_outside, optional, when enabled, pages outside of the given url won't be crawled.\n",
    "- timeout, optional, an aiohttp.ClientTimeout object provided to the aiohttp session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.async_recursive_url_loader import AsyncRecursiveUrlLoader\n",
    "import goose3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(raw: str) -> str:\n",
    "    extractor = goose3.Goose()\n",
    "    article = extractor.extract(raw_html=raw)\n",
    "    return article.cleaned_text\n",
    "\n",
    "url = \"https://python.langchain.com/docs/get_started\"\n",
    "loader = AsyncRecursiveUrlLoader(url, max_depth=2, raw_webpage_to_text_converter=converter)\n",
    "loader.load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
