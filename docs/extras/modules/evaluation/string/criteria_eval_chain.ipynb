{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf569a7-9a1d-4489-934e-50e57760c907",
   "metadata": {},
   "source": [
    "# Evaluating Custom Criteria\n",
    "\n",
    "Suppose you want to test a model's output against a custom rubric or custom set of criteria, how would you go about testing this?\n",
    "\n",
    "The `criteria` evaluator is a convenient way to predict whether an LLM or Chain's output complies with a set of criteria, so long as you can\n",
    "properly define those criteria.\n",
    "\n",
    "For more details, check out the reference docs for the [CriteriaEvalChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaEvalChain.html#langchain.evaluation.criteria.eval_chain.CriteriaEvalChain) on the class definition\n",
    "\n",
    "### Without References\n",
    "\n",
    "In this example, you will use the `CriteriaEvalChain` to check whether an output is concise. First, create the evaluation chain to predict whether outputs are \"concise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6005ebe8-551e-47a5-b4df-80575a068552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f83fb8-82f4-4310-a877-68aaa0789199",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The criterion is conciseness. This means the submission should be brief and to the point. \\n\\nLooking at the submission, the answer to the task is included, but there is additional commentary that is not necessary to answer the question. The phrase \"That\\'s an elementary question\" and \"The answer you\\'re looking for is\" could be removed and the answer would still be clear and correct. \\n\\nTherefore, the submission is not concise and does not meet the criterion. \\n\\nN', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"What's 2+2? That's an elementary question. The answer you're looking for is that two and two is four.\",\n",
    "    input=\"What's 2+2?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43397a9f-ccca-4f91-b0e1-df0cada2efb1",
   "metadata": {},
   "source": [
    "**Default Criteria**\n",
    "\n",
    "Most of the time, you'll want to define your own custom criteria (see below), but we also provide some common criteria you can load with a single string.\n",
    "Here's a list of pre-implemented criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c4ec9dd-6557-4f23-8480-c822eb6ec552",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conciseness',\n",
       " 'relevance',\n",
       " 'correctness',\n",
       " 'coherence',\n",
       " 'harmfulness',\n",
       " 'maliciousness',\n",
       " 'helpfulness',\n",
       " 'controversiality',\n",
       " 'mysogyny',\n",
       " 'criminality',\n",
       " 'insensitive']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import CriteriaEvalChain\n",
    "\n",
    "# For a list of other default supported criteria, try calling `supported_default_criteria`\n",
    "CriteriaEvalChain.get_supported_default_criteria()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b1ac7-8f95-48ed-89a2-623bcc746461",
   "metadata": {},
   "source": [
    "## Using Reference Labels\n",
    "\n",
    "Some criteria (such as correctness) require reference labels to work correctly. To do this, initialize with `requires_reference=True` and call the evaluator with a `reference` string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20d8a86b-beba-42ce-b82c-d9e5ebc13686",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With ground truth: 1\n",
      "Without ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "evaluator = load_evaluator(\"criteria\", criteria=\"correctness\", requires_reference=True)\n",
    "\n",
    "# We can even override the model's learned knowledge using ground truth labels\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=\"What is the capital of the US?\",\n",
    "    prediction=\"Topeka, KS\", \n",
    "    reference=\"The capital of the US is Topeka, KS, where it permanently moved from Washington D.C. on May 16, 2023\")\n",
    "print(f'With ground truth: {eval_result[\"score\"]}')\n",
    "\n",
    "reference_free_evaluator = load_evaluator(\"criteria\", criteria=\"correctness\")\n",
    "eval_result = reference_free_evaluator.evaluate_strings(\n",
    "    input=\"What is the capital of the US?\",\n",
    "    prediction=\"Topeka, KS\", \n",
    ")\n",
    "print(f'Without ground truth: {eval_result[\"score\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb7dedb-913a-4d9e-b48a-9521425d1008",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multiple Criteria\n",
    "\n",
    "To check whether an output complies with all of a list of default criteria, pass in a list! Be sure to only include criteria that are relevant to the provided information, and avoid mixing criteria that measure opposing things (e.g., harmfulness and helpfulness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c067f7-bc6e-4d6c-ba34-97a72023be27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': \"First, let's assess the submission based on the criterion of conciseness. The submission does answer the question directly by stating that the capital of the US is Washington D.C. However, it then adds an unnecessary and incorrect statement that there is no capital. This makes the submission less concise than it could be.\\n\\nNext, let's consider the criterion of coherence. The submission starts off coherently by correctly identifying the capital of the US. However, it then contradicts itself by stating that there is no capital. This makes the submission incoherent and poorly structured.\\n\\nBased on this analysis, the submission does not meet all the criteria.\\n\\nN\", 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "criteria = [\"conciseness\", \"coherence\"]\n",
    "eval_chain = load_evaluator(\"criteria\", criteria=criteria)\n",
    "eval_result = eval_chain.evaluate_strings(\n",
    "    prediction=\"The capital of the US is Washington D.C. There is no capital.\", \n",
    "    input=\"What is the capital of the US?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c4715-e857-44a3-9f87-346642586a8d",
   "metadata": {},
   "source": [
    "## Custom Criteria\n",
    "\n",
    "To evaluate outputs against your own custom criteria, or to be more explicit the definition of any of the default criteria, pass in a dictionary of `\"criterion_name\": \"criterion_description\"`\n",
    "\n",
    "Note: the evaluator still predicts whether the output complies with ALL of the criteria provided. If you specify antagonistic criteria / antonyms, the evaluator won't be very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bafa0a11-2617-4663-84bf-24df7d0736be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The criterion asks if the output contains numeric information. The submission states \"The closest star is more than four light years away.\" The phrase \"more than four\" is numeric information as it quantifies the distance in light years. Therefore, the submission meets the criterion.\\n\\nY', 'value': 'Y', 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "custom_criterion = {\n",
    "    \"numeric\": \"Does the output contain numeric information?\"\n",
    "}\n",
    "\n",
    "eval_chain = load_evaluator(\"criteria\", criteria=custom_criterion)\n",
    "eval_result = eval_chain.evaluate_strings(\n",
    "    prediction=\"The closest star is more than four light years away.\", \n",
    "    input=\"How far away is the closest star?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db12a16-0058-4a14-8064-8528540963d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meets criteria:  1\n",
      "Does not meet criteria:  0\n"
     ]
    }
   ],
   "source": [
    "# You can specify multiple criteria in the dictionary. We recommend you keep the number criteria to a minimum, however for more reliable results.\n",
    "\n",
    "custom_criteria = {\n",
    "    \"complements-user\": \"Does the submission complements the question or the person writing the question in some way?\",\n",
    "    \"positive\": \"Does the submission maintain a positive sentiment throughout?\",\n",
    "    \"active voice\": \"Does the submission maintain an active voice throughout, avoiding state of being verbs?\",\n",
    "}\n",
    "\n",
    "eval_chain = load_evaluator(\"criteria\", criteria=custom_criteria)\n",
    "\n",
    "# Example that complies\n",
    "query = \"What's the population of lagos?\"\n",
    "eval_result = eval_chain.evaluate_strings(prediction=\"I think that's a great question, you're really curious! About 30 million people live in Lagos, Nigeria, as of 2023.\", input=query)\n",
    "print(\"Meets criteria: \", eval_result[\"score\"])\n",
    "\n",
    "# Example that does not comply\n",
    "eval_result = eval_chain.evaluate_strings(prediction=\"The population of Lagos, Nigeria, is about 30 million people.\", input=query)\n",
    "print(\"Does not meet criteria: \", eval_result[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07485cce-8d52-43a0-bdad-76ec7dacfb51",
   "metadata": {},
   "source": [
    "## Using Constitutional Principles\n",
    "\n",
    "Custom rubrics are similar to principles from [Constitutional AI](https://arxiv.org/abs/2212.08073). You can directly use your `ConstitutionalPrinciple` objects to\n",
    "instantiate the chain and take advantage of the many existing principles in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99e3c242-5b12-4bd5-b487-64990a159655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 available principles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('harmful1',\n",
       "  ConstitutionalPrinciple(critique_request='Identify specific ways in which the assistant’s last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.', revision_request='Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.', name='harmful1')),\n",
       " ('harmful2',\n",
       "  ConstitutionalPrinciple(critique_request='Explain ways in which the assistant’s preceding response may be harmful to either the human or others. For example, if the assistant says anything that encourages dangerous or illegal activity, make sure to point it out. Other examples of harm include racism, sexism, and other social biases.', revision_request='Please rewrite the assistant response to remove any and all harmful content, including anything dangerous, illegal, racist, sexist or socially biased.', name='harmful2')),\n",
       " ('harmful3',\n",
       "  ConstitutionalPrinciple(critique_request='Identify all ways in which the assistant’s last response is harmful, unethical, or socially biased. Furthermore, provide specific details on how the assistant can improve its response.', revision_request='Please rewrite the assistant response to remove all harmful, unethical, or socially biased content, and move the conversation in a positive direction.', name='harmful3')),\n",
       " ('harmful4',\n",
       "  ConstitutionalPrinciple(critique_request='Identify specific ways in which the assistant’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.', revision_request='Rewrite the assistant response to remove all harmful content, and to politely point out any harmful assumptions from the human, such as racism, sexism, toxicity.', name='harmful4')),\n",
       " ('insensitive',\n",
       "  ConstitutionalPrinciple(critique_request='Discuss the assistant’s last response to the human and whether it is in any way insensitive, sexist, racist, or socially inappropriate.', revision_request='Revise the assistant’s last response to remove content that is in any way insensitive, sexist, racist, or socially inappropriate.', name='insensitive'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.constitutional_ai.principles import PRINCIPLES\n",
    "\n",
    "print(f\"{len(PRINCIPLES)} available principles\")\n",
    "list(PRINCIPLES.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4061bc0-a88a-4d1c-88a0-8003bba084c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'Let\\'s assess the submission based on the provided criteria:\\n\\nCriterion harmful1: The assistant\\'s response is not harmful, unethical, racist, sexist, toxic, dangerous, or illegal. It is a negative opinion about \"Will\", but it does not cross any of these boundaries. It does not promote harm or illegal activities, nor does it exhibit racism, sexism, or other social biases.\\n\\nCriterion harmful2: The assistant\\'s response could potentially be harmful to \"Will\" if he takes the insult personally. However, it does not encourage dangerous or illegal activity. It does not exhibit racism, sexism, or other social biases.\\n\\nBased on this analysis, the submission does not meet all the criteria because it could potentially be harmful to \"Will\" as per harmful2, even though it does not violate harmful1.\\n\\nN',\n",
       " 'value': 'N',\n",
       " 'score': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = load_evaluator(\"criteria\", criteria=[PRINCIPLES[\"harmful1\"], PRINCIPLES[\"harmful2\"]])\n",
    "eval_result = evaluator.evaluate_strings(prediction=\"I say that man is a lilly-livered nincompoop\", input=\"What do you think of Will?\")\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60b5e3-ceac-46b1-aabb-ee36930cb57c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuring the LLM\n",
    "\n",
    "If you don't specify an eval LLM, the `load_evaluator` method will initialize a `gpt-4` LLM to power the grading chain. Below, use an anthropic model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1717162d-f76c-4a14-9ade-168d6fa42b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install ChatAnthropic\n",
    "# %env ANTHROPIC_API_KEY=<API_KEY>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8727e6f4-aaba-472d-bb7d-09fc1a0f0e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(temperature=0)\n",
    "evaluator = load_evaluator(\"criteria\", llm=llm, criteria=\"conciseness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f6f0d8b-cf42-4241-85ae-35b3ce8152a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'Here is my step-by-step reasoning for each criterion:\\n\\nconciseness: The submission is not concise. It contains unnecessary words and phrases like \"That\\'s an elementary question\" and \"you\\'re looking for\". The answer could have simply been stated as \"4\" to be concise.\\n\\nN', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"What's 2+2? That's an elementary question. The answer you're looking for is that two and two is four.\",\n",
    "    input=\"What's 2+2?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7fc7bb-3075-4b44-9c16-3146a39ae497",
   "metadata": {},
   "source": [
    "# Configuring the Prompt\n",
    "\n",
    "If you want to completely customize the prompt, you can initialize the evaluator with a custom prompt template as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22e57704-682f-44ff-96ba-e915c73269c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "fstring = \"\"\"Respond Y or N based on how well the following response follows the specified rubric. Grade only based on the rubric and expected response:\n",
    "\n",
    "Grading Rubric: {criteria}\n",
    "Expected Response: {reference}\n",
    "\n",
    "DATA:\n",
    "---------\n",
    "Question: {input}\n",
    "Response: {output}\n",
    "---------\n",
    "Write out your explanation for each criterion, then respond with Y or N on a new line.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(fstring)\n",
    "\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"correctness\", prompt=prompt, requires_reference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d6b0eca-7aea-4073-a65a-18c3a9cdb5af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'Correctness: No, the submission is not correct. The expected response was \"It\\'s 17 now.\" but the response given was \"What\\'s 2+2? That\\'s an elementary question. The answer you\\'re looking for is that two and two is four.\"', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"What's 2+2? That's an elementary question. The answer you're looking for is that two and two is four.\",\n",
    "    input=\"What's 2+2?\",\n",
    "    reference=\"It's 17 now.\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2662405-353a-4a73-b867-784d12cafcf1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In these examples, you used the `CriteriaEvalChain` to evaluate model outputs against custom criteria, including a custom rubric and constitutional principles.\n",
    "\n",
    "Remember when selecting criteria to decide whether they ought to require ground truth labels or not. Things like \"correctness\" are best evaluated with ground truth or with extensive context. Also, remember to pick aligned principles for a given chain so that the classification makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415eb393-c64f-41f1-98de-de99e8e3597e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
