{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "\n",
    "[Ollama](https://ollama.ai/) allows you to run open-source large language models, such as Llama 2, locally.\n",
    "\n",
    "Ollama bundles model weights, configuration, and data into a single package, defined by a Modelfile. \n",
    "\n",
    "It optimizes setup and configuration details, including GPU usage.\n",
    "\n",
    "For a complete list of supported models and model variants, see the [Ollama model library](https://ollama.ai/library).\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, follow [these instructions](https://github.com/jmorganca/ollama) to set up and run a local Ollama instance:\n",
    "\n",
    "* [Download](https://ollama.ai/download)\n",
    "* Fetch a model via `ollama pull <model family>`\n",
    "* e.g., for `Llama-7b`: `ollama pull llama2`\n",
    "* This will download the most basic version of the model typically (e.g., smallest # parameters and `q4_0`)\n",
    "* On Mac, it will download to \n",
    "\n",
    "`~/.ollama/models/manifests/registry.ollama.ai/library/<model family>/latest`\n",
    "\n",
    "* And we specify a particular version, e.g., for `ollama pull vicuna:13b-v1.5-16k-q4_0`\n",
    "* The file is here with the model version in place of `latest`\n",
    "\n",
    "`~/.ollama/models/manifests/registry.ollama.ai/library/vicuna/13b-v1.5-16k-q4_0`\n",
    "\n",
    "You can easily access models in a few ways:\n",
    "\n",
    "1/ if the app is running:\n",
    "* All of your local models are automatically served on `localhost:11434`\n",
    "* Select your model when setting `llm = Ollama(..., model=\"<model family>:<version>\")`\n",
    "* If you set `llm = Ollama(..., model=\"<model family\")` withoout a version it will simply look for `latest`\n",
    "\n",
    "2/ if building from source or just running the binary: \n",
    "* Then you must run `ollama serve`\n",
    "* All of your local models are automatically served on `localhost:11434`\n",
    "* Then, select as shown above\n",
    "\n",
    "\n",
    "## Usage\n",
    "\n",
    "You can see a full list of supported parameters on the [API reference page](https://api.python.langchain.com/en/latest/llms/langchain.llms.ollama.Ollama.html).\n",
    "\n",
    "If you are using a LLaMA `chat` model (e.g., `ollama pull llama2:7b-chat`) then you can use the `ChatOllama` interface.\n",
    "\n",
    "This includes [special tokens](https://huggingface.co/blog/llama2#how-to-prompt-llama-2) for system message and user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler                                  \n",
    "chat_model = ChatOllama(base_url=\"http://localhost:11434\", \n",
    "             model=\"llama2:7b-chat\", \n",
    "             callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `StreamingStdOutCallbackHandler`, you will see tokens streamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Artificial intelligence (AI) has a rich and varied history that spans several decades. Hinweis: This is a brief overview of some of the key milestones in the history of AI. For a more detailed account, I recommend checking out some of the many books on the subject or visiting a reputable online resource.\n",
      "\n",
      "1. Early Ideas and Concepts (1950s-1960s): The concept of artificial intelligence can be traced back to ancient Greece, where myths spoke of artificial creatures crafted by humans. However, the modern field of AI began to take shape in the 1950s with the work of computer scientists like Alan Turing and Marvin Minsky. They proposed the idea of a machine that could think and learn like a human, leading to the development of the first AI programs.\n",
      "2. Rule-Based Systems (1970s-1980s): In the 1970s and 1980s, AI research focused on developing rule-based systems, which used pre-defined rules to reason and make decisions. This led to the development of expert systems, which were designed to solve specific problems by mimicking the decision-making process of a human expert in a particular domain.\n",
      "3. Machine Learning (1990s-Present): In the 1990s, machine learning emerged as a new approach to AI. This involves training algorithms on large datasets to enable them to learn and improve their performance over time. With the rise of big data and advances in computing power, machine learning has become a dominant force in modern AI research.\n",
      "4. Deep Learning (2000s-Present): In the 2000s, deep learning algorithms were developed, which are capable of learning and improving their performance on complex tasks such as image recognition, natural language processing, and speech recognition. These algorithms use multiple layers of artificial neural networks to learn and represent complex patterns in data.\n",
      "5. Natural Language Processing (NLP) (1980s-Present): NLP is a subfield of AI that focuses on enabling machines to understand, interpret, and generate human language. This has applications in areas such as chatbots, virtual assistants, and language translation.\n",
      "6. Robotics (1980s-Present): Robotics involves the development of robots that can perform tasks that typically require human intelligence, such as manipulation, locomotion, and interaction with their environment.\n",
      "7. Computer Vision (1980s-Present): Computer vision is a subfield of AI that focuses on enabling machines to interpret and understand visual data from the world around them. This has applications in areas such as image recognition, object detection, and autonomous vehicles.\n",
      "8. Ethical and Social Implications (1990s-Present): As AI has become more advanced and integrated into various aspects of society, there have been growing concerns about its ethical and social implications. This includes issues related to privacy, bias, and the impact of automation on jobs and communities.\n",
      "9. Reinforcement Learning (2000s-Present): Reinforcement learning is a subfield of machine learning that involves training algorithms to make decisions based on rewards or penalties. This has applications in areas such as game playing, robotics, and autonomous vehicles.\n",
      "10. Edge AI (2010s-Present): With the proliferation of IoT devices and the growth of cloud computing, there has been a growing interest in edge AI, which involves performing AI tasks at the edge of the network, closer to the source of the data. This can reduce latency and improve real-time performance for applications such as autonomous vehicles and smart homes.\n",
      "\n",
      "These are some of the key milestones in the history of AI, but there is still much ongoing research and development in the field. As AI continues to evolve, it is likely that new subfields and applications will emerge, further expanding its reach and impact on society."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=' Artificial intelligence (AI) has a rich and varied history that spans several decades. Hinweis: This is a brief overview of some of the key milestones in the history of AI. For a more detailed account, I recommend checking out some of the many books on the subject or visiting a reputable online resource.\\n\\n1. Early Ideas and Concepts (1950s-1960s): The concept of artificial intelligence can be traced back to ancient Greece, where myths spoke of artificial creatures crafted by humans. However, the modern field of AI began to take shape in the 1950s with the work of computer scientists like Alan Turing and Marvin Minsky. They proposed the idea of a machine that could think and learn like a human, leading to the development of the first AI programs.\\n2. Rule-Based Systems (1970s-1980s): In the 1970s and 1980s, AI research focused on developing rule-based systems, which used pre-defined rules to reason and make decisions. This led to the development of expert systems, which were designed to solve specific problems by mimicking the decision-making process of a human expert in a particular domain.\\n3. Machine Learning (1990s-Present): In the 1990s, machine learning emerged as a new approach to AI. This involves training algorithms on large datasets to enable them to learn and improve their performance over time. With the rise of big data and advances in computing power, machine learning has become a dominant force in modern AI research.\\n4. Deep Learning (2000s-Present): In the 2000s, deep learning algorithms were developed, which are capable of learning and improving their performance on complex tasks such as image recognition, natural language processing, and speech recognition. These algorithms use multiple layers of artificial neural networks to learn and represent complex patterns in data.\\n5. Natural Language Processing (NLP) (1980s-Present): NLP is a subfield of AI that focuses on enabling machines to understand, interpret, and generate human language. This has applications in areas such as chatbots, virtual assistants, and language translation.\\n6. Robotics (1980s-Present): Robotics involves the development of robots that can perform tasks that typically require human intelligence, such as manipulation, locomotion, and interaction with their environment.\\n7. Computer Vision (1980s-Present): Computer vision is a subfield of AI that focuses on enabling machines to interpret and understand visual data from the world around them. This has applications in areas such as image recognition, object detection, and autonomous vehicles.\\n8. Ethical and Social Implications (1990s-Present): As AI has become more advanced and integrated into various aspects of society, there have been growing concerns about its ethical and social implications. This includes issues related to privacy, bias, and the impact of automation on jobs and communities.\\n9. Reinforcement Learning (2000s-Present): Reinforcement learning is a subfield of machine learning that involves training algorithms to make decisions based on rewards or penalties. This has applications in areas such as game playing, robotics, and autonomous vehicles.\\n10. Edge AI (2010s-Present): With the proliferation of IoT devices and the growth of cloud computing, there has been a growing interest in edge AI, which involves performing AI tasks at the edge of the network, closer to the source of the data. This can reduce latency and improve real-time performance for applications such as autonomous vehicles and smart homes.\\n\\nThese are some of the key milestones in the history of AI, but there is still much ongoing research and development in the field. As AI continues to evolve, it is likely that new subfields and applications will emerge, further expanding its reach and impact on society.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"Tell me about the history of AI\")\n",
    "]\n",
    "chat_model(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "We can use Olama with RAG, [just as shown here](https://python.langchain.com/docs/use_cases/question_answering/how_to/local_retrieval_qa).\n",
    "\n",
    "Let's use the 13b model:\n",
    "\n",
    "```\n",
    "ollama pull llama2:13b\n",
    "ollama run llama2:13b \n",
    "```\n",
    "\n",
    "Let's also use local embeddings from `GPT4AllEmbeddings` and `Chroma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gpt4all chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"[INST] <<SYS>> Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum and keep the answer as concise as possible. <</SYS>>\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:[/INST]\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat model\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "chat_model = ChatOllama(base_url=\"http://localhost:11434\",\n",
    "             model=\"llama2:7b-chat\",\n",
    "             verbose=True,\n",
    "             callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA chain\n",
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    chat_model,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are three main approaches to task decomposition for AI agents: (1) using simple prompts like \"Steps for XYZ.\" and \"What are the subgoals for achieving XYZ?\", (2) using task-specific instructions, and (3) incorporating human inputs."
     ]
    }
   ],
   "source": [
    "question = \"What are the various approaches to Task Decomposition for AI Agents?\"\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get logging for tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are three approaches to task decomposition: (1) using simple prompts like \"Steps for XYZ.\\n1.\" and \"What are the subgoals for achieving XYZ?\" to guide the LLM, (2) providing task-specific instructions, such as \"Write a story outline.\" for writing a novel, and (3) incorporating human inputs.{'model': 'llama2', 'created_at': '2023-08-23T04:11:15.097541Z', 'done': True, 'context': [1, 29871, 1, 29961, 25580, 29962, 518, 25580, 29962, 518, 25580, 29962, 3532, 14816, 29903, 6778, 4803, 278, 1494, 12785, 310, 3030, 304, 1234, 278, 1139, 472, 278, 1095, 29889, 29871, 13, 3644, 366, 1016, 29915, 29873, 1073, 278, 1234, 29892, 925, 1827, 393, 366, 1016, 29915, 29873, 1073, 29892, 1016, 29915, 29873, 1018, 304, 1207, 701, 385, 1234, 29889, 29871, 13, 11403, 2211, 25260, 7472, 322, 3013, 278, 1234, 408, 3022, 895, 408, 1950, 29889, 529, 829, 14816, 29903, 6778, 13, 5398, 26227, 508, 367, 2309, 313, 29896, 29897, 491, 365, 26369, 411, 2560, 9508, 292, 763, 376, 7789, 567, 363, 1060, 29979, 29999, 7790, 29876, 29896, 19602, 376, 5618, 526, 278, 1014, 1484, 1338, 363, 3657, 15387, 1060, 29979, 29999, 29973, 613, 313, 29906, 29897, 491, 773, 3414, 29899, 14940, 11994, 29936, 321, 29889, 29887, 29889, 376, 6113, 263, 5828, 27887, 1213, 363, 5007, 263, 9554, 29892, 470, 313, 29941, 29897, 411, 5199, 10970, 29889, 13, 13, 5398, 26227, 508, 367, 2309, 313, 29896, 29897, 491, 365, 26369, 411, 2560, 9508, 292, 763, 376, 7789, 567, 363, 1060, 29979, 29999, 7790, 29876, 29896, 19602, 376, 5618, 526, 278, 1014, 1484, 1338, 363, 3657, 15387, 1060, 29979, 29999, 29973, 613, 313, 29906, 29897, 491, 773, 3414, 29899, 14940, 11994, 29936, 321, 29889, 29887, 29889, 376, 6113, 263, 5828, 27887, 1213, 363, 5007, 263, 9554, 29892, 470, 313, 29941, 29897, 411, 5199, 10970, 29889, 13, 13, 1451, 16047, 267, 297, 1472, 29899, 8489, 18987, 322, 3414, 26227, 29901, 1858, 9450, 975, 263, 3309, 29891, 4955, 322, 17583, 3902, 8253, 278, 1650, 2913, 3933, 18066, 292, 29889, 365, 26369, 29879, 21117, 304, 10365, 13900, 746, 20050, 411, 15668, 4436, 29892, 3907, 963, 3109, 16424, 9401, 304, 25618, 1058, 5110, 515, 14260, 322, 1059, 29889, 13, 13, 1451, 16047, 267, 297, 1472, 29899, 8489, 18987, 322, 3414, 26227, 29901, 1858, 9450, 975, 263, 3309, 29891, 4955, 322, 17583, 3902, 8253, 278, 1650, 2913, 3933, 18066, 292, 29889, 365, 26369, 29879, 21117, 304, 10365, 13900, 746, 20050, 411, 15668, 4436, 29892, 3907, 963, 3109, 16424, 9401, 304, 25618, 1058, 5110, 515, 14260, 322, 1059, 29889, 13, 16492, 29901, 1724, 526, 278, 13501, 304, 9330, 897, 510, 3283, 29973, 13, 29648, 1319, 673, 10834, 29914, 25580, 29962, 518, 29914, 25580, 29962, 518, 29914, 25580, 29962, 29871, 1670, 526, 2211, 13501, 304, 3414, 26227, 29901, 313, 29896, 29897, 773, 2560, 9508, 29879, 763, 376, 7789, 567, 363, 1060, 29979, 29999, 7790, 29876, 29896, 1213, 322, 376, 5618, 526, 278, 1014, 1484, 1338, 363, 3657, 15387, 1060, 29979, 29999, 3026, 304, 10754, 278, 365, 26369, 29892, 313, 29906, 29897, 13138, 3414, 29899, 14940, 11994, 29892, 1316, 408, 376, 6113, 263, 5828, 27887, 1213, 363, 5007, 263, 9554, 29892, 322, 313, 29941, 29897, 11039, 1218, 5199, 10970, 29889, 2], 'total_duration': 5056492792, 'load_duration': 2230375, 'sample_count': 80, 'sample_duration': 58641000, 'prompt_eval_count': 146, 'prompt_eval_duration': 3401658000, 'eval_count': 79, 'eval_duration': 1585813000}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import LLMResult\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "class GenerationStatisticsCallback(BaseCallbackHandler):\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
    "        print(response.generations[0][0].generation_info)\n",
    "        \n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler(), GenerationStatisticsCallback()])\n",
    "\n",
    "chat_model = ChatOllama(base_url=\"http://localhost:11434\",\n",
    "             model=\"llama2\",\n",
    "             verbose=True,\n",
    "             callback_manager=callback_manager)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    chat_model,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eval_count` / (`eval_duration`/10e9)  gets `tok / s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.8167186168861"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "79 / (1585813000/1000/1000/1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
