---
sidebar_position: 1
---

# Chat models

import DocCardList from "@theme/DocCardList";

## Feature (natively supported)
All `LLM`s implement the LCEL `Runnable` interface, meaning they all expose functioning `invoke`, `ainvoke`, `stream`, and `astream` (and `batch`, `abatch`) methods.
*That is, they all have functioning async and streaming generation methods.*
This table highlights specifically those integrations that **natively support** streaming and asynchronous generation (meaning these features are built into the 3rd-party integration).

Model|Async|Stream|Async stream
:-|:-:|:-:|:-:|:-:
AzureChatOpenAI|✅|✅|✅
BedrockChat|❌|✅|❌
ChatAnthropic|✅|✅|✅
ChatAnyscale|✅|✅|✅
ChatGooglePalm|✅|❌|❌
ChatJavelinAIGateway|✅|❌|❌
ChatKonko|❌|❌|❌
ChatLiteLLM|✅|✅|✅
ChatMLflowAIGateway|❌|❌|❌
ChatOllama|❌|✅|❌
ChatOpenAI|✅|✅|✅
ChatVertexAI|❌|✅|❌
ErnieBotChat|❌|❌|❌
JinaChat|✅|✅|✅
MiniMaxChat|✅|❌|❌
PromptLayerChatOpenAI|❌|❌|❌
QianfanChatEndpoint|✅|✅|✅

<DocCardList />
