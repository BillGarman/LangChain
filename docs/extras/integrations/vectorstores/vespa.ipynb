{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0f17b9",
   "metadata": {},
   "source": [
    "# Vespa\n",
    "\n",
    ">[Vespa](https://vespa.ai/) is a fully featured search engine and vector database. It supports vector search (ANN), lexical search, and search in structured data, all in the same query.\n",
    "\n",
    "This notebook shows how to use `Vespa.ai` as a LangChain vector store.\n",
    "\n",
    "In order to create a retriever, we use [pyvespa](https://pyvespa.readthedocs.io/en/latest/index.html) to\n",
    "create a connection a `Vespa` service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a11ab-38bd-4920-ba11-60cb2f075754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pyvespa"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using `pyvespa` package, you can either connect to a\n",
    "[Vespa Cloud instance](https://pyvespa.readthedocs.io/en/latest/deploy-vespa-cloud.html)\n",
    "or a local\n",
    "[Docker instance](https://pyvespa.readthedocs.io/en/latest/deploy-docker.html).\n",
    "Here, we will create a new Vespa application and deploy that using Docker.\n",
    "\n",
    "First, we need to create an application package:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from vespa.package import ApplicationPackage, Field, RankProfile\n",
    "\n",
    "app_package = ApplicationPackage(name=\"testapp\")\n",
    "app_package.schema.add_fields(\n",
    "    Field(name=\"text\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"),\n",
    "    Field(name=\"embedding\", type=\"tensor<float>(x[384])\",\n",
    "          indexing=[\"attribute\", \"summary\"],\n",
    "          attribute=[f\"distance-metric: angular\"]),\n",
    ")\n",
    "app_package.schema.add_rank_profile(\n",
    "    RankProfile(name=\"default\",\n",
    "                first_phase=\"closeness(field, embedding)\",\n",
    "                inputs=[(\"query(query_embedding)\", \"tensor<float>(x[384])\")]\n",
    "                )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This sets up a Vespa application with a schema for each document that contains\n",
    "two fields: `text` for holding the document text and `embedding` for holding\n",
    "the embedding vector. The `text` field is set up to use a BM25 index for\n",
    "efficient text retrieval, and we'll see how to use this and hybrid search a\n",
    "bit later.\n",
    "\n",
    "The `embedding` field is set up with a vector of length 384 to hold the\n",
    "embedding representation of the text. See\n",
    "[Vespa's Tensor Guide](https://docs.vespa.ai/en/tensor-user-guide.html)\n",
    "for more on tensors in Vespa.\n",
    "\n",
    "Lastly, we add a [rank profile](https://docs.vespa.ai/en/ranking.html) to\n",
    "instruct Vespa how to order documents. Here we set this up with a\n",
    "[nearest neighbor search](https://docs.vespa.ai/en/nearest-neighbor-search.html).\n",
    "\n",
    "Now we can deploy this application locally:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10dd962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vespa.deployment import VespaDocker\n",
    "\n",
    "vespa_docker = VespaDocker()\n",
    "vespa_app = vespa_docker.deploy(application_package=app_package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df4ce53",
   "metadata": {},
   "source": [
    "This deploys and creates a connection to a `Vespa` service. In case you\n",
    "already have a Vespa application running for instance in the cloud,\n",
    "please refer to the PyVespa application for how to connect.\n",
    "\n",
    "Now, let's load some documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"../../modules/state_of_the_union.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we also set up local sentence embedder to transform the text to embedding\n",
    "vectors. One could also use OpenAI embeddings, which we'll show a bit later.\n",
    "\n",
    "To feed these to Vespa, we need to configure how the vector store should map to\n",
    "fields in the Vespa application. Then we create the vector store directly from\n",
    "this set of documents:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vespa_config = dict(\n",
    "    page_content_field=\"text\",\n",
    "    embedding_field=\"embedding\",\n",
    "    input_field=\"query_embedding\"\n",
    ")\n",
    "\n",
    "from langchain.vectorstores import VespaStore\n",
    "\n",
    "db = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This creates a Vespa vector store and feeds that set of documents to Vespa.\n",
    "The vector store takes care of calling the embedding function for each document\n",
    "and inserts them into the database.\n",
    "\n",
    "We can now query the vector store:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccca1f4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e34e1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This will embed the query using the embedding function given above and query\n",
    "Vespa. Note that this will use the `default` ranking function, which we set up\n",
    "in the application package to be nearest neighbor ranking.\n",
    "\n",
    "Please refer to the [pyvespa documentation](https://pyvespa.readthedocs.io/en/latest/getting-started-pyvespa.html#Query)\n",
    "for more information.\n",
    "\n",
    "Now you can return the results and continue using the results in LangChain.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}