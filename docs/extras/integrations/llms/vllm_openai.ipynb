{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-12 07:23:15 llm_engine.py:72] Initializing an LLM engine with config: model='facebook/opt-125m', tokenizer='facebook/opt-125m', tokenizer_mode=auto, revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n",
      "WARNING 10-12 07:23:15 config.py:221] Possibly too large swap space. 4.00 GiB out of the 5.81 GiB total CPU memory is allocated for the swap space.\n",
      "Downloading (…)okenizer_config.json: 100%|█████| 685/685 [00:00<00:00, 2.03MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|███| 899k/899k [00:00<00:00, 8.77MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|███| 456k/456k [00:00<00:00, 6.93MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████| 441/441 [00:00<00:00, 675kB/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/vscode/langchain-py-env/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 616, in <module>\n",
      "    engine = AsyncLLMEngine.from_engine_args(engine_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/langchain-py-env/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 486, in from_engine_args\n",
      "    engine = cls(engine_args.worker_use_ray,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/langchain-py-env/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 270, in __init__\n",
      "    self.engine = self._init_engine(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/langchain-py-env/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 306, in _init_engine\n",
      "    return engine_class(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/langchain-py-env/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 108, in __init__\n",
      "    self._init_workers(distributed_init_method)\n",
      "  File \"/home/vscode/langchain-py-env/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 140, in _init_workers\n",
      "    self._run_workers(\n",
      "  File \"/home/vscode/langchain-py-env/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 692, in _run_workers\n",
      "    output = executor(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/langchain-py-env/lib/python3.11/site-packages/vllm/worker/worker.py\", line 60, in init_model\n",
      "    torch.cuda.set_device(self.device)\n",
      "  File \"/home/vscode/langchain-py-env/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 350, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "  File \"/home/vscode/langchain-py-env/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 247, in _lazy_init\n",
      "    torch._C._cuda_init()\n",
      "RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n"
     ]
    }
   ],
   "source": [
    "# e.g. cited from https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html#openai-compatible-server\n",
    "\n",
    "# start server which hosts model opt-125m\n",
    "! python -m vllm.entrypoints.openai.api_server --model facebook/opt-125m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue e.g. above\n",
    "# query server via openAI api\n",
    "\n",
    "import openai\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai.api_key = \"EMPTY\"\n",
    "openai.api_base = \"http://localhost:8000/v1\"\n",
    "completion = openai.Completion.create(model=\"facebook/opt-125m\",\n",
    "                                      prompt=\"San Francisco is a\")\n",
    "print(\"Completion result:\", completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, query server via langchain\n",
    "from langchain.llms import VLLMOpenAI\n",
    "\n",
    "base_url = \"http://localhost:8000/v1\"\n",
    "\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_base=base_url,\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    model_name=\"tiiuae/falcon-7b\",\n",
    "    temperature=0\n",
    ")\n",
    "llm(\"Hello world!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
