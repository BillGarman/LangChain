{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPF4vhdZyJ7S"
   },
   "source": [
    "# KoboldAI API\n",
    "\n",
    "[KoboldAI](https://github.com/KoboldAI/KoboldAI-Client) is a \"a browser-based front-end for AI-assisted writing with multiple local & remote AI models...\". It has a public and local API that is able to be used in langchain.\n",
    "\n",
    "This example goes over how to use LangChain with that API.\n",
    "\n",
    "Documentation on parameters can be found [here](https://api.python.langchain.com/en/latest/llms/langchain.llms.koboldai.KoboldApiLLM.html)\n",
    "\n",
    "Windows: run 'remoteplay.bat' for Public\n",
    "\n",
    "windows or linux users: play.sh --remote for a public url or ./play.sh --host XXX.XXX.XXX.0/24 --port 5000  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lyzOsRRTf_Vr"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import KoboldApiLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a_H7mvfy51O"
   },
   "source": [
    "Replace the endpoint seen below with the one shown in the output after starting it with the correct arguments. \n",
    "\n",
    "Optionally, you can pass in parameters like temperature or max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "g3vGebq8f_Vr"
   },
   "outputs": [],
   "source": [
    "llm = KoboldApiLLM(endpoint=\"http://127.0.0.1:5000\", max_length=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Kobold API LLM Wraper \n",
    "In the next two cells, a DuckDuckGo search is performed based on the question to gather information, then the language model is utilized to generate a response based on this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When was llamav2, from meta, released?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "sPxNGGiDf_Vr",
    "outputId": "024a1d62-3cd7-49a8-c6a8-5278224d02ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:tool:duckduckgo_search] Entering Tool run with input:\n",
      "\u001b[0m\"When was llamav2, from meta, released?\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:tool:duckduckgo_search] [1.25s] Exiting Tool run with output:\n",
      "\u001b[0m\"First published on Wed 19 Jul 2023 11.11 EDT Mark Zuckerberg's Meta has this week released an open-source version of an artificial intelligence model, Llama 2, for public use. The large... Takeaways Today, we're introducing the availability of Llama 2, the next generation of our open source large language model. Llama 2 is free for research and commercial use. Microsoft and Meta are expanding their longstanding partnership, with Microsoft as the preferred partner for Llama 2. In February, Meta released the precursor of Llama 2, LLaMA, as open source with a non-commercial license. Officially only available to academics with certain credentials, someone soon leaked... Part of our collaboration with Meta led to combining Meta's safety techniques with Azure AI Content Safety so that by default, the deployments of the Llama 2 models in Azure AI come with a layered safety approach. Meta, better known to most of us as Facebook, has released a commercial version of Llama-v2, its open-source large language model (LLM) that uses artificial intelligence (AI) to generate text ...\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"When was llamav2, from meta, released?\",\n",
      "  \"search_result\": \"First published on Wed 19 Jul 2023 11.11 EDT Mark Zuckerberg's Meta has this week released an open-source version of an artificial intelligence model, Llama 2, for public use. The large... Takeaways Today, we're introducing the availability of Llama 2, the next generation of our open source large language model. Llama 2 is free for research and commercial use. Microsoft and Meta are expanding their longstanding partnership, with Microsoft as the preferred partner for Llama 2. In February, Meta released the precursor of Llama 2, LLaMA, as open source with a non-commercial license. Officially only available to academics with certain credentials, someone soon leaked... Part of our collaboration with Meta led to combining Meta's safety techniques with Azure AI Content Safety so that by default, the deployments of the Llama 2 models in Azure AI come with a layered safety approach. Meta, better known to most of us as Facebook, has released a commercial version of Llama-v2, its open-source large language model (LLM) that uses artificial intelligence (AI) to generate text ...\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:KoboldApiLLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Question: When was llamav2, from meta, released?\\n\\nInput: First published on Wed 19 Jul 2023 11.11 EDT Mark Zuckerberg's Meta has this week released an open-source version of an artificial intelligence model, Llama 2, for public use. The large... Takeaways Today, we're introducing the availability of Llama 2, the next generation of our open source large language model. Llama 2 is free for research and commercial use. Microsoft and Meta are expanding their longstanding partnership, with Microsoft as the preferred partner for Llama 2. In February, Meta released the precursor of Llama 2, LLaMA, as open source with a non-commercial license. Officially only available to academics with certain credentials, someone soon leaked... Part of our collaboration with Meta led to combining Meta's safety techniques with Azure AI Content Safety so that by default, the deployments of the Llama 2 models in Azure AI come with a layered safety approach. Meta, better known to most of us as Facebook, has released a commercial version of Llama-v2, its open-source large language model (LLM) that uses artificial intelligence (AI) to generate text ...\\n\\nAnswer: Based on the input above,\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:KoboldApiLLM] [3.22s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Llama 2, from meta, was released on Wed 19 Jul 2023 11.11 EDT.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [3.22s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Llama 2, from meta, was released on Wed 19 Jul 2023 11.11 EDT.\"\n",
      "}\n",
      "\n",
      "\n",
      "Final Answer: Llama 2, from meta, was released on Wed 19 Jul 2023 11.11 EDT.\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "langchain.debug = True\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Input: {search_result}\n",
    "\n",
    "Answer: Based on the input above, \"\"\"\n",
    "\n",
    "search_result = search(question)\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"search_result\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "response = llm_chain.run({'question': question, 'search_result': search_result})\n",
    "\n",
    "print(\"\\n\\nFinal Answer: \" + response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
