{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTranslate2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CTranslate2** is a C++ and Python library for efficient inference with Transformer models.\n",
    "\n",
    "The project implements a custom runtime that applies many performance optimization techniques such as weights quantization, layers fusion, batch reordering, etc., to accelerate and reduce the memory usage of Transformer models on CPU and GPU.\n",
    "\n",
    "Full list of features and supported models/frameworks is included in the [project's repository](https://github.com/OpenNMT/CTranslate2). To start, please check out the official [quickstart guide](https://opennmt.net/CTranslate2/quickstart.html).\n",
    "\n",
    "To use, you should have `ctranslate2` python package installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ctranslate2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in quickstart guide, to use Hugging Face model with CTranslate2, it has to be first converted to CTranslate2 format using the command `ct2-transformers-converter` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ct2-transformers-converter --model meta-llama/Llama-2-7b-hf --quantization int8 --output_dir llama-2-7b-ct2 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTranslate2\n",
    "\n",
    "llm = CTranslate2(\n",
    "    model_path=\"/mnt/ml-team/homes/eryk.mazus/llama-2-7b-ct2\",\n",
    "    tokenizer_name=\"meta-llama/Llama-2-7b-hf\",\n",
    "    device=\"cuda\",\n",
    "    # device_index can be either single int or list or ints,\n",
    "    # indicating the ids of GPUs to use for inference:\n",
    "    device_index=[0,1], \n",
    "    compute_type=\"bfloat16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    llm(\n",
    "        \"A rap battle between Stephen Colbert and John Oliver\",\n",
    "        max_length=256,\n",
    "        sampling_topp=0.95,\n",
    "        sampling_temperature=1.0,\n",
    "        repetition_penalty=2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    llm.generate(\n",
    "        [\"List of European capital cities:\", \"List of capital cities in Asia:\"],\n",
    "        max_length=128\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate the model in an LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"Who was the US president in the year the first Pokemon game was released?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('langchain_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3372ef96e068313d34c91eab0f20d815c93d37110de821968e5d598f73bfb74c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
