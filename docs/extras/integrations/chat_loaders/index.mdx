---
sidebar_position: 0
---

# Chat loaders

Like document loaders, chat loaders are utilities designed to help load conversations from popular communication platforms such as Facebook, Slack, Discord, etc. These are loaded into memory as LangChain chat message objects. Such utilities facilitate tasks such as fine-tuning a language model to match your personal style or voice. 

This brief guide will illustrate the process using [OpenAI's fine-tuning API](https://platform.openai.com/docs/guides/fine-tuning) comprised of six steps:

1. Export your Facebook Messenger chat data in a compatible format for your intended chat loader.
2. Load the chat data into memory as LangChain chat message objects.
    - Optional: You can filter and group messages to mold your desired style or improve the quality of the fine-tuned model.
3. Export these acquired messages in a format expected by the fine-tuning API.
4. Upload this data to OpenAI.
5. Fine-tune your model.
6. Implement the fine-tuned model in LangChain.

This guide is not wholly comprehensive but is designed to take you through the fundamentals and get you started on the process.

We will demonstrate the procedure through an example of fine-tuning a `gpt-3.5-turbo` model to adopt the voice of a user. 

Let's go over each step using an example from Facebook Messenger:

### 1. Export your chat data

To export your Facebook messenger data, you can follow the [instructions here](https://www.zapptales.com/en/download-facebook-messenger-chat-history-how-to/). 

:::important JSON format
You must select JSON format (instead of HTML) to be compatible with the current loader.
:::

OpenAI requires at least 10 examples to fine-tune your model, but they recommend between 50-100 for more optimal results.

### 2. Load the chat

Once you've obtained your chat data, you can load it into memory as LangChain chat message objects. Here’s an example of loading data using the Python code:

```python
from langchain.chat_loaders.facebook_messenger import FolderFacebookMessengerChatLoader

loader = FolderFacebookMessengerChatLoader(
    path="./facebook_messenger_chats",
)

chat_sessions = loader.load()
```

In this snippet, we point the loader to a directory of Facebook chat dumps which are then loaded as multiple "sessions" of messages, one session per conversation file.

Once you've loaded the messages, you should decide which person you want to fine-tune the model to (usually yourself). You can also decide to merge consecutive messages from the same sender into a single chat message.
For both of these tasks, you can use the chat_loaders utilities to do so:

```
from langchain.chat_loaders.utils import (
    merge_chat_runs,
    map_ai_messages,
)

merged_sessions = merge_chat_runs(chat_sessions)
alternating_sessions = list(map_ai_messages(merged_sessions, "My Name"))
```

### 3. Export messages to OpenAI format

Convert the chat messages to dictionaries using the `convert_messages_for_finetuning()` function. Then, group the data into chunks for better context modeling and overlap management.

```python
from langchain.adapters.openai import convert_messages_for_finetuning

openai_messages = convert_messages_for_finetuning(chat_sessions)
```

At this point, the data is ready for upload to OpenAI. You can choose to split up conversations into smaller chunks for training if you
do not have enough conversations to train on. Feel free to play around with different chunk sizes or with adding system messages to the fine-tuning data.

```python
chunk_size = 8
overlap = 2

message_groups = [
    conversation_messages[i: i + chunk_size] 
    for conversation_messages in openai_messages
    for i in range(
        0, len(conversation_messages) - chunk_size + 1, 
        chunk_size - overlap)
]

len(message_groups)
```

### 4. Upload the data to OpenAI

Ensure you have set your OpenAI API key by following these [instructions](https://platform.openai.com/account/api-keys). 

```python
import json
import io
import openai

my_file = io.BytesIO()
for group in message_groups:
    my_file.write((json.dumps({"messages": group}) + "\n").encode('utf-8'))

my_file.seek(0)
training_file = openai.File.create(
  file=my_file,
  purpose='fine-tune'
)
```

The file is uploaded to OpenAI’s servers. An audit is performed to ensure data compliance, so you may have to wait a few minutes for the dataset to be ready for use.

### 5. Fine-tune the model

Start the fine-tuning job with your chosen base model.

```python
job = openai.FineTuningJob.create(
    training_file=training_file.id,
    model="gpt-3.5-turbo",
)
```

This might take a while. Check the status with `openai.FineTuningJob.retrieve(job.id).status` and wait for it to report `succeeded`.

### 6. Use the model in LangChain

You're almost there! Use the fine-tuned model in LangChain.

```python
from langchain import chat_models, prompts

model_name = job.fine_tuned_model
# Example: ft:gpt-3.5-turbo-0613:personal::5mty86jblapsed
model = chat_models.ChatOpenAI(model=model_name)
```

import DocCardList from "@theme/DocCardList";

<DocCardList />