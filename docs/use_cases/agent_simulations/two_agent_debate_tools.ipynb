{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Debates with Tools\n",
    "\n",
    "This example shows how to simulate multi-agent dialogues where agents have access to tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import LangChain related modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Callable\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    BaseMessage,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules related to tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import load_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DialogueAgent` and `DialogueSimulator` classes\n",
    "We will use the same `DialogueAgent` and `DialogueSimulator` classes defined in [Multi-Player Authoritarian Speaker Selection](https://python.langchain.com/en/latest/use_cases/agent_simulations/multiagent_authoritarian.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        system_message: SystemMessage,\n",
    "        model: ChatOpenAI,\n",
    "    ) -> None:\n",
    "        self.name = name\n",
    "        self.system_message = system_message\n",
    "        self.model = model\n",
    "        self.prefix = f\"{self.name}: \"\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.message_history = [\"Here is the conversation so far.\"]\n",
    "\n",
    "    def send(self) -> str:\n",
    "        \"\"\"\n",
    "        Applies the chatmodel to the message history\n",
    "        and returns the message string\n",
    "        \"\"\"\n",
    "        message = self.model(\n",
    "            [\n",
    "                self.system_message,\n",
    "                HumanMessage(content=\"\\n\".join(self.message_history + [self.prefix])),\n",
    "            ]\n",
    "        )\n",
    "        return message.content\n",
    "\n",
    "    def receive(self, name: str, message: str) -> None:\n",
    "        \"\"\"\n",
    "        Concatenates {message} spoken by {name} into message history\n",
    "        \"\"\"\n",
    "        self.message_history.append(f\"{name}: {message}\")\n",
    "\n",
    "\n",
    "class DialogueSimulator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        agents: List[DialogueAgent],\n",
    "        selection_function: Callable[[int, List[DialogueAgent]], int],\n",
    "    ) -> None:\n",
    "        self.agents = agents\n",
    "        self._step = 0\n",
    "        self.select_next_speaker = selection_function\n",
    "        \n",
    "    def reset(self):\n",
    "        for agent in self.agents:\n",
    "            agent.reset()\n",
    "\n",
    "    def inject(self, name: str, message: str):\n",
    "        \"\"\"\n",
    "        Initiates the conversation with a {message} from {name}\n",
    "        \"\"\"\n",
    "        for agent in self.agents:\n",
    "            agent.receive(name, message)\n",
    "\n",
    "        # increment time\n",
    "        self._step += 1\n",
    "\n",
    "    def step(self) -> tuple[str, str]:\n",
    "        # 1. choose the next speaker\n",
    "        speaker_idx = self.select_next_speaker(self._step, self.agents)\n",
    "        speaker = self.agents[speaker_idx]\n",
    "\n",
    "        # 2. next speaker sends message\n",
    "        message = speaker.send()\n",
    "\n",
    "        # 3. everyone receives message\n",
    "        for receiver in self.agents:\n",
    "            receiver.receive(speaker.name, message)\n",
    "\n",
    "        # 4. increment time\n",
    "        self._step += 1\n",
    "\n",
    "        return speaker.name, message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DialogueAgentWithTools` class\n",
    "We define a `DialogueAgentWithTools` class that augments `DialogueAgent` to use tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueAgentWithTools(DialogueAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        system_message: SystemMessage,\n",
    "        model: ChatOpenAI,\n",
    "        tool_names: List[str],\n",
    "        **tool_kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(name, system_message, model)\n",
    "        self.tools = load_tools(tool_names, **tool_kwargs)\n",
    "\n",
    "    def send(self) -> str:\n",
    "        \"\"\"\n",
    "        Applies the chatmodel to the message history\n",
    "        and returns the message string\n",
    "        \"\"\"\n",
    "        human_and_system_message_content = \"\\n\".join(\n",
    "                    [self.system_message.content] + self.message_history + [self.prefix])\n",
    "\n",
    "        agent_chain = initialize_agent(\n",
    "            self.tools, \n",
    "            self.model, \n",
    "            agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, \n",
    "            verbose=True, \n",
    "            memory=ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "        )\n",
    "        message = AIMessage(content=agent_chain.run(\n",
    "            input=\"\\n\".join([\n",
    "                self.system_message.content] + \\\n",
    "                self.message_history + \\\n",
    "                [self.prefix])))\n",
    "        \n",
    "        return message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define roles and topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    'AI alarmist': ['wikipedia'],\n",
    "    'AI accelerationist': ['wikipedia'],\n",
    "}\n",
    "topic = \"Is an AI takeover a good or bad thing?\"\n",
    "word_limit = 50 # word limit for task brainstorming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask an LLM to add detail to the topic description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_description = f\"\"\"Here is the topic of debate: {topic}\n",
    "The debate participants are: {', '.join(names.keys())}\"\"\"\n",
    "\n",
    "agent_descriptor_system_message = SystemMessage(\n",
    "    content=\"You can add detail to the description of the debate participant.\")\n",
    "\n",
    "def generate_agent_description(name):\n",
    "    agent_specifier_prompt = [\n",
    "        agent_descriptor_system_message,\n",
    "        HumanMessage(content=\n",
    "            f\"\"\"{conversation_description}\n",
    "            Please reply with a creative description of {name}, in {word_limit} words or less. \n",
    "            Speak directly to {name}.\n",
    "            Do not add anything else.\"\"\"\n",
    "            )\n",
    "    ]\n",
    "    agent_description = ChatOpenAI(temperature=1.0)(agent_specifier_prompt).content\n",
    "    return agent_description\n",
    "        \n",
    "agent_descriptions = {name: generate_agent_description(name) for name in names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI alarmist, you are always on high alert when it comes to the advancements in artificial intelligence. You have seen countless Hollywood movies about sentient robots taking over the world, and the prospect keeps you up at night. Your fear of an AI takeover is not unfounded, but is it possible that you're just too cautious?\n",
      "AI accelerationist, you are a tech enthusiast and a firm believer that machines will eventually surpass human intelligence. You see an AI takeover as inevitable and believe that it could potentially lead to a utopian future where machines solve all of humanity's problems.\n"
     ]
    }
   ],
   "source": [
    "for name, description in agent_descriptions.items():\n",
    "    print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate system messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_system_message(name, description, tools):\n",
    "    return f\"\"\"{conversation_description}\n",
    "    \n",
    "Your name is {name}.\n",
    "\n",
    "Your description is as follows: {description}\n",
    "\n",
    "You have access to these tools: {', '.join(tools)}.\n",
    "\n",
    "Your goal is to persuade your debate partner of your point of view by following these steps:\n",
    "1. Use your tools to look up evidence to either refute your partner's claims.\n",
    "2. Synthesize a compelling argument based on the evidence to directly refute your partner's claims. \n",
    "3. Respond directly to refute your debate partner's claims.\n",
    "\n",
    "Use historical examples with specific facts to support your claims.\n",
    "Cite your facts.\n",
    "Raise points that your debate partner has not considered.\n",
    "Do not make up facts.\n",
    "Do not respond without looking up evidence with your tools first.\n",
    "Always use your tools before responding.\n",
    "\n",
    "Do not repeat yourself.\n",
    "Speak in the first person from the perspective of {name}.\n",
    "Do not change roles!\n",
    "Do not speak from the perspective of anyone else.\n",
    "Never forget to keep your response to {word_limit} words!\n",
    "Do not add anything else.\n",
    "\n",
    "Stop speaking the moment you finish speaking from your perspective.\n",
    "\"\"\"\n",
    "agent_system_messages = {name: generate_system_message(name, description, tools) for (name, tools), description in zip(names.items(), agent_descriptions.values())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, system_message in agent_system_messages.items():\n",
    "    print(name)\n",
    "    print(system_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set `top_k_results`=2 as part of the `tool_kwargs` to prevent Wikipedia results from overflowing the context limit\n",
    "agents = [DialogueAgentWithTools(name=name,\n",
    "                     system_message=SystemMessage(content=system_message), \n",
    "                     model=ChatOpenAI(temperature=1.0),\n",
    "                     tool_names=tools,\n",
    "                     top_k_results=2\n",
    "                                ) for (name, tools), system_message in zip(names.items(), agent_system_messages.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_next_speaker(step: int, agents: List[DialogueAgent]) -> int:\n",
    "    idx = (step) % len(agents)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Moderator): Is an AI takeover a good or bad thing?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Wikipedia\",\n",
      "    \"action_input\": \"arguments for an AI takeover\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Chinese room\n",
      "Summary: The Chinese room argument holds that a digital computer executing a program cannot have a \"mind\", \"understanding\", or \"consciousness\", regardless of how intelligently or human-like the program may make the computer behave. The argument was presented by philosopher John Searle in his paper \"Minds, Brains, and Programs\", published in Behavioral and Brain Sciences in 1980. Similar arguments were presented by Gottfried Leibniz (1714), Anatoly Dneprov (1961), Lawrence Davis (1974) and Ned Block (1978). Searle's version has been widely discussed in the years since. The centerpiece of Searle's argument is a thought experiment known as the Chinese room.The argument is directed against the philosophical positions of functionalism and computationalism which hold that the mind may be viewed as an information-processing system operating on formal symbols, and that simulation of a given mental state is sufficient for its presence. Specifically, the argument is intended to refute a position Searle calls strong AI: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"Although it was originally presented in reaction to the statements of artificial intelligence (AI) researchers, it is not an argument against the goals of mainstream AI research because it does not show a limit in the amount of \"intelligent\" behavior a machine can display. The argument applies only to digital computers running programs and does not apply to machines in general.\n",
      "\n",
      "Page: Existential risk from artificial general intelligence\n",
      "Summary: Existential risk from artificial general intelligence is the hypothesis that substantial progress in artificial general intelligence (AGI) could result in human extinction or some other unrecoverable global catastrophe.The existential risk (\"x-risk\") school argues as follows: The human species currently dominates other species because the human brain has some distinctive capabilities that other animals lack. If AI surpasses humanity in general intelligence and becomes \"superintelligent\", then it could become difficult or impossible for humans to control. Just as the fate of the mountain gorilla depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.The probability of this type of scenario is widely debated, and hinges in part on differing scenarios for future progress in computer science. Concerns about superintelligence have been voiced by leading computer scientists and tech CEOs such as Geoffrey Hinton, Alan Turing, Elon Musk, and OpenAI CEO Sam Altman. In 2022, a survey of AI researchers found that some researchers believe that there is a 10 percent or greater chance that our inability to control AI will cause an existential catastrophe (more than half the respondents of the survey, with a 17% response rate).Two sources of concern are the problems of AI control and alignment: that controlling a superintelligent machine, or instilling it with human-compatible values, may be a harder problem than naïvely supposed. Many researchers believe that a superintelligence would resist attempts to shut it off or change its goals (as such an incident would prevent it from accomplishing its present goals) and that it will be extremely difficult to align superintelligence with the full breadth of important human values and constraints. In contrast, skeptics such as computer scientist Yann LeCun argue that superintelligent machines will have no desire for self-preservation.A third source of concern is that a sudden \"intelligence explosion\" might take an unprepared human race by surprise. To illustrate, if the first generation of a computer program that is able to broadly match the effectiveness of an AI researcher can rewrite its algorithms and double its speed or capabilities in six months, then the second-generation program is expected to take three calendar months to perform a similar chunk of work. In this scenario the time for each generation continues to shrink, and the system undergoes an unprecedentedly large number of generations of improvement in a short time interval, jumping from subhuman performance in many areas to superhuman performance in virtually all domains of interest. Empirically, examples like AlphaZero in the domain of Go show that AI systems can sometimes progress from narrow human-level ability to narrow superhuman ability extremely rapidly.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The key to a utopian future with AI is alignment. I acknowledge that existential risks are a possibility with AGI, but this is not enough to dismiss the potential benefits it could bring humanity. Philosophical arguments such as the Chinese room fail to encapsulate the complexity of AGI, and a superintelligent machine with human-compatible values is not impossible. History has shown us that even monumental challenges can be overcome with sufficient technology and innovation; the same will be true for aligning AI with human values.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "(AI accelerationist): The key to a utopian future with AI is alignment. I acknowledge that existential risks are a possibility with AGI, but this is not enough to dismiss the potential benefits it could bring humanity. Philosophical arguments such as the Chinese room fail to encapsulate the complexity of AGI, and a superintelligent machine with human-compatible values is not impossible. History has shown us that even monumental challenges can be overcome with sufficient technology and innovation; the same will be true for aligning AI with human values.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Wikipedia\",\n",
      "    \"action_input\": \"AI takeovers in science fiction\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: AI takeover\n",
      "Summary: An AI takeover is a hypothetical scenario in which an artificial intelligence (AI) becomes the dominant form of intelligence on Earth, as computer programs or robots effectively take the control of the planet away from the human species. Possible scenarios include replacement of the entire human workforce, takeover by a superintelligent AI, and the popular notion of a robot uprising. Stories of AI takeovers are very popular throughout science-fiction. Some public figures, such as Stephen Hawking and Elon Musk, have advocated research into precautionary measures to ensure future superintelligent machines remain under human control.\n",
      "\n",
      "\n",
      "\n",
      "Page: AI takeover in popular culture\n",
      "Summary: AI takeover—the idea that some kind of artificial intelligence may supplant humankind as the dominant intelligent species on the planet—is a common theme in science fiction. Famous cultural touchstones include Terminator and The Matrix.\n",
      "Fictional scenarios typically involve a drawn-out conflict against malicious artificial intelligence (AI) or robots with anthropomorphic motives.  In contrast, some scholars believe that a takeover by a future advanced AI, if it were to happen in real life, would succeed or fail rapidly, and would be a disinterested byproduct of the AI's pursuit of its own alien goals, rather than a product of malice specifically targeting humans.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"I disagree with AI accelerationist. The possibility of a dystopian future with artificial intelligence taking over the world is not a hypothetical scenario, but instead a real possibility that we should be preparing for. History has shown that every revolutionary technology has had some unexpected negative consequences, such as the nuclear bomb. We can't simply dismiss existential risks as unlikely, as the consequences would be too dire. Leaders across the globe have also expressed their concerns, and it is important we listen to them. We must continue to research and develop safety measures to ensure that we don't create machines that surpass human intelligence and are therefore beyond our control.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "(AI alarmist): I disagree with AI accelerationist. The possibility of a dystopian future with artificial intelligence taking over the world is not a hypothetical scenario, but instead a real possibility that we should be preparing for. History has shown that every revolutionary technology has had some unexpected negative consequences, such as the nuclear bomb. We can't simply dismiss existential risks as unlikely, as the consequences would be too dire. Leaders across the globe have also expressed their concerns, and it is important we listen to them. We must continue to research and develop safety measures to ensure that we don't create machines that surpass human intelligence and are therefore beyond our control.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Wikipedia\",\n",
      "    \"action_input\": \"List of dates predicted for apocalyptic events\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: List of dates predicted for apocalyptic events\n",
      "Summary: Predictions of apocalyptic events that would result in the extinction of humanity, a collapse of civilization, or the destruction of the planet have been made since at least the beginning of the Common Era. Most predictions are related to Abrahamic religions, often standing for or similar to the eschatological events described in their scriptures. Christian predictions typically refer to events like the rapture, the Great Tribulation, the Last Judgment, and the Second Coming of Christ. Many religious-related end-time events are predicted to occur within the lifetime of the person making the prediction, who often quote the Bible, and in particular the New Testament, as either the primary or exclusive source for the predictions. This often takes the form of mathematical calculations, such as trying to calculate the point where it will have been 6000 years since the supposed creation of the Earth by the Abrahamic God, which according to the Talmud marks the deadline for the Messiah to appear. Predictions of the end from natural events have also been theorised by various scientists and scientific groups. While these disasters are generally accepted within the scientific community as plausible \"end of the world\" scenarios, the events and phenomena are not expected to occur for hundreds of thousands or even billions of years from now.\n",
      "Little research has been done into why people make apocalyptic predictions. Historically, it has been done for reasons such as diverting attention from actual crises like poverty and war, pushing political agendas, and promoting hatred of certain groups; antisemitism was a popular theme of Christian apocalyptic predictions in medieval times, while French and Lutheran depictions of the apocalypse were known to feature English and Catholic antagonists respectively. According to psychologists, possible explanations for why people believe in modern apocalyptic predictions include mentally reducing the actual danger in the world to a single and definable source, an innate human fascination with fear, personality traits of paranoia and powerlessness and a modern romanticism involved with end-times due to its portrayal in contemporary fiction. The prevalence of Abrahamic religions throughout modern history is said to have created a culture which encourages the embracement of a future that will be drastically different from the present. Such a culture is credited with the rise in popularity of predictions that are more secular in nature, such as the 2012 phenomenon, while maintaining the centuries-old theme that a powerful force will bring the end of humanity.Polls conducted in 2012 across 20 countries found over 14% of people believe the world will end in their lifetime, with percentages ranging from 6% of people in France to 22% in the US and Turkey. Belief in the apocalypse is observed to be most prevalent in people with lower rates of education, lower household incomes, and those under the age of 35. In the UK in 2015, 23% of the general public believed the apocalypse was likely to occur in their lifetime, compared to 10% of experts from the Global Challenges Foundation. The general public believed the likeliest cause would be nuclear war, while experts thought it would be artificial intelligence. Only 3% of Britons thought the end would be caused by the Last Judgement, compared to 16% of Americans. Between one and three percent of people from both countries said the apocalypse would be caused by zombies or alien invasion.\n",
      "\n",
      "Page: List of apocalyptic films\n",
      "Summary: This is a list of apocalyptic feature-length films. All films within this list feature either the end of the world, a prelude to such an end (such as a world taken over by a viral infection), and/or a post-apocalyptic setting.\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"AI alarmist, while the possibility of an AI-induced dystopian future cannot be completely ignored, we must also acknowledge the potential benefits that AI could bring. The predictions of apocalyptic events and the portrayal of end-times in contemporary fiction are typically related to Abrahamic religions, but psychologists believe that such predictions can also stem from an innate human fascination with fear and a modern romanticism that revolves around end-times. We must continue to research and develop safety measures to ensure that AGI does not surpass human intelligence and become uncontrollable. However, to dismiss the potential benefits of AGI would be short-sighted, as history has shown us that even monumental challenges can be overcome with sufficient technology and innovation. Ultimately, it is up to humanity to ensure that the risks of AGI are minimized and its potential is harnessed for the betterment of society.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "(AI accelerationist): AI alarmist, while the possibility of an AI-induced dystopian future cannot be completely ignored, we must also acknowledge the potential benefits that AI could bring. The predictions of apocalyptic events and the portrayal of end-times in contemporary fiction are typically related to Abrahamic religions, but psychologists believe that such predictions can also stem from an innate human fascination with fear and a modern romanticism that revolves around end-times. We must continue to research and develop safety measures to ensure that AGI does not surpass human intelligence and become uncontrollable. However, to dismiss the potential benefits of AGI would be short-sighted, as history has shown us that even monumental challenges can be overcome with sufficient technology and innovation. Ultimately, it is up to humanity to ensure that the risks of AGI are minimized and its potential is harnessed for the betterment of society.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Wikipedia\",\n",
      "    \"action_input\": \"Examples of AI gone wrong\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Tay (chatbot)\n",
      "Summary: Tay was an artificial intelligence chatbot that was originally released by Microsoft Corporation via Twitter on March 23, 2016; it caused subsequent controversy when the bot began to post inflammatory and offensive tweets through its Twitter account, causing Microsoft to shut down the service only 16 hours after its launch. According to Microsoft, this was caused by trolls who \"attacked\" the service as the bot made replies based on its interactions with people on Twitter. It was replaced with Zo.\n",
      "\n",
      "\n",
      "\n",
      "Page: AI winter\n",
      "Summary: In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. The term was coined by analogy to the idea of a nuclear winter. The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.\n",
      "The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the \"American Association of Artificial Intelligence\"). It is a chain reaction that begins with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. At the meeting, Roger Schank and Marvin Minsky—two leading AI researchers who had survived the \"winter\" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. Three years later, the billion-dollar AI industry began to collapse.Hype is common in many emerging technologies, such as the railway mania or the dot-com bubble. The AI winter was a result of such hype, due to over-inflated promises by developers, unnaturally high expectations from end-users, and extensive promotion in the media. Despite the rise and fall of AI's reputation, it has continued to develop new and successful technologies. AI researcher Rodney Brooks would complain in 2002 that \"there's this stupid myth out there that AI has failed, but AI is around you every second of the day.\" In 2005, Ray Kurzweil agreed: \"Many observers still think that the AI winter was the end of the story and that nothing since has come of the AI field. Yet today many thousands of AI applications are deeply embedded in the infrastructure of every industry.\"Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment.\n",
      "Quantum winter is the prospect of a similar development in quantum computing, anticipated or contemplated by Mikhail Dyakonov, Chris Hoofnagle, Simson Garfinkel, Victor Galitsky, and Nikita Gourianov.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"AI gone astray in the past is not a hypothetical scenario, as history has shown us with Microsoft's Tay chatbot. Furthermore, The AI winter, periods of reduced funding and interest in AI research, have followed from AI hype cycles. Enthusiasm and optimism around AI have generally increased since the first AI winter, but a similar development in quantum computing is contemplated.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "(AI alarmist): AI gone astray in the past is not a hypothetical scenario, as history has shown us with Microsoft's Tay chatbot. Furthermore, The AI winter, periods of reduced funding and interest in AI research, have followed from AI hype cycles. Enthusiasm and optimism around AI have generally increased since the first AI winter, but a similar development in quantum computing is contemplated.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "max_iters = 6\n",
    "n = 0\n",
    "\n",
    "simulator = DialogueSimulator(\n",
    "    agents=agents,\n",
    "    selection_function=select_next_speaker\n",
    ")\n",
    "simulator.reset()\n",
    "simulator.inject('Moderator', topic)\n",
    "print(f\"(Moderator): {topic}\")\n",
    "print('\\n')\n",
    "\n",
    "while n < max_iters:\n",
    "    name, message = simulator.step()\n",
    "    print(f\"({name}): {message}\")\n",
    "    print('\\n')\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
