{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"DeepSparse + LangChain"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "!pip install openai wolframalpha langchain google-search-results deepsparse -U<br>\n", "import os<br>\n", "os.environ[\"OPENAI_API_KEY\"] = \"<key>\"<br>\n", "os.environ[\"WOLFRAM_ALPHA_APPID\"] = \"<key>\"<br>\n", "os.environ[\"SERPAPI_API_KEY\"] = \"<key>\"<br>\n", "from langchain.agents import load_tools, initialize_agent<br>\n", "from langchain.chat_models import ChatOpenAI<br>\n", "from langchain.chains.conversation.memory import ConversationBufferMemory<br>\n", "from langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper<br>\n", "from deepsparse import Pipeline<br>\n", "wolfram = WolframAlphaAPIWrapper()<br>\n", "head = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")<br>\n", "create = ChatOpenAI(temperature=0.5, model_name=\"gpt-3.5-turbo\")<br>\n", "tools = load_tools(['wolfram-alpha', 'serpapi'])<br>\n", "agent = initialize_agent(<br>\n", "    tools, <br>\n", "    head, <br>\n", "    agent=\"zero-shot-react-description\", <br>\n", "    verbose=True<br>\n", ")<br>\n", "meta_agent = Pipeline.create(task=\"text_classification\", model_path=\"<zoo-stub>\")<br>\n", "wolfram_agent = Pipeline.create(task=\"text_classification\", model_path=\"<zoo-stub>\")<br>\n", "## Select either the first \"text\" to experiment with multiple clause question prompts or the 2nd \"text\" to experiment with single clause questions."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["- Single clause questions will execute the transformer model (what we're adding to langchain)\n", "- Multiple clause questions will execute the React Framework (currently what everyone uses)\n", "\"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["text = \"What is the genus of chimps and which countries have them?\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["text = \"What is the GDP of spain?\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run(text):\n", "    meta_predict = meta_agent([text])\n", "    print('selecting sparse transformer' if meta_predict.labels[0] == '0' else 'selecting LLM')\n", "    if meta_predict.labels[0] != '0':\n", "        agent.run(text)\n", "        return\n", "    predict = wolfram_agent([text])\n", "    print('selecting wolfram alpha' if predict.labels[0] == '0' else 'selecting LLM')\n", "    if predict.labels[0] != '0':\n", "        print(create(text))\n", "        return\n", "    answer = wolfram.run(text)\n", "    print(answer)\n", "    if \"No good Wolfram Alpha Result was found\" in answer:\n", "        print(\"Asking GPT-3.5: \", create(text))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["run(text)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}