{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying LLMs in Production\n",
    "\n",
    "Ray Serve is a scalable model serving library for building online inference APIs. Serve is particularly well suited for system composition, enabling you to build a complex inference service consisting of multiple chains and business logic all in Python code. This notebook shows examples of how to deploy a simple openai chain into production. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install ray with `pip install ray[serve]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "from starlette.requests import Request"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general skeleton for deploying a service is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats collection is enabled by default for nightly wheels. To disable this, run the following command: `ray disable-usage-stats` before starting Ray. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 07:30:14,524\tINFO worker.py:1607 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=23810)\u001b[0m INFO 2023-05-03 07:30:16,437 controller 23810 deployment_state.py:1168 - Deploying new version of deployment default_LLMServe.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=23811)\u001b[0m INFO:     Started server process [23811]\n",
      "\u001b[2m\u001b[36m(ServeController pid=23810)\u001b[0m INFO 2023-05-03 07:30:16,506 controller 23810 deployment_state.py:1386 - Adding 1 replica to deployment default_LLMServe.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayServeSyncHandle(deployment='default_LLMServe')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import serve\n",
    "\n",
    "# Deployment resources\n",
    "deployment_resources = {}\n",
    "\n",
    "@serve.deployment(**deployment_resources)\n",
    "class LLMServe:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        # All the initialization code goes here\n",
    "        pass\n",
    "\n",
    "    async def __call__(self, request: Request) -> str:\n",
    "        # You can parse the request here\n",
    "        # and return a response\n",
    "        return \"Hello World\"\n",
    "\n",
    "# Bind the model to the deployment\n",
    "deployment = LLMServe.bind()\n",
    "\n",
    "# Deployment options\n",
    "deployment_options = {}\n",
    "\n",
    "# Run the deployment\n",
    "serve.api.run(deployment, **deployment_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=23810)\u001b[0m INFO 2023-05-03 07:30:17,482 controller 23810 deployment_state.py:1151 - Deleting deployment default_LLMServe.\n",
      "\u001b[2m\u001b[36m(ServeController pid=23810)\u001b[0m INFO 2023-05-03 07:30:17,533 controller 23810 deployment_state.py:1412 - Removing 1 replica from deployment 'default_LLMServe'.\n"
     ]
    }
   ],
   "source": [
    "# Shutdown the deployment\n",
    "serve.api.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an OpenAI API key from [here](https://platform.openai.com/account/api-keys). By running the following code, you will be asked to provide your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "OPENAI_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class DeployLLM:\n",
    "\n",
    "    def __init__(self):\n",
    "        llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "        template = \"Question: {question}\\n\\nAnswer: Let's think step by step.\"\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "        self.chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    def _run_chain(self, text: str):\n",
    "        return self.chain(text)\n",
    "\n",
    "    async def __call__(self, request: Request):\n",
    "        text = request.query_params[\"text\"]\n",
    "        resp = self._run_chain(text)\n",
    "        return resp[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = DeployLLM.bind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=23833)\u001b[0m INFO 2023-05-03 07:30:25,693 controller 23833 deployment_state.py:1168 - Deploying new version of deployment default_DeployLLM.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=23838)\u001b[0m INFO:     Started server process [23838]\n",
      "\u001b[2m\u001b[36m(ServeController pid=23833)\u001b[0m INFO 2023-05-03 07:30:25,782 controller 23833 deployment_state.py:1386 - Adding 1 replica to deployment default_DeployLLM.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayServeSyncHandle(deployment='default_DeployLLM')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example port number\n",
    "PORT_NUMBER = 8282\n",
    "# Run the deployment\n",
    "serve.api.run(deployment, port=PORT_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Justin Bieber was born in 1994, so the NFL team that won the Super Bowl that year was the Dallas Cowboys.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:default_DeployLLM pid=23839)\u001b[0m INFO 2023-05-03 07:30:31,339 default_DeployLLM default_DeployLLM#lwdEnb mhenxcssHV / replica.py:527 - __CALL__ OK 3591.8ms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "text = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "response = requests.post(f'http://localhost:{PORT_NUMBER}/?text={text}')\n",
    "print(response.content.decode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
