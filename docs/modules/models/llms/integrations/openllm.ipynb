{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "026cc336",
   "metadata": {},
   "source": [
    "# OpenLLM\n",
    "\n",
    "[ðŸ¦¾ OpenLLM](https://github.com/bentoml/OpenLLM) is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ddca1",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install `openllm` through [PyPI](https://pypi.org/project/openllm/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6601c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90174fe3",
   "metadata": {},
   "source": [
    "## Launch OpenLLM server locally\n",
    "\n",
    "To start an LLM server, use `openllm start` command. For example, to start a dolly-v2 server, running the following command from a terminal:\n",
    "\n",
    "```bash\n",
    "openllm start dolly-v2\n",
    "```\n",
    "\n",
    "\n",
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b6bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenLLM\n",
    "\n",
    "server_url = \"http://localhost:3000\" # Replace with remote host if you are running on a remote server \n",
    "llm = OpenLLM(server_url=server_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f830f9d",
   "metadata": {},
   "source": [
    "### Optional: Local LLM Inference\n",
    "\n",
    "You may also choose to initialize an LLM managed by OpenLLM locally from current process. This is useful for development purpose and allows developers to quickly try out different types of LLMs.\n",
    "\n",
    "When moving LLM applications to production, we recommend deploying the OpenLLM server separately and access via the `server_url` option demonstrated above.\n",
    "\n",
    "To load an LLM locally via the LangChain wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c392b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aarnphm/.pyenv/versions/3.8.16/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenLLM\n",
    "\n",
    "llm = OpenLLM(\n",
    "    model_name=\"dolly-v2\",\n",
    "    model_id=\"databricks/dolly-v2-3b\",\n",
    "    temperature=0.94,\n",
    "    repetition_penalty=1.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ebe0d",
   "metadata": {},
   "source": [
    "### Integrate with a LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b02a97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When atoms or molecules in the sun's core fuse together, they create helium at temperatures of up to 250 million degrees Fahrenheit (121 Kelvin). This energy is released as high-intensity light and radiation known as gamma rays. Nuclear fusion occurs very rarely inside stars; however, it can happen near other atomic nuclei such as protons ordealin stars known as plasmas. When two atomic nuclei collide enough times, the strong force causes them to bind into one larger nucleus called a atomino union between the atoms results in a release of more than 1022 joules of energy compared to nuclear fission which releases about 1018 joules of energy, making it be considered as the most probable reaction leading to a universe ending Big Bang. Both nuclear fission andfusion reactions are currently occurring in research reactors around the world.\n",
      "Nuclear fusion creates additional energy from its own input without necessarily having to convert foodstuff, fuel, etc into heat/power using mechanisms like steam turbines.  This leads scientists hope that sustainable technology based on nuclear fusion might exist some day providing clean and unlimited energy indefinitely - far exceeding anything else we know today. At present there are several scientific efforts ongoing which aim toward achieving this goal.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "Explain the following topics for a 5-year-old.\n",
    "Question: {question}\n",
    "Answer: {topic} is\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"topic\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "topic = \"nuclear fusion\"\n",
    "question = f\"What is {topic}?\"\n",
    "\n",
    "generated = llm_chain.run(question=question, topic=topic)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Joe Biden\"\n",
    "print(llm_chain.run(question=f\"Who is {name}?\", topic=name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
