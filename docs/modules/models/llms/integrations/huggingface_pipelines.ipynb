{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "959300d4",
   "metadata": {},
   "source": [
    "# Hugging Face Local Pipelines\n",
    "\n",
    "Hugging Face models can be run locally through the `HuggingFacePipeline` class.\n",
    "\n",
    "The [Hugging Face Model Hub](https://huggingface.co/models) hosts over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.\n",
    "\n",
    "These can be called from LangChain either through this local pipeline wrapper or by calling their hosted inference endpoints through the HuggingFaceHub class. For more information on the hosted pipelines, see the [HuggingFaceHub](huggingface_hub.ipynb) notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c1b8450-5eaf-4d34-8341-2d785448a1ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "To use, you should have the ``transformers`` python [package installed](https://pypi.org/project/transformers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d772b637-de00-4663-bd77-9bc96d798db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers > /dev/null"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91ad075f-71d5-4bc8-ab91-cc0ad5ef16bb",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ae236-962a-4763-8052-c4836d78a5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(model_id=\"bigscience/bloom-1b7\", task=\"text-generation\", model_kwargs={\"temperature\":0, \"max_length\":64})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47e45c72",
   "metadata": {},
   "source": [
    "### Accelerate Model Inference With DeepSpeed\n",
    "\n",
    "DeepSpeed allows an over 2X inference speedup for a wide variety of models.  To use DeepSpeed, you need to have Cuda installed and install a version of Pytorch that was compiled with the version of Cuda you have installed.  Using docker and nvidia-docker along with official Cuda docker images can help set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepspeed > /dev/null"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8da848c2",
   "metadata": {},
   "source": [
    "#### DeepSpeed Arguments\n",
    "You can control DeepSpeed with an optional argument called ```deepspeed_args```\n",
    "\n",
    "This variable takes a dictionary with two key values, ```dtype``` and ```max_tokens```\n",
    "\n",
    "Valid values for ```dtype``` include ```fp32```, ```fp16``` and ```int8```.  Defaults to ```fp16```\n",
    "\n",
    "Valid values for ```max_tokens``` is any integer less than the max context window for the model.  Smaller values take less memory.  Defaults to the max size given a model.\n",
    "\n",
    "Passing any value, including an empty dictionary, will activate DeepSpeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf51ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "import torch\n",
    "llm = HuggingFacePipeline.from_model_id(model_id=\"EleutherAI/gpt-j-6b\", task=\"text-generation\", model_kwargs={\"torch_dytpe\":torch.float16},deepspeed_args={\"dtype\":\"fp16\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00104b27-0c15-4a97-b198-4512337ee211",
   "metadata": {},
   "source": [
    "### Integrate the model in an LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate,  LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a3837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
