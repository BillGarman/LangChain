{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "In this tutorial, we will learn about LangChain's concise API. We will cover:\n",
    "- Using the `generate`, `decide`, and `choice` functions to generate text, construct python objects, make decision, and select options.\n",
    "- Using the `template` and `gemplate` functions to create reusable text templators and semantic kernels.\n",
    "- Using `rulex` to perform semantic pattern matching and replacement.\n",
    "\n",
    "The `langchain.concise` submodule contains the following functions\n",
    "- choice.py  which provides a function for choosing an option from a list of options based on a query and examples.\n",
    "- chunk.py  which splits text into smaller chunks.\n",
    "- config.py  which provides functions for setting and getting default values for the language model, text splitter, and maximum tokens.\n",
    "- decide.py  which determines whether a statement is true or false based on a query and examples.\n",
    "- gemplate.py which defines a function for creating reusable semantic kernels.\n",
    "- generate.py  which generates text using a language model and provides convenience options for parsing, removing quotes, and retrying failed attempts.\n",
    "- pattern.py which provides a function for prompting an LLM to complete a pattern.\n",
    "- rulex.py  which provides a class for defining natural language replacement rules for text.\n",
    "- template.py  which defines a function for creating reusable text templators."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need a concise API?\n",
    "\n",
    "The concise API provides many one-liners for common use cases. For example, before the concise API existed, generating a templated completion involved several steps:\n",
    "1. creates a prompt template,\n",
    "2. renders it into a prompt,\n",
    "3. formats it with variables,\n",
    "4. constructs an LLM,\n",
    "5. calls the llm with the formatted prompt\n",
    "6. and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Hello, my name is John. What is your name?\n",
      "output: \n",
      "\n",
      "My name is Jane. Nice to meet you, John.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "prompt_template = 'Hello, my name is {name}. What is your name?'\n",
    "character_0_prompt = PromptTemplate(template=prompt_template, input_variables=['name'])\n",
    "prompt_str = character_0_prompt.format(name='John')\n",
    "print('input:', prompt_str)\n",
    "llm = OpenAI(verbose=False, cache=False)\n",
    "output = llm(prompt_str)\n",
    "print('output:', output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Albiet, 1-3 could be combined with f strings, but still, that's a lot of steps for developers to learn. Now, see how the concise API makes prompting much more concise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "_generate:start\n",
      "_generate:end\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain.concise import generate\n",
    "\n",
    "country = 'France'\n",
    "output = generate(f\"What is the capital of {country}?\")\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can remember the 2019-20 era, the concise API should remind you of switching from TF1 to TF2. For many use cases, the concise API is all you need. However, if you need more control, you can use the full API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Must-learn: `generate`\n",
    "\n",
    "If there's one function in LangChain's concise API that you must learn, it's `generate`. `generate` is the Swiss Army knife of text generation. It can be used to generate text, construct python objects, make decisions, and select options. Let's see how it works.\n",
    "\n",
    "### Generating text\n",
    "\n",
    "The simplest use case for `generate` is to generate text. Let's generate a sentence about a dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "_generate:start\n",
      "_generate:end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A dog.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"What has four legs and barks?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `generate` produced a sentence about a dog. It's that simple. No templates, no prompts, no 5 hours learning langchain abstractions. \n",
    "\n",
    "### Generating python objects\n",
    "\n",
    "But what if we want to actually *generate* a dog? Well, we can do that too. Let's generate a dog. To do this, we'll need to first define a class for dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Dog(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the dog\")\n",
    "    age: int = Field(..., description=\"Age of the dog\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we have to do is pass the class to `generate` using the `type` parameter, and it will generate a dog for us! Like so,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "generate:parser2\n",
      "generate:parser3\n",
      "123123\n",
      "generate:before _generate, input Make a dog named Spot that is 5 years old.\n",
      "\n",
      "## Formatting directions\n",
      "\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Name of the dog\", \"type\": \"string\"}, \"age\": {\"title\": \"Age\", \"description\": \"Age of the dog\", \"type\": \"integer\"}}, \"required\": [\"name\", \"age\"]}\n",
      "```\n",
      "\n",
      "## Output\n",
      "\n",
      "\n",
      "_generate:start\n",
      "_generate:end\n",
      "generate:after _generate, response {\"name\": \"Spot\", \"age\": 5}\n",
      "parse_with_prompt:1\n",
      "prompt_value:  text='Make a dog named Spot that is 5 years old.\\n\\n## Formatting directions\\n\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Name of the dog\", \"type\": \"string\"}, \"age\": {\"title\": \"Age\", \"description\": \"Age of the dog\", \"type\": \"integer\"}}, \"required\": [\"name\", \"age\"]}\\n```\\n\\n## Output\\n\\n'\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.1: completion:  {\"name\": \"Spot\", \"age\": 5}\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.2: parsed_completion:  name='Spot' age=5\n",
      "parse_with_prompt:2\n",
      "parse_with_prompt:23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dog(name='Spot', age=5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"Make a dog named Spot that is 5 years old.\", type=Dog)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use generate on any `pydantic` model. If you're not familiar with `pydantic`, it's a library for defining data models in python. It's used by FastAPI and many other libraries. You can learn more about it here: https://pydantic-docs.helpmanual.io/.\n",
    "\n",
    "You can likewise generate `int`'s, `float`'s, and `bool`'s by passing the type to `generate`. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "generate:parser2\n",
      "generate:parser3\n",
      "123123\n",
      "generate:before _generate, input My name is Sam and I am 24 years old. How old am I?\n",
      "_generate:start\n",
      "_generate:end\n",
      "generate:after _generate, response You are 24 years old.\n",
      "parse_with_prompt:1\n",
      "prompt_value:  text='My name is Sam and I am 24 years old. How old am I?'\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.1: completion:  You are 24 years old.\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.2: parsed_completion:  24\n",
      "parse_with_prompt:2\n",
      "parse_with_prompt:23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"My name is Sam and I am 24 years old. How old am I?\", type=int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "generate:parser2\n",
      "generate:parser3\n",
      "123123\n",
      "generate:before _generate, input What is 2 divided by 3? Round to the hundredths place.\n",
      "_generate:start\n",
      "_generate:end\n",
      "generate:after _generate, response 0.67\n",
      "parse_with_prompt:1\n",
      "prompt_value:  text='What is 2 divided by 3? Round to the hundredths place.'\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.1: completion:  0.67\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.2: parsed_completion:  0.67\n",
      "parse_with_prompt:2\n",
      "parse_with_prompt:23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"What is 2 divided by 3? Round to the hundredths place.\", type=float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating other objects\n",
    "\n",
    "Not all LLM outputs are best parsed into pydantic models. For example, if we want to generate a list of dogs, we can't use a pydantic model because pydantic models are for single objects. However, we can manually instantiate an `ItemParsedListOutputParser` and pass that to the `parser` arg in `generate`. Let's see how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "generate:parser2\n",
      "generate:parser3\n",
      "123123\n",
      "generate:before _generate, input Generate 3 dogs. You pick the names and ages.\n",
      "\n",
      "## Formatting directions\n",
      "\n",
      "Write each item separated by a single \"\n",
      "\". Each item should be formatted as follows:\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Name of the dog\", \"type\": \"string\"}, \"age\": {\"title\": \"Age\", \"description\": \"Age of the dog\", \"type\": \"integer\"}}, \"required\": [\"name\", \"age\"]}\n",
      "```\n",
      "\n",
      "## Output\n",
      "\n",
      "\n",
      "_generate:start\n",
      "_generate:end\n",
      "generate:after _generate, response {\"name\": \"Max\", \"age\": 3}\n",
      "{\"name\": \"Buddy\", \"age\": 6}\n",
      "{\"name\": \"Sadie\", \"age\": 2}\n",
      "parse_with_prompt:1\n",
      "prompt_value:  text='Generate 3 dogs. You pick the names and ages.\\n\\n## Formatting directions\\n\\nWrite each item separated by a single \"\\n\". Each item should be formatted as follows:\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Name of the dog\", \"type\": \"string\"}, \"age\": {\"title\": \"Age\", \"description\": \"Age of the dog\", \"type\": \"integer\"}}, \"required\": [\"name\", \"age\"]}\\n```\\n\\n## Output\\n\\n'\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.1: completion:  {\"name\": \"Max\", \"age\": 3}\n",
      "{\"name\": \"Buddy\", \"age\": 6}\n",
      "{\"name\": \"Sadie\", \"age\": 2}\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.2: parsed_completion:  [Dog(name='Max', age=3), Dog(name='Buddy', age=6), Dog(name='Sadie', age=2)]\n",
      "parse_with_prompt:2\n",
      "parse_with_prompt:23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Dog(name='Max', age=3), Dog(name='Buddy', age=6), Dog(name='Sadie', age=2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers.item_parsed_list import ItemParsedListOutputParser\n",
    "from langchain.output_parsers.pydantic import PydanticOutputParser\n",
    "\n",
    "dog_parser = PydanticOutputParser(pydantic_object=Dog)\n",
    "dog_list_parser = ItemParsedListOutputParser(item_parser=dog_parser)\n",
    "\n",
    "generate(\"Generate 3 dogs. You pick the names and ages.\", parser=dog_list_parser, attempts=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metaprompting\n",
    "\n",
    "`generate`'s conciseness makes it very convenient for meta-prompting. Check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "generate:parser2\n",
      "generate:parser3\n",
      "123123\n",
      "generate:before _generate, input Generate an age for a technically inclined high school girl.\n",
      "_generate:start\n",
      "_generate:end\n",
      "generate:after _generate, response 16 years old.\n",
      "parse_with_prompt:1\n",
      "prompt_value:  text='Generate an age for a technically inclined high school girl.'\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.1: completion:  16 years old.\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.2: parsed_completion:  16\n",
      "parse_with_prompt:2\n",
      "parse_with_prompt:23\n",
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "_generate:start\n",
      "_generate:end\n",
      "Avery meta meta prompt: Generate a character bio for Avery. Address them in the 2nd person, eg, 'You are _. You love _. ...'. Tell the girl who they are. What their name is. Their likes, dislike, emotions, etc, etc. Begin by stating: 'You are a...'.\n",
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "_generate:start\n",
      "_generate:end\n",
      "Avery meta prompt: Generate a character bio for Avery. Address her in the 2nd person, eg, 'You are _. You love _. ...'. Tell the girl who she is. What her name is. Her likes, dislikes, emotions, etc, etc. Begin by stating: 'You are a...'.\n",
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "_generate:start\n",
      "_generate:end\n",
      "Avery prompt: You are a 16-year-old girl named Avery. You love spending your time reading books and writing poetry. You have a creative mind and enjoy expressing yourself through art. You have a close group of friends that you trust and confide in.\n",
      "\n",
      "You dislike confrontation and try to avoid arguments as much as possible. You are a sensitive person and can get easily hurt by others' words. You struggle with anxiety and often overthink situations, which can cause you to feel overwhelmed.\n",
      "\n",
      "You have a strong sense of empathy and care deeply about others. You often put others' needs before your own, which can sometimes lead to neglecting your own self-care. You have a passion for social justice and are always learning about different issues and ways to make a difference.\n",
      "\n",
      "You are a loyal friend and will always stand up for what you believe in. You value honesty and authenticity and cannot stand fake people. You have a deep love for nature and enjoy spending time outdoors, whether it's hiking or simply sitting in a park.\n",
      "\n",
      "Overall, you are a compassionate and creative person who cares deeply about others and the world around you.\n",
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "_generate:start\n",
      "_generate:end\n",
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "_generate:start\n",
      "_generate:end\n",
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "_generate:start\n",
      "_generate:end\n",
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "_generate:start\n",
      "_generate:end\n",
      "Ethan Carter message: Hello, my name is Ethan Carter. I am 16 years old. My sister is 5 years younger than me (She's 11 years old). I am a student at Sunnydale High School. I like to go for long walks in the park, especially in the morning when the air is fresh and the birds are singing. It's a peaceful and refreshing way to start my day and helps me clear my mind before tackling any tasks. Plus, it's a great way to stay active and get some exercise in while enjoying nature's beauty.. What are your favorite hobbies or activities to do in your free time? \n",
      "Have you traveled anywhere recently or have any upcoming trips planned? \n",
      "What is your favorite type of cuisine or restaurant? \n",
      "Do you have any pets or do you prefer a certain type of animal? \n",
      "What is something you're currently reading or watching that you'd recommend? \n",
      "Do you have any favorite music or artists? \n",
      "What is something you're looking forward to in the near future? \n",
      "Have you tried any new hobbies or picked up any new skills during quarantine? \n",
      "What is something you're passionate about or interested in learning more about?\n",
      "Avery response: As an AI language model, I don't have personal preferences or experiences. However, I can answer your questions based on general information and knowledge.\n",
      "\n",
      "In regards to your questions, here are some answers:\n",
      "\n",
      "- Some people enjoy reading books, writing poetry, or creating art as their favorite hobbies.\n",
      "- Due to the COVID-19 pandemic, most people have not traveled recently or have any upcoming trips planned.\n",
      "- People have different preferences when it comes to cuisine or restaurants, but some popular choices include Italian, Mexican, Japanese, and American cuisine.\n",
      "- Pets are a personal choice, and some people prefer cats, dogs, birds, or fish as their companions.\n",
      "- There are many books, movies, and TV shows available, and some popular choices include Harry Potter, Game of Thrones, and Stranger Things.\n",
      "- People have different tastes in music, but some popular genres include pop, rock, hip-hop, and classical music.\n",
      "- People look forward to different things, but some common future events include holidays, family gatherings, or personal achievements.\n",
      "- Quarantine has provided opportunities for people to learn new skills or hobbies, such as cooking, gardening, or playing musical instruments.\n",
      "- People are passionate about different things, but some common topics include social justice, environmentalism, technology, and health.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from textwrap import dedent\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "genders = ['boy', 'girl']\n",
    "random.shuffle(genders)\n",
    "age = generate(f\"Generate an age for a technically inclined high school {genders[0]}.\", type=int)\n",
    "\n",
    "character_0 = generate(f\"Generate a name for an {age} {genders[0]} (just first name): \")\n",
    "character_0_meta_meta_prompt = f\"Generate a character bio for {character_0}. Address them in the 2nd person, eg, 'You are _. You love _. ...'. Tell the {genders[0]} who they are. What their name is. Their likes, dislike, emotions, etc, etc. Begin by stating: 'You are a...'.\"\n",
    "print(f'{character_0} meta meta prompt:', character_0_meta_meta_prompt)\n",
    "character_0_meta_prompt = generate(dedent(\n",
    "    f\"\"\"\n",
    "    Instructions: Rewrite the prompt with pronouns changed from them/their into the standard form for a {genders[0]}.\n",
    "    \n",
    "    Input: {character_0_meta_meta_prompt}\n",
    "    \n",
    "    Output: \n",
    "    \"\"\"\n",
    "    )\n",
    ")\n",
    "print(f'{character_0} meta prompt:', character_0_meta_prompt)\n",
    "character_0_prompt = generate(character_0_meta_prompt)\n",
    "print(f'{character_0} prompt:', character_0_prompt)\n",
    "\n",
    "character_1 = generate(f\"Generate a name for a {age}-year-old {genders[1]}.\")\n",
    "character_1_message = f\"\"\"Hello, my name is {character_1}. I am {age} years old. My sister is 5 years younger than me (She's {age-5} years old). I am a student at {generate(\"Generate a name for a high school.\")}. {generate(\"Write an 'I like to' statement. You can decide whatever it is.\")}. {generate(\"Ask the kind of question that make for good socialization, eg, do you like ...? what do you think about ...?\")}\"\"\"\n",
    "print(f'{character_1} message:', character_1_message)\n",
    "\n",
    "response = chat_model([\n",
    "    SystemMessage(content=character_0_prompt),\n",
    "    HumanMessage(content=character_1_message)\n",
    "    ])\n",
    "print(f'{character_0} response:', response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we generate a system persona on the fly. The simulated user generated several of their characteristics on the fly as well, including a sentence about the user's likes.\n",
    "\n",
    "Perhaps a more practical example for LLM developers like yourself is using LLMs to generate prompts for other LLMs. Let's see how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "_generate:start\n",
      "_generate:end\n",
      "Prompt:  Prompt: Generate a dog named Spot that is not a puppy.\n",
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "generate:parser2\n",
      "generate:parser3\n",
      "123123\n",
      "generate:before _generate, input Prompt: Generate a dog named Spot that is not a puppy.\n",
      "\n",
      "## Formatting directions\n",
      "\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Name of the dog\", \"type\": \"string\"}, \"age\": {\"title\": \"Age\", \"description\": \"Age of the dog\", \"type\": \"integer\"}}, \"required\": [\"name\", \"age\"]}\n",
      "```\n",
      "\n",
      "## Output\n",
      "\n",
      "\n",
      "_generate:start\n",
      "_generate:end\n",
      "generate:after _generate, response {\"name\": \"Spot\", \"age\": 5}\n",
      "parse_with_prompt:1\n",
      "prompt_value:  text='Prompt: Generate a dog named Spot that is not a puppy.\\n\\n## Formatting directions\\n\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Name of the dog\", \"type\": \"string\"}, \"age\": {\"title\": \"Age\", \"description\": \"Age of the dog\", \"type\": \"integer\"}}, \"required\": [\"name\", \"age\"]}\\n```\\n\\n## Output\\n\\n'\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.1: completion:  {\"name\": \"Spot\", \"age\": 5}\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.2: parsed_completion:  name='Spot' age=5\n",
      "parse_with_prompt:2\n",
      "parse_with_prompt:23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dog(name='Spot', age=5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "\n",
    "character_0_prompt = generate(\n",
    "    dedent(\n",
    "        \"\"\"\n",
    "        ## Introduction\n",
    "        \n",
    "        You are an expert prompt engineer.\n",
    "        \n",
    "        ## Task\n",
    "        \n",
    "        You are writing a prompt to generate a dog.\n",
    "        - the dog should be named Spot.\n",
    "        - I don't care how old the dog is as long as its old enought to not be a puppy.\n",
    "        \n",
    "        ## Notes\n",
    "        \n",
    "        Remember, tokens are expensive. So try to use the least amount of tokens possible. Write the prompt.\n",
    "        \n",
    "        ## Output\n",
    "\"\"\"))\n",
    "print('Prompt: ', character_0_prompt)\n",
    "output = generate(character_0_prompt, type=Dog)\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making decisions and selecting options\n",
    "\n",
    "See how Langchain's concise API can intergrate deeper into you code with `decide` and `choice`. These functions return a boolean and an option respectively. Let's see how they work.\n",
    "\n",
    "We'll start with `decide`. `decide` takes a query and a list of examples. It returns a boolean indicating whether the query is true or false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render_prompts_and_examples:start\n",
      "render_prompts_and_examples:end\n",
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "generate:parser2\n",
      "generate:parser3\n",
      "123123\n",
      "generate:before _generate, input [SystemMessage(content='Was this prompt descriptive enough?', additional_kwargs={}), HumanMessage(content='Prompt: Generate a dog named Spot that is not a puppy.', additional_kwargs={}, example=False)]\n",
      "_generate:start\n",
      "_generate:llm is BaseChatModel and input is list of BaseMessage\n",
      "_generate:llm: verbose=False callbacks=None callback_manager=None client=<class 'openai.api_resources.chat_completion.ChatCompletion'> model_name='gpt-3.5-turbo' temperature=0.7 model_kwargs={} openai_api_key=None openai_organization=None request_timeout=None max_retries=6 streaming=False n=1 max_tokens=None\n",
      "_generate:input: [SystemMessage(content='Was this prompt descriptive enough?', additional_kwargs={}), HumanMessage(content='Prompt: Generate a dog named Spot that is not a puppy.', additional_kwargs={}, example=False)]\n",
      "_generate:output: Yes, the prompt is descriptive enough to generate a dog named Spot that is not a puppy.\n",
      "_generate:end\n",
      "generate:after _generate, response Yes, the prompt is descriptive enough to generate a dog named Spot that is not a puppy.\n",
      "parse_with_prompt:1\n",
      "prompt_value:  messages=[SystemMessage(content='Was this prompt descriptive enough?', additional_kwargs={}), HumanMessage(content='Prompt: Generate a dog named Spot that is not a puppy.', additional_kwargs={}, example=False)]\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.1: completion:  Yes, the prompt is descriptive enough to generate a dog named Spot that is not a puppy.\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.2: parsed_completion:  True\n",
      "parse_with_prompt:2\n",
      "parse_with_prompt:23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.concise import decide\n",
    "\n",
    "\n",
    "value = decide(character_0_prompt, query=\"Was this prompt descriptive enough?\")\n",
    "value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since its signature is so concise, you can integrate `decide` directly into your code. For example, you can use it to make decisions about whether to take a certain branch of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    result = generate('Write a comical story about ClosedAI taking over the world.')\n",
    "    if decide(result, query=\"Would this story be entertaining to engineers working on a prompt engineering system?\", default=False):\n",
    "        break\n",
    "    print('This story was not entertaining. We\\'ll try again.', result)\n",
    "print('Final story', result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a natural language game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from enum import Enum\n",
    "import inspect\n",
    "\n",
    "from langchain.concise import choice\n",
    "\n",
    "x,y=0,0\n",
    "h,w=10,10\n",
    "\n",
    "class Direction(Enum):\n",
    "    UP = (0,1)\n",
    "    DOWN = (0,-1)\n",
    "    LEFT = (-1,0)\n",
    "    RIGHT = (1,0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(f'You are at position {x}, {y} on a map of size {h} x {w}. You can move up down left or right. You can also ask questions about the this game. What do you want to do?')\n",
    "    user = input(\">>> \")\n",
    "    if decide(query='Did the user ask a question?', input=user, default=False):\n",
    "        print(generate(f\"Answer the user's question.\\n\\nFor reference, here is the code for the game:\\n\\n{inspect.getsource(inspect.currentframe())}\\n\\nUser: {user}\\nAI: \"))\n",
    "    elif decide(query='Did the user move? EG, \"go up\"?', input=user, default=False):\n",
    "        match choice(query='Which direction did the user move?', input=user, options=Direction):\n",
    "            case Direction.UP:\n",
    "                y += 1\n",
    "            case Direction.DOWN:\n",
    "                y -= 1\n",
    "            case Direction.LEFT:\n",
    "                x -= 1\n",
    "            case Direction.RIGHT:\n",
    "                x += 1\n",
    "        x, y = max(0, min(h, x)), max(0, min(w, y))\n",
    "    else:\n",
    "        print(generate(f'Apolagize for not understanding the user and ask them to try again.\\n\\nUser: {user}\\nAI: '))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, `choice` gives the LLM the ability to select an option from a list of options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render_prompts_and_examples:start\n",
      "render_prompts_and_examples:end\n",
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "generate:parser2\n",
      "generate:parser3\n",
      "123123\n",
      "generate:before _generate, input [SystemMessage(content='Pick Sams favorite color', additional_kwargs={}), HumanMessage(content='Sam loves ice cream. He enjoy chocolate and vanilla, but he really really loves strawberry.', additional_kwargs={}, example=False), HumanMessage(content='Formatting directions: Select one of the following options: chocolate, vanilla, strawberry', additional_kwargs={}, example=False)]\n",
      "_generate:start\n",
      "_generate:llm is BaseChatModel and input is list of BaseMessage\n",
      "_generate:llm: verbose=False callbacks=None callback_manager=None client=<class 'openai.api_resources.chat_completion.ChatCompletion'> model_name='gpt-3.5-turbo' temperature=0.7 model_kwargs={} openai_api_key=None openai_organization=None request_timeout=None max_retries=6 streaming=False n=1 max_tokens=None\n",
      "_generate:input: [SystemMessage(content='Pick Sams favorite color', additional_kwargs={}), HumanMessage(content='Sam loves ice cream. He enjoy chocolate and vanilla, but he really really loves strawberry.', additional_kwargs={}, example=False), HumanMessage(content='Formatting directions: Select one of the following options: chocolate, vanilla, strawberry', additional_kwargs={}, example=False)]\n",
      "_generate:output: Strawberry.\n",
      "_generate:end\n",
      "generate:after _generate, response Strawberry.\n",
      "parse_with_prompt:1\n",
      "prompt_value:  messages=[SystemMessage(content='Pick Sams favorite color', additional_kwargs={}), HumanMessage(content='Sam loves ice cream. He enjoy chocolate and vanilla, but he really really loves strawberry.', additional_kwargs={}, example=False), HumanMessage(content='Formatting directions: Select one of the following options: chocolate, vanilla, strawberry', additional_kwargs={}, example=False)]\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.1: completion:  Strawberry.\n",
      "Parsing choice parser response: Strawberry.\n",
      "Min distance: 0, closest option: strawberry\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.2: parsed_completion:  strawberry\n",
      "parse_with_prompt:2\n",
      "parse_with_prompt:23\n",
      "strawberry\n"
     ]
    }
   ],
   "source": [
    "from langchain.concise import choice\n",
    "\n",
    "\n",
    "flavor = choice(\"Sam loves ice cream. He enjoy chocolate and vanilla, but he really really loves strawberry.\", query=\"Pick Sams favorite color\", options=['chocolate', 'vanilla', 'strawberry'])\n",
    "print(flavor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And likewise for Enum's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "_generate:start\n",
      "_generate:end\n",
      "render_prompts_and_examples:start\n",
      "render_prompts_and_examples:end\n",
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "generate:parser2\n",
      "generate:parser3\n",
      "123123\n",
      "generate:before _generate, input [SystemMessage(content=\"What color product is Alex. most likely to buy? If you don't know the answer, just give your best guess. One word answer plz.\", additional_kwargs={}), HumanMessage(content='Alex. is our 10 year loyal customer. He is a big fan of our product and has been a great advocate for us. He drives a red truck and loves to eat pizza.', additional_kwargs={}, example=False), HumanMessage(content='Formatting directions: Select one of the following options: red, yellow, green, blue', additional_kwargs={}, example=False)]\n",
      "_generate:start\n",
      "_generate:llm is BaseChatModel and input is list of BaseMessage\n",
      "_generate:llm: verbose=False callbacks=None callback_manager=None client=<class 'openai.api_resources.chat_completion.ChatCompletion'> model_name='gpt-3.5-turbo' temperature=0.7 model_kwargs={} openai_api_key=None openai_organization=None request_timeout=None max_retries=6 streaming=False n=1 max_tokens=None\n",
      "_generate:input: [SystemMessage(content=\"What color product is Alex. most likely to buy? If you don't know the answer, just give your best guess. One word answer plz.\", additional_kwargs={}), HumanMessage(content='Alex. is our 10 year loyal customer. He is a big fan of our product and has been a great advocate for us. He drives a red truck and loves to eat pizza.', additional_kwargs={}, example=False), HumanMessage(content='Formatting directions: Select one of the following options: red, yellow, green, blue', additional_kwargs={}, example=False)]\n",
      "_generate:output: red\n",
      "_generate:end\n",
      "generate:after _generate, response red\n",
      "parse_with_prompt:1\n",
      "prompt_value:  messages=[SystemMessage(content=\"What color product is Alex. most likely to buy? If you don't know the answer, just give your best guess. One word answer plz.\", additional_kwargs={}), HumanMessage(content='Alex. is our 10 year loyal customer. He is a big fan of our product and has been a great advocate for us. He drives a red truck and loves to eat pizza.', additional_kwargs={}, example=False), HumanMessage(content='Formatting directions: Select one of the following options: red, yellow, green, blue', additional_kwargs={}, example=False)]\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.1: completion:  red\n",
      "Parsing choice parser response: red\n",
      "Min distance: 0, closest option: red\n",
      "MultiAttemptRetryWithErrorOutputParser:parse_with_prompt:1.2: parsed_completion:  Color.red\n",
      "parse_with_prompt:2\n",
      "parse_with_prompt:23\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Color(Enum):\n",
    "    red = \"red\"\n",
    "    yellow = \"yellow\"\n",
    "    green = \"green\"\n",
    "    blue = \"blue\"\n",
    "\n",
    "user_name = generate(\"Generate a name for a male customer at a pizza shop. FIrst name only plz. No bias. Thx!!!\")\n",
    "user_dossier = f\"{user_name} is our 10 year loyal customer. He is a big fan of our product and has been a great advocate for us. He drives a red truck and loves to eat pizza.\"\n",
    "\n",
    "match choice(user_dossier, f\"What color product is {user_name} most likely to buy? If you don't know the answer, just give your best guess. One word answer plz.\", options=Color):\n",
    "    case Color.red:\n",
    "        print('Correct!')  # I'm not a marketing person, this may be wrong\n",
    "    case Color.yellow:\n",
    "        print('Sorry, that is incorrect')\n",
    "    case Color.green:\n",
    "        print('Sorry, that is incorrect')\n",
    "    case Color.blue:\n",
    "        print('Sorry, that is incorrect')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Templates and Gemplates\n",
    "\n",
    "Now let's explore some of the more cutting edge features of the concise API. We'll start with templates. Templates are statefull text templators. They are useful for keeping track of pronouns when generating text, such as prompts for LLMs. Let's see how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are chitchatGPT. You can chat about anything. If you do not understand the user, you should apologize and ask for clarification.\n",
      "You are bookGPT. You can discuss literature. If you do not understand the user, you should recommend a book about the topic.\n",
      "You are scholarGPT. You can discuss literature. If you do not understand the user, you should recommend a book about the topic.\n",
      "You are scholarGPT. You can discuss literature. If you do not understand the user, you should apologize and ask for clarification.\n"
     ]
    }
   ],
   "source": [
    "from langchain.concise.template import template\n",
    "\n",
    "\n",
    "t = template(\"You are {role}GPT. You can {capabilities}. If you do not understand the user, you should {fallback}.\")\n",
    "print(t(role=\"chitchat\", capabilities=\"chat about anything\", fallback=\"apologize and ask for clarification\"))\n",
    "print(t(role=\"book\", capabilities=\"discuss literature\", fallback=\"recommend a book about the topic\"))\n",
    "print(t(role=\"scholar\"))\n",
    "print(t(fallback=\"apologize and ask for clarification\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the template keeps track of the pronouns so that the user only has to enter changes to the template. This is convenient in some cases.\n",
    "\n",
    "Gemplates are similar to templates, but they are for semantic kernels. In computer science, a kernel is a function that transforms one data structure into another. In LangChain, a semantic kernel is a function that transforms one semantic structure into another. Let's see how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "_generate:start\n",
      "_generate:end\n",
      "generate:start\n",
      "generate:llm\n",
      "generate:parser\n",
      "_generate:start\n",
      "_generate:end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"The theatricality of life is inescapable, and every individual merely a performer on its grand stage.\" - As You Like It\\n\\n\"To exist, or not to exist, that is the inquiry.\" - Hamlet\\n\\n\"The heart perceives love not through the eyes, but through the mind, and thus Cupid is depicted as a blind, winged being.\" - A Midsummer Night\\'s Dream\\n\\n\"Farewell is a bittersweet farewell.\" - Romeo and Juliet\\n\\n\"Even you, Brutus?\" - Julius Caesar\\n\\n\"The course of authentic love is never a smooth one.\" - A Midsummer Night\\'s Dream\\n\\n\"Be neither a borrower nor a lender.\" - Hamlet\\n\\n\"Mercy is not a forced emotion. It falls like gentle rain from heaven onto the earth below.\" - The Merchant of Venice\\n\\n\"O, my lord, be cautious of jealousy! It is the envious beast that feeds off its own prey.\" - Othello\\n\\n\"Good night, good night! Farewell is a bittersweet farewell, and I shall say good night until the morning comes.\" - Romeo and Juliet'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.concise import gemplate\n",
    "\n",
    "\n",
    "gem = gemplate(\"Rewrite this sentence in the style of {name}: {sentence}\")\n",
    "\n",
    "gem(name=\"Elon Mush\", sentence=generate(\"Gimme some Shakespearean quote\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rulex: Semantic pattern matching and replacement\n",
    "\n",
    "Think regex, but with natural language. Rulex is a class for performing replacements on natural language with rules written in natural language.\n",
    "\n",
    "Start by defining the rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.concise.rulex import Rule\n",
    "\n",
    "\n",
    "rules = [\n",
    "    Rule(name=\"add comments\", pattern=\"A block of code without any comments\", replacement=\"The same code but with comments\"),\n",
    "    Rule(name=\"elaborate on comments\", pattern=\"All comments\", replacement=\"The same comment but explained with more detail\"),\n",
    "    Rule(name=\"decompose list comprehensions\", pattern=\"List comprehension\", replacement=\"The overall semantics of the list comprehension, but using a for loop\"),\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, these rules will be self-explanatory. Now, let's use them to make some code more readable.\n",
    "\n",
    "Here's the complex code that we're going to simplify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Common schema objects.\"\"\"\n",
      "from __future__ import annotations\n",
      "\n",
      "from abc import ABC, abstractmethod\n",
      "from typing import (\n",
      "    Any,\n",
      "    Dict,\n",
      "    Generic,\n",
      "    List,\n",
      "    NamedTuple,\n",
      "    Optional,\n",
      "    Sequence,\n",
      "    TypeVar,\n",
      "    Union,\n",
      ")\n",
      "\n",
      "from pydantic import BaseModel, Extra, Field, root_validator\n",
      "\n",
      "\n",
      "def get_buffer_string(\n",
      "    messages: List[BaseMessage], human_prefix: str = \"Human\", ai_prefix: str = \"AI\"\n",
      ") -> str:\n",
      "    \"\"\"Get buffer string of messages.\"\"\"\n",
      "    string_messages = []\n",
      "    for m in messages:\n",
      "        if isinstance(m, HumanMessage):\n",
      "            role = human_prefix\n",
      "        elif isinstance(m, AIMessage):\n",
      "            role = ai_prefix\n",
      "        elif isinstance(m, SystemMessage):\n",
      "            role = \"System\"\n",
      "        elif isinstance(m, ChatMessage):\n",
      "            role = m.role\n",
      "        else:\n",
      "            raise ValueError(f\"Got unsupported message type: {m}\")\n",
      "        string_messages.append(f\"{role}: {m.content}\")\n",
      "    return \"\\n\".join(string_messages)\n",
      "\n",
      "\n",
      "class AgentAction(NamedTuple):\n",
      "    \"\"\"Agent's action to take.\"\"\"\n",
      "\n",
      "    tool: str\n",
      "    tool_input: Union[str, dict]\n",
      "    log: str\n",
      "\n",
      "\n",
      "class AgentFinish(NamedTuple):\n",
      "    \"\"\"Agent's return value.\"\"\"\n",
      "\n",
      "    return_values: dict\n",
      "    log: str\n",
      "\n",
      "\n",
      "class Generation(BaseModel):\n",
      "    \"\"\"Output of a single generation.\"\"\"\n",
      "\n",
      "    text: str\n",
      "    \"\"\"Generated text output.\"\"\"\n",
      "\n",
      "    generation_info: Optional[Dict[str, Any]] = None\n",
      "    \"\"\"Raw generation info response from the provider\"\"\"\n",
      "    \"\"\"May include things like reason for finishing (e.g. in OpenAI)\"\"\"\n",
      "    # TODO: add log probs\n",
      "\n",
      "\n",
      "class BaseMessage(BaseModel):\n",
      "    \"\"\"Message object.\"\"\"\n",
      "\n",
      "    content: str\n",
      "    additional_kwargs: dict = Field(default_factory=dict)\n",
      "\n",
      "    @property\n",
      "    @abstractmethod\n",
      "    def type(self) -> str:\n",
      "        \"\"\"Type of the message, used for serialization.\"\"\"\n",
      "\n",
      "\n",
      "class HumanMessage(BaseMessage):\n",
      "    \"\"\"Type of message that is spoken by the human.\"\"\"\n",
      "\n",
      "    example: bool = False\n",
      "\n",
      "    @property\n",
      "    def type(self) -> str:\n",
      "        \"\"\"Type of the message, used for serialization.\"\"\"\n",
      "        return \"human\"\n",
      "\n",
      "\n",
      "class AIMessage(BaseMessage):\n",
      "    \"\"\"Type of message that is spoken by the AI.\"\"\"\n",
      "\n",
      "    example: bool = False\n",
      "\n",
      "    @property\n",
      "    def type(self) -> str:\n",
      "        \"\"\"Type of the message, used for serialization.\"\"\"\n",
      "        return \"ai\"\n",
      "\n",
      "\n",
      "class SystemMessage(BaseMessage):\n",
      "    \"\"\"Type of message that is a system message.\"\"\"\n",
      "\n",
      "    @property\n",
      "    def type(self) -> str:\n",
      "        \"\"\"Type of the message, used for serialization.\"\"\"\n",
      "        return \"system\"\n",
      "\n",
      "\n",
      "class ChatMessage(BaseMessage):\n",
      "    \"\"\"Type of message with arbitrary speaker.\"\"\"\n",
      "\n",
      "    role: str\n",
      "\n",
      "    @property\n",
      "    def type(self) -> str:\n",
      "        \"\"\"Type of the message, used for serialization.\"\"\"\n",
      "        return \"chat\"\n",
      "\n",
      "\n",
      "def _message_to_dict(message: BaseMessage) -> dict:\n",
      "    return {\"type\": message.type, \"data\": message.dict()}\n",
      "\n",
      "\n",
      "def messages_to_dict(messages: List[BaseMessage]) -> List[dict]:\n",
      "    return [_message_to_dict(m) for m in messages]\n",
      "\n",
      "\n",
      "def _message_from_dict(message: dict) -> BaseMessage:\n",
      "    _type = message[\"type\"]\n",
      "    if _type == \"human\":\n",
      "        return HumanMessage(**message[\"data\"])\n",
      "    elif _type == \"ai\":\n",
      "        return AIMessage(**message[\"data\"])\n",
      "    elif _type == \"system\":\n",
      "        return SystemMessage(**message[\"data\"])\n",
      "    elif _type == \"chat\":\n",
      "        return ChatMessage(**message[\"data\"])\n",
      "    else:\n",
      "        raise ValueError(f\"Got unexpected type: {_type}\")\n",
      "\n",
      "\n",
      "def messages_from_dict(messages: List[dict]) -> List[BaseMessage]:\n",
      "    return [_message_from_dict(m) for m in messages]\n",
      "\n",
      "\n",
      "class ChatGeneration(Generation):\n",
      "    \"\"\"Output of a single generation.\"\"\"\n",
      "\n",
      "    text = \"\"\n",
      "    message: BaseMessage\n",
      "\n",
      "    @root_validator\n",
      "    def set_text(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
      "        values[\"text\"] = values[\"message\"].content\n",
      "        return values\n",
      "\n",
      "\n",
      "class ChatResult(BaseModel):\n",
      "    \"\"\"Class that contains all relevant information for a Chat Result.\"\"\"\n",
      "\n",
      "    generations: List[ChatGeneration]\n",
      "    \"\"\"List of the things generated.\"\"\"\n",
      "    llm_output: Optional[dict] = None\n",
      "    \"\"\"For arbitrary LLM provider specific output.\"\"\"\n",
      "\n",
      "\n",
      "class LLMResult(BaseModel):\n",
      "    \"\"\"Class that contains all relevant information for an LLM Result.\"\"\"\n",
      "\n",
      "    generations: List[List[Generation]]\n",
      "    \"\"\"List of the things generated. This is List[List[]] because\n",
      "    each input could have multiple generations.\"\"\"\n",
      "    llm_output: Optional[dict] = None\n",
      "    \"\"\"For arbitrary LLM provider specific output.\"\"\"\n",
      "\n",
      "\n",
      "class PromptValue(BaseModel, ABC):\n",
      "    @abstractmethod\n",
      "    def to_string(self) -> str:\n",
      "        \"\"\"Return prompt as string.\"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def to_messages(self) -> List[BaseMessage]:\n",
      "        \"\"\"Return prompt as messages.\"\"\"\n",
      "\n",
      "\n",
      "class BaseMemory(BaseModel, ABC):\n",
      "    \"\"\"Base interface for memory in chains.\"\"\"\n",
      "\n",
      "    class Config:\n",
      "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
      "\n",
      "        extra = Extra.forbid\n",
      "        arbitrary_types_allowed = True\n",
      "\n",
      "    @property\n",
      "    @abstractmethod\n",
      "    def memory_variables(self) -> List[str]:\n",
      "        \"\"\"Input keys this memory class will load dynamically.\"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
      "        \"\"\"Return key-value pairs given the text input to the chain.\n",
      "\n",
      "        If None, return all memories\n",
      "        \"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n",
      "        \"\"\"Save the context of this model run to memory.\"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def clear(self) -> None:\n",
      "        \"\"\"Clear memory contents.\"\"\"\n",
      "\n",
      "\n",
      "class BaseChatMessageHistory(ABC):\n",
      "    \"\"\"Base interface for chat message history\n",
      "    See `ChatMessageHistory` for default implementation.\n",
      "    \"\"\"\n",
      "\n",
      "    \"\"\"\n",
      "    Example:\n",
      "        .. code-block:: python\n",
      "\n",
      "            class FileChatMessageHistory(BaseChatMessageHistory):\n",
      "                storage_path:  str\n",
      "                session_id: str\n",
      "\n",
      "               @property\n",
      "               def messages(self):\n",
      "                   with open(os.path.join(storage_path, session_id), 'r:utf-8') as f:\n",
      "                       messages = json.loads(f.read())\n",
      "                    return messages_from_dict(messages)\n",
      "\n",
      "               def add_user_message(self, message: str):\n",
      "                   message_ = HumanMessage(content=message)\n",
      "                   messages = self.messages.append(_message_to_dict(_message))\n",
      "                   with open(os.path.join(storage_path, session_id), 'w') as f:\n",
      "                       json.dump(f, messages)\n",
      "\n",
      "               def add_ai_message(self, message: str):\n",
      "                   message_ = AIMessage(content=message)\n",
      "                   messages = self.messages.append(_message_to_dict(_message))\n",
      "                   with open(os.path.join(storage_path, session_id), 'w') as f:\n",
      "                       json.dump(f, messages)\n",
      "\n",
      "               def clear(self):\n",
      "                   with open(os.path.join(storage_path, session_id), 'w') as f:\n",
      "                       f.write(\"[]\")\n",
      "    \"\"\"\n",
      "\n",
      "    messages: List[BaseMessage]\n",
      "\n",
      "    @abstractmethod\n",
      "    def add_user_message(self, message: str) -> None:\n",
      "        \"\"\"Add a user message to the store\"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def add_ai_message(self, message: str) -> None:\n",
      "        \"\"\"Add an AI message to the store\"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def clear(self) -> None:\n",
      "        \"\"\"Remove all messages from the store\"\"\"\n",
      "\n",
      "\n",
      "class Document(BaseModel):\n",
      "    \"\"\"Interface for interacting with a document.\"\"\"\n",
      "\n",
      "    page_content: str\n",
      "    metadata: dict = Field(default_factory=dict)\n",
      "\n",
      "\n",
      "class BaseRetriever(ABC):\n",
      "    @abstractmethod\n",
      "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
      "        \"\"\"Get documents relevant for a query.\n",
      "\n",
      "        Args:\n",
      "            query: string to find relevant documents for\n",
      "\n",
      "        Returns:\n",
      "            List of relevant documents\n",
      "        \"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
      "        \"\"\"Get documents relevant for a query.\n",
      "\n",
      "        Args:\n",
      "            query: string to find relevant documents for\n",
      "\n",
      "        Returns:\n",
      "            List of relevant documents\n",
      "        \"\"\"\n",
      "\n",
      "\n",
      "# For backwards compatibility\n",
      "\n",
      "\n",
      "Memory = BaseMemory\n",
      "\n",
      "T = TypeVar(\"T\")\n",
      "\n",
      "\n",
      "class BaseOutputParser(BaseModel, ABC, Generic[T]):\n",
      "    \"\"\"Class to parse the output of an LLM call.\n",
      "\n",
      "    Output parsers help structure language model responses.\n",
      "    \"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def parse(self, text: str) -> T:\n",
      "        \"\"\"Parse the output of an LLM call.\n",
      "\n",
      "        A method which takes in a string (assumed output of language model )\n",
      "        and parses it into some structure.\n",
      "\n",
      "        Args:\n",
      "            text: output of language model\n",
      "\n",
      "        Returns:\n",
      "            structured output\n",
      "        \"\"\"\n",
      "\n",
      "    def parse_with_prompt(self, completion: str, prompt: PromptValue) -> Any:\n",
      "        \"\"\"Optional method to parse the output of an LLM call with a prompt.\n",
      "\n",
      "        The prompt is largely provided in the event the OutputParser wants\n",
      "        to retry or fix the output in some way, and needs information from\n",
      "        the prompt to do so.\n",
      "\n",
      "        Args:\n",
      "            completion: output of language model\n",
      "            prompt: prompt value\n",
      "\n",
      "        Returns:\n",
      "            structured output\n",
      "        \"\"\"\n",
      "        return self.parse(completion)\n",
      "\n",
      "    def get_format_instructions(self) -> str:\n",
      "        \"\"\"Instructions on how the LLM output should be formatted.\"\"\"\n",
      "        raise NotImplementedError\n",
      "\n",
      "    @property\n",
      "    def _type(self) -> str:\n",
      "        \"\"\"Return the type key.\"\"\"\n",
      "        raise NotImplementedError(\n",
      "            f\"_type property is not implemented in class {self.__class__.__name__}.\"\n",
      "            \" This is required for serialization.\"\n",
      "        )\n",
      "\n",
      "    def dict(self, **kwargs: Any) -> Dict:\n",
      "        \"\"\"Return dictionary representation of output parser.\"\"\"\n",
      "        output_parser_dict = super().dict()\n",
      "        output_parser_dict[\"_type\"] = self._type\n",
      "        return output_parser_dict\n",
      "\n",
      "\n",
      "class OutputParserException(Exception):\n",
      "    \"\"\"Exception that output parsers should raise to signify a parsing error.\n",
      "\n",
      "    This exists to differentiate parsing errors from other code or execution errors\n",
      "    that also may arise inside the output parser. OutputParserExceptions will be\n",
      "    available to catch and handle in ways to fix the parsing error, while other\n",
      "    errors will be raised.\n",
      "    \"\"\"\n",
      "\n",
      "    pass\n",
      "\n",
      "\n",
      "class BaseDocumentTransformer(ABC):\n",
      "    \"\"\"Base interface for transforming documents.\"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def transform_documents(\n",
      "        self, documents: Sequence[Document], **kwargs: Any\n",
      "    ) -> Sequence[Document]:\n",
      "        \"\"\"Transform a list of documents.\"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    async def atransform_documents(\n",
      "        self, documents: Sequence[Document], **kwargs: Any\n",
      "    ) -> Sequence[Document]:\n",
      "        \"\"\"Asynchronously transform a list of documents.\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from langchain import schema\n",
    "\n",
    "code = inspect.getsource(schema)\n",
    "print(code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the simple code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.concise.config import get_default_max_tokens\n",
    "\n",
    "\n",
    "get_default_max_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_NO_RULE_MATCH' from 'langchain.concise.rulex' (/home/jacob/github/langchain/langchain/concise/rulex.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconcise\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrulex\u001b[39;00m \u001b[39mimport\u001b[39;00m _NO_RULE_MATCH\n\u001b[1;32m      3\u001b[0m _NO_RULE_MATCH\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_NO_RULE_MATCH' from 'langchain.concise.rulex' (/home/jacob/github/langchain/langchain/concise/rulex.py)"
     ]
    }
   ],
   "source": [
    "from langchain.concise.rulex import _NO_RULE_MATCH\n",
    "\n",
    "_NO_RULE_MATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'RulEx' has no attribute '_NO_RULE_MATCH'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconcise\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrulex\u001b[39;00m \u001b[39mimport\u001b[39;00m RulEx\n\u001b[0;32m----> 4\u001b[0m ru \u001b[39m=\u001b[39m RulEx\u001b[39m.\u001b[39;49mcreate(rules\u001b[39m=\u001b[39;49mrules)\n\u001b[1;32m      6\u001b[0m simplified_code \u001b[39m=\u001b[39m ru(code)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(simplified_code)\n",
      "File \u001b[0;32m~/github/langchain/langchain/concise/rulex.py:52\u001b[0m, in \u001b[0;36mRulEx.create\u001b[0;34m(cls, rules, text_splitter, llm)\u001b[0m\n\u001b[1;32m     49\u001b[0m llm \u001b[39m=\u001b[39m llm \u001b[39mor\u001b[39;00m get_default_model()\n\u001b[1;32m     50\u001b[0m rules \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_parse_rules(rules, llm)\n\u001b[1;32m     51\u001b[0m choice_parser \u001b[39m=\u001b[39m ChoiceOutputParser(\n\u001b[0;32m---> 52\u001b[0m     options\u001b[39m=\u001b[39m[rule\u001b[39m.\u001b[39mpattern \u001b[39mfor\u001b[39;00m rule \u001b[39min\u001b[39;00m rules] \u001b[39m+\u001b[39m [\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_NO_RULE_MATCH], llm\u001b[39m=\u001b[39mllm\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[1;32m     55\u001b[0m     rules\u001b[39m=\u001b[39mrules, text_splitter\u001b[39m=\u001b[39mtext_splitter, choice_parser\u001b[39m=\u001b[39mchoice_parser, llm\u001b[39m=\u001b[39mllm\n\u001b[1;32m     56\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'RulEx' has no attribute '_NO_RULE_MATCH'"
     ]
    }
   ],
   "source": [
    "from langchain.concise.rulex import RulEx\n",
    "\n",
    "\n",
    "ru = RulEx.create(rules=rules)\n",
    "\n",
    "simplified_code = ru(code)\n",
    "\n",
    "print(simplified_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retaining flexbillity\n",
    "\n",
    "Even though the `concise` API is, well, concise, it's still flexible. For example, you can pass a custom `LLM` or `TextSplitter` to any function that uses it. You can also change the default `LLM`, `TextSplitter`, and max tokens in `langchain.concise.config`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1677b440931f40d89ef8be7bf03acb108ce003de0ac9b18e8d43753ea2e7103"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
