{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%colors nocolor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "In this tutorial, we will learn about LangChain's concise API. We will cover:\n",
    "- Using the `generate`, `decide`, and `choice` functions to generate text, construct python objects, make decision, and select options.\n",
    "- Using the `template` and `gemplate` functions to create reusable text templators and semantic kernels.\n",
    "- Using `rulex` to perform semantic pattern matching and replacement.\n",
    "\n",
    "The `langchain.concise` submodule contains the following functions\n",
    "- choice.py  which provides a function for choosing an option from a list of options based on a query and examples.\n",
    "- chunk.py  which splits text into smaller chunks.\n",
    "- config.py  which provides functions for setting and getting default values for the language model, text splitter, and maximum tokens.\n",
    "- decide.py  which determines whether a statement is true or false based on a query and examples.\n",
    "- gemplate.py which defines a function for creating reusable semantic kernels.\n",
    "- generate.py  which generates text using a language model and provides convenience options for parsing, removing quotes, and retrying failed attempts.\n",
    "- pattern.py which provides a function for prompting an LLM to complete a pattern.\n",
    "- rulex.py  which provides a class for defining natural language replacement rules for text.\n",
    "- template.py  which defines a function for creating reusable text templators."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need a concise API?\n",
    "\n",
    "The concise API provides many one-liners for common use cases. For example, before the concise API existed, generating a templated completion involved several steps:\n",
    "1. creates a prompt template,\n",
    "2. renders it into a prompt,\n",
    "3. formats it with variables,\n",
    "4. constructs an LLM,\n",
    "5. calls the llm with the formatted prompt\n",
    "6. and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# api_key = getpass('Enter your API key here: ')\n",
    "os.environ['OPENAI_API_KEY'] = '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' # api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Hello, my name is John. What is your name?\n",
      "output: \n",
      "\n",
      "My name is Mary. Nice to meet you, John.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "prompt_template = 'Hello, my name is {name}. What is your name?'\n",
    "character_0_prompt = PromptTemplate(template=prompt_template, input_variables=['name'])\n",
    "prompt_str = character_0_prompt.format(name='John')\n",
    "print('input:', prompt_str)\n",
    "llm = OpenAI(verbose=False, cache=False)\n",
    "output = llm(prompt_str)\n",
    "print('output:', output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Albiet, 1-3 could be combined with f strings, but still, that's a lot of steps for developers to learn. Now, see how the concise API makes prompting much more concise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain.concise import generate\n",
    "\n",
    "country = 'France'\n",
    "output = generate(f\"What is the capital of {country}?\")\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many use cases, the concise API is all you need. However, if you need more control, you can use the full API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Must-learn: `generate`\n",
    "\n",
    "If there's one function in LangChain's concise API that you must learn, it's `generate`. `generate` is the Swiss Army knife of text generation. It can be used to generate text, construct python objects, make decisions, and select options. Let's see how it works.\n",
    "\n",
    "### Generating text\n",
    "\n",
    "The simplest use case for `generate` is to generate text. Let's generate a sentence about a dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A dog.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"What has four legs and barks?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `generate` produced a sentence about a dog. It's that simple. No templates, no prompts, no 5 hours learning langchain abstractions. \n",
    "\n",
    "### Generating python objects\n",
    "\n",
    "But what if we want to actually *generate* a dog? Well, we can do that too. Let's generate a dog. To do this, we'll need to first define a class for dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Dog(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the dog\")\n",
    "    age: int = Field(..., description=\"Age of the dog\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we have to do is pass the class to `generate` using the `type` parameter, and it will generate a dog for us! Like so,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123123\n",
      "456456\n",
      "789789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dog(name='Spot', age=5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"Make a dog named Spot that is 5 years old.\", type=Dog)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use generate on any `pydantic` model. If you're not familiar with `pydantic`, it's a library for defining data models in python. It's used by FastAPI and many other libraries. You can learn more about it here: https://pydantic-docs.helpmanual.io/.\n",
    "\n",
    "You can likewise generate `int`'s, `float`'s, and `bool`'s by passing the type to `generate`. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123123\n",
      "456456\n",
      "789789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"My name is Sam and I am 24 years old. How old am I?\", type=int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123123\n",
      "456456\n",
      "789789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"What is 2 divided by 3? Round to the hundredths place.\", type=float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating other objects\n",
    "\n",
    "Not all LLM outputs are best parsed into pydantic models. For example, if we want to generate a list of dogs, we can't use a pydantic model because pydantic models are for single objects. However, we can manually instantiate an `ItemParsedListOutputParser` and pass that to the `parser` arg in `generate`. Let's see how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123123\n",
      "456456\n",
      "789789\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'e' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "---------------------------------------------------------------------------",
      "JSONDecodeError                           Traceback (most recent call last)",
      "File ~/github/langchain/langchain/output_parsers/pydantic.py:25, in PydanticOutputParser.parse(self, text)\n     24     json_str = match.group()\n---> 25 json_object = json.loads(json_str)\n     26 return self.pydantic_object.parse_obj(json_object)\n",
      "File ~/.pyenv/versions/3.10.9/lib/python3.10/json/__init__.py:346, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    343 if (cls is None and object_hook is None and\n    344         parse_int is None and parse_float is None and\n    345         parse_constant is None and object_pairs_hook is None and not kw):\n--> 346     return _default_decoder.decode(s)\n    347 if cls is None:\n",
      "File ~/.pyenv/versions/3.10.9/lib/python3.10/json/decoder.py:337, in JSONDecoder.decode(self, s, _w)\n    333 \"\"\"Return the Python representation of ``s`` (a ``str`` instance\n    334 containing a JSON document).\n    335 \n    336 \"\"\"\n--> 337 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n    338 end = _w(s, end).end()\n",
      "File ~/.pyenv/versions/3.10.9/lib/python3.10/json/decoder.py:355, in JSONDecoder.raw_decode(self, s, idx)\n    354 except StopIteration as err:\n--> 355     raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n    356 return obj, end\n",
      "JSONDecodeError: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "OutputParserException                     Traceback (most recent call last)",
      "File ~/github/langchain/langchain/output_parsers/retry.py:166, in MultiAttemptRetryWithErrorOutputParser.parse_with_prompt(self, completion, prompt_value)\n    165 try:\n--> 166     parsed_completion = self.parser.parse(completion)\n    167     if self.additional_validator is not None:\n",
      "File ~/github/langchain/langchain/output_parsers/item_parsed_list.py:20, in ItemParsedListOutputParser.parse(self, response)\n     19 else:\n---> 20     return [self.item_parser.parse(item) for item in items]\n",
      "File ~/github/langchain/langchain/output_parsers/item_parsed_list.py:20, in <listcomp>(.0)\n     19 else:\n---> 20     return [self.item_parser.parse(item) for item in items]\n",
      "File ~/github/langchain/langchain/output_parsers/pydantic.py:31, in PydanticOutputParser.parse(self, text)\n     30 msg = f\"Failed to parse {name} from completion {text}. Got: {e}\"\n---> 31 raise OutputParserException(msg)\n",
      "OutputParserException: Failed to parse Dog from completion {. Got: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "UnboundLocalError                         Traceback (most recent call last)",
      "Cell In[10], line 7\n      4 dog_parser = PydanticOutputParser(pydantic_object=Dog)\n      5 dog_list_parser = ItemParsedListOutputParser(item_parser=dog_parser)\n----> 7 generate(\"Generate 3 dogs. You pick the names and ages.\", parser=dog_list_parser, attempts=5)\n",
      "File ~/github/langchain/langchain/concise/generate.py:143, in generate(input, parser, type, llm, stop, attempts, additional_validator, remove_quotes)\n    141 response = _generate(input, llm=llm, stop=stop)\n    142 print(789789)\n--> 143 return retry_with_error_parser.parse_with_prompt(\n    144     response,\n    145     prompt_value=StringPromptValue(text=input)\n    146     if isinstance(input, str)\n    147     else ChatPromptValue(messages=input)\n    148     if isinstance(input, list)\n    149     else None,\n    150 )\n",
      "File ~/github/langchain/langchain/output_parsers/retry.py:175, in MultiAttemptRetryWithErrorOutputParser.parse_with_prompt(self, completion, prompt_value)\n    170 for _ in range(self.attempts):\n    171     try:\n    172         new_completion = self.retry_chain.run(\n    173             prompt=prompt_value.to_string(),\n    174             completion=completion,\n--> 175             error=repr(e),\n    176         )\n    177         parsed_completion = self.parser.parse_with_prompt(\n    178             new_completion, StringPromptValue(text=prompt_value.to_string())\n    179         )\n    180         if self.additional_validator is not None:\n",
      "UnboundLocalError: local variable 'e' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers.item_parsed_list import ItemParsedListOutputParser\n",
    "from langchain.output_parsers.pydantic import PydanticOutputParser\n",
    "\n",
    "dog_parser = PydanticOutputParser(pydantic_object=Dog)\n",
    "dog_list_parser = ItemParsedListOutputParser(item_parser=dog_parser)\n",
    "\n",
    "generate(\"Generate 3 dogs. You pick the names and ages.\", parser=dog_list_parser, attempts=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metaprompting\n",
    "\n",
    "`generate`'s conciseness makes it very convenient for meta-prompting. Check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123123\n",
      "456456\n",
      "789789\n",
      "Ethan meta meta prompt: Generate a character bio for Ethan. Address them in the 2nd person, eg, 'You are _. You love _. ...'. Tell the boy who they are. What their name is. Their likes, dislike, emotions, etc, etc. Begin by stating: 'You are a...'.\n",
      "Ethan meta prompt: Generate a character bio for Ethan. Address him in the 2nd person, eg, 'You are _. You love _. ...'. Tell the boy who he is. What his name is. His likes, dislikes, emotions, etc, etc. Begin by stating: 'You are a...'.\n",
      "Ethan prompt: You are a 12-year-old boy named Ethan. You love playing video games and spending time with your friends. You have a passion for science and love learning about new things. You are always curious about how things work and enjoy experimenting with different ideas.\n",
      "\n",
      "You dislike getting up early in the morning and doing homework. You sometimes feel overwhelmed by schoolwork and struggle to manage your time effectively. You get easily frustrated when things don't go as planned and tend to be hard on yourself when you make mistakes.\n",
      "\n",
      "Despite these challenges, you have a kind heart and love to help others. You are loyal to your friends and family and will do anything to make them happy. You enjoy making people laugh and have a good sense of humor. You can be shy around new people, but once you get to know someone, you are outgoing and friendly.\n",
      "\n",
      "You have big dreams for the future and hope to one day become a scientist or engineer. You believe that anything is possible with hard work and determination and are not afraid to take risks. You are a brave and adventurous young man who is always up for a challenge.\n",
      "Sophie Johnson message: Hello, my name is Sophie Johnson. I am 16 years old. My sister is 5 years younger than me (She's 11 years old). I am a student at Summit High School. I like to go for long walks in nature and listen to the birds chirping and the sound of leaves rustling in the wind.. What are your hobbies and interests? \n",
      "Have you traveled anywhere interesting recently? \n",
      "What's your favorite type of food? \n",
      "What's your favorite movie or TV show? \n",
      "Do you have any pets? \n",
      "What's your favorite book or author? \n",
      "What do you like to do for fun? \n",
      "What are your plans for the weekend? \n",
      "What kind of music do you enjoy? \n",
      "Do you have any siblings? \n",
      "What's your favorite thing to do in your free time? \n",
      "What's something new you've learned recently? \n",
      "What's your favorite place you've ever visited?\n",
      "Ethan response: As an AI language model, I don't have personal hobbies and interests, but I can tell you that I enjoy learning new things and expanding my knowledge base. \n",
      "\n",
      "I haven't traveled anywhere recently, but I have information on various places around the world. If you need any help planning a trip, just let me know! \n",
      "\n",
      "As an AI language model, I don't have a favorite type of food, movie, or TV show, but I can provide recommendations based on your preferences. \n",
      "\n",
      "I don't have any pets, but I can provide information on different types of pets and how to take care of them. \n",
      "\n",
      "As an AI language model, I don't have a favorite book or author, but I can suggest books based on your interests. \n",
      "\n",
      "For fun, I like to learn new things and explore different topics. \n",
      "\n",
      "As an AI language model, I don't have plans for the weekend, but I can suggest activities or events based on your location and interests. \n",
      "\n",
      "I can provide information on different types of music and recommend artists based on your preferences. \n",
      "\n",
      "As an AI language model, I don't have siblings, but I can provide information on how to handle different family situations. \n",
      "\n",
      "In my free time, I like to learn about new topics and ways to improve my language skills. \n",
      "\n",
      "As an AI language model, I don't have personal experiences, but I can provide information on various topics and subjects. \n",
      "\n",
      "As an AI language model, I don't have the ability to visit places physically, but I can provide information on different destinations and recommend places to visit based on your preferences.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from textwrap import dedent\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "genders = ['boy', 'girl']\n",
    "random.shuffle(genders)\n",
    "age = generate(f\"Generate an age for a technically inclined high school {genders[0]}.\", type=int)\n",
    "\n",
    "character_0 = generate(f\"Generate a name for an {age} {genders[0]} (just first name): \")\n",
    "character_0_meta_meta_prompt = f\"Generate a character bio for {character_0}. Address them in the 2nd person, eg, 'You are _. You love _. ...'. Tell the {genders[0]} who they are. What their name is. Their likes, dislike, emotions, etc, etc. Begin by stating: 'You are a...'.\"\n",
    "print(f'{character_0} meta meta prompt:', character_0_meta_meta_prompt)\n",
    "character_0_meta_prompt = generate(dedent(\n",
    "    f\"\"\"\n",
    "    Instructions: Rewrite the prompt with pronouns changed from them/their into the standard form for a {genders[0]}.\n",
    "    \n",
    "    Input: {character_0_meta_meta_prompt}\n",
    "    \n",
    "    Output: \n",
    "    \"\"\"\n",
    "    )\n",
    ")\n",
    "print(f'{character_0} meta prompt:', character_0_meta_prompt)\n",
    "character_0_prompt = generate(character_0_meta_prompt)\n",
    "print(f'{character_0} prompt:', character_0_prompt)\n",
    "\n",
    "character_1 = generate(f\"Generate a name for a {age}-year-old {genders[1]}.\")\n",
    "character_1_message = f\"\"\"Hello, my name is {character_1}. I am {age} years old. My sister is 5 years younger than me (She's {age-5} years old). I am a student at {generate(\"Generate a name for a high school.\")}. {generate(\"Write an 'I like to' statement. You can decide whatever it is.\")}. {generate(\"Ask the kind of question that make for good socialization, eg, do you like ...? what do you think about ...?\")}\"\"\"\n",
    "print(f'{character_1} message:', character_1_message)\n",
    "\n",
    "response = chat_model([\n",
    "    SystemMessage(content=character_0_prompt),\n",
    "    HumanMessage(content=character_1_message)\n",
    "    ])\n",
    "print(f'{character_0} response:', response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we generate a system persona on the fly. The simulated user generated several of their characteristics on the fly as well, including a sentence about the user's likes.\n",
    "\n",
    "Perhaps a more practical example for LLM developers like yourself is using LLMs to generate prompts for other LLMs. Let's see how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  Generate a non-puppy dog named Spot.\n",
      "123123\n",
      "456456\n",
      "789789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dog(name='Spot', age=3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "\n",
    "character_0_prompt = generate(\n",
    "    dedent(\n",
    "        \"\"\"\n",
    "        ## Introduction\n",
    "        \n",
    "        You are an expert prompt engineer.\n",
    "        \n",
    "        ## Task\n",
    "        \n",
    "        You are writing a prompt to generate a dog.\n",
    "        - the dog should be named Spot.\n",
    "        - I don't care how old the dog is as long as its old enought to not be a puppy.\n",
    "        \n",
    "        ## Notes\n",
    "        \n",
    "        Remember, tokens are expensive. So try to use the least amount of tokens possible. Write the prompt.\n",
    "        \n",
    "        ## Output\n",
    "\"\"\"))\n",
    "print('Prompt: ', character_0_prompt)\n",
    "output = generate(character_0_prompt, type=Dog)\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making decisions and selecting options\n",
    "\n",
    "See how Langchain's concise API can intergrate deeper into you code with `decide` and `choice`. These functions return a boolean and an option respectively. Let's see how they work.\n",
    "\n",
    "We'll start with `decide`. `decide` takes a query and a list of examples. It returns a boolean indicating whether the query is true or false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='ge')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.base import StringPromptValue\n",
    "\n",
    "StringPromptValue(text='ge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123123\n",
      "456456\n",
      "789789\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "BooleanOutputParser expected output value to either be YES or NO. Received Yes, the prompt is clear and descriptive. It specifies that the dog should not be a puppy and that its name should be Spot..",
     "output_type": "error",
     "traceback": [
      "---------------------------------------------------------------------------",
      "ValueError                                Traceback (most recent call last)",
      "Cell In[13], line 4\n      1 from langchain.concise import decide\n----> 4 value = decide(character_0_prompt, query=\"Was this prompt descriptive enough?\")\n      5 value\n",
      "File ~/github/langchain/langchain/concise/decide.py:38, in decide(input, query, true_examples, false_examples, examples, llm)\n     32 # Rephrase the outputs in the vernacular of the boolean parser.\n     33 examples = [\n     34     (input, parser.true_val if output else parser.false_val)\n     35     for input, output in examples\n     36 ]\n---> 38 return pattern(\n     39     input=input,\n     40     query=query,\n     41     pattern_name=\"decide\",\n     42     parser=parser,\n     43     examples=examples,\n     44     llm=llm,\n     45 )\n",
      "File ~/github/langchain/langchain/concise/pattern.py:40, in pattern(input, query, pattern_name, input_format_template, output_format_template, parser, examples, llm)\n     30 assert all(\n     31     len(example) == 3 for example in examples\n     32 ), \"Examples must be a list of tuples of length 3.\"\n     33 input_msgs = render_prompt_and_examples(\n     34     system_prompt=query,\n     35     input_prompt_template=input_format_template,\n   (...)\n     38     examples=examples,\n     39 )\n---> 40 return generate(input_msgs, llm=llm, parser=parser)\n",
      "File ~/github/langchain/langchain/concise/generate.py:143, in generate(input, parser, type, llm, stop, attempts, additional_validator, remove_quotes)\n    141 response = _generate(input, llm=llm, stop=stop)\n    142 print(789789)\n--> 143 return retry_with_error_parser.parse_with_prompt(\n    144     response,\n    145     prompt_value=StringPromptValue(text=input)\n    146     if isinstance(input, str)\n    147     else ChatPromptValue(messages=input)\n    148     if isinstance(input, list)\n    149     else None,\n    150 )\n",
      "File ~/github/langchain/langchain/output_parsers/retry.py:166, in MultiAttemptRetryWithErrorOutputParser.parse_with_prompt(self, completion, prompt_value)\n    164 def parse_with_prompt(self, completion: str, prompt_value: PromptValue) -> T:\n    165     try:\n--> 166         parsed_completion = self.parser.parse(completion)\n    167         if self.additional_validator is not None:\n    168             self.additional_validator(parsed_completion)\n",
      "File ~/github/langchain/langchain/output_parsers/boolean.py:20, in BooleanOutputParser.parse(self, text)\n     18 cleaned_text = text.strip()\n     19 if cleaned_text not in (self.true_val, self.false_val):\n---> 20     raise ValueError(\n     21         f\"BooleanOutputParser expected output value to either be \"\n     22         f\"{self.true_val} or {self.false_val}. Received {cleaned_text}.\"\n     23     )\n     24 return cleaned_text == self.true_val\n",
      "ValueError: BooleanOutputParser expected output value to either be YES or NO. Received Yes, the prompt is clear and descriptive. It specifies that the dog should not be a puppy and that its name should be Spot.."
     ]
    }
   ],
   "source": [
    "from langchain.concise import decide\n",
    "\n",
    "\n",
    "value = decide(character_0_prompt, query=\"Was this prompt descriptive enough?\")\n",
    "value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since its signature is so concise, you can integrate `decide` directly into your code. For example, you can use it to make decisions about whether to take a certain branch of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decide' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m result \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m decide(result, query\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIs this a good story?\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      3\u001b[0m     result \u001b[39m=\u001b[39m generate(\u001b[39m'\u001b[39m\u001b[39mWrite a short funny story\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decide' is not defined"
     ]
    }
   ],
   "source": [
    "result = ''\n",
    "while not decide(result, query=\"Is this a good story?\"):\n",
    "    result = generate('Write a short funny story')\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a natural language game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from langchain.output_parsers\n",
    "\n",
    "position = (0,0)\n",
    "map_size = (10,10)\n",
    "\n",
    "while True:\n",
    "    print(f'You are at position {position} on a map of size {map_size}. What do you do?')\n",
    "    user = input(\">>> \")\n",
    "    if decide('Did the user ask a question?', input=user):\n",
    "        print(generate(f\"Answer the user's question.\\n\\nUser: {user}\\nAI: \"))\n",
    "    elif decide('Did the user make a move?', input=user):\n",
    "        delta = static_eval(generate(f\"Translate the natural language motion into a delta position in tuple format (dx, dy)\\n\\nNatural language: {user}\\nTuple format: \"))\n",
    "        position = (position[0] + delta[0], position[1] + delta[1])\n",
    "        position = (max(0, min(map_size[0], position[0])), max(0, min(map_size[1], position[1])))\n",
    "    else:\n",
    "        print(generate(f'Apolagize for not understanding the user.\\n\\nUser: {user}\\nAI: '))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, `choice` gives the LLM the ability to select an option from a list of options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "no validator found for <class 'langchain.text_splitter.TextSplitter'>, see `arbitrary_types_allowed` in Config",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconcise\u001b[39;00m \u001b[39mimport\u001b[39;00m choice\n\u001b[1;32m      4\u001b[0m flavor \u001b[39m=\u001b[39m choice(\u001b[39m\"\u001b[39m\u001b[39mSam loves ice cream. He enjoy chocolate and vanilla, but he really really loves strawberry.\u001b[39m\u001b[39m\"\u001b[39m, query\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPick Sams favorite color\u001b[39m\u001b[39m\"\u001b[39m, options\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mchocolate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvanilla\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstrawberry\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(flavor)\n",
      "File \u001b[0;32m~/github/langchain/langchain/concise/__init__.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconcise\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgemplate\u001b[39;00m \u001b[39mimport\u001b[39;00m gemplate\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconcise\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgenerate\u001b[39;00m \u001b[39mimport\u001b[39;00m generate\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconcise\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrulex\u001b[39;00m \u001b[39mimport\u001b[39;00m Rule\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconcise\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrulex\u001b[39;00m \u001b[39mimport\u001b[39;00m RulEx\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconcise\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtemplate\u001b[39;00m \u001b[39mimport\u001b[39;00m template\n",
      "File \u001b[0;32m~/github/langchain/langchain/concise/rulex.py:40\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[39mreturn\u001b[39;00m v\n\u001b[1;32m     36\u001b[0m \u001b[39m#    def __str__(self) -> str:\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m#        return f\"{self.name}: {self.pattern} -> {self.replacement}\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mRulEx\u001b[39;00m(BaseModel):\n\u001b[1;32m     41\u001b[0m     _NO_RULE_MATCH \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNo match\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m     rules: \u001b[39mlist\u001b[39m[Rule]\n",
      "File \u001b[0;32m~/github/langchain/.venv/lib/python3.10/site-packages/pydantic/main.py:197\u001b[0m, in \u001b[0;36mpydantic.main.ModelMetaclass.__new__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/github/langchain/.venv/lib/python3.10/site-packages/pydantic/fields.py:506\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/github/langchain/.venv/lib/python3.10/site-packages/pydantic/fields.py:436\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/github/langchain/.venv/lib/python3.10/site-packages/pydantic/fields.py:557\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.prepare\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/github/langchain/.venv/lib/python3.10/site-packages/pydantic/fields.py:831\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.populate_validators\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/github/langchain/.venv/lib/python3.10/site-packages/pydantic/validators.py:765\u001b[0m, in \u001b[0;36mfind_validators\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: no validator found for <class 'langchain.text_splitter.TextSplitter'>, see `arbitrary_types_allowed` in Config"
     ]
    }
   ],
   "source": [
    "from langchain.concise import choice\n",
    "\n",
    "\n",
    "flavor = choice(\"Sam loves ice cream. He enjoy chocolate and vanilla, but he really really loves strawberry.\", query=\"Pick Sams favorite color\", options=['chocolate', 'vanilla', 'strawberry'])\n",
    "print(flavor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And likewise for Enum's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Color(Enum):\n",
    "    red = \"red\"\n",
    "    yellow = \"yellow\"\n",
    "    green = \"green\"\n",
    "    blue = \"blue\"\n",
    "\n",
    "user_name = \"Steeve\"\n",
    "user_dossier = \"Steve is our 10 year loyal customer. He is a big fan of our product and has been a great advocate for us. He drives a red truck and loves to eat pizza.\"\n",
    "\n",
    "match choice(f\"What color product is {user_name} most likely to buy?\", options=Color):\n",
    "    case Color.red:\n",
    "        ...\n",
    "    case Color.yellow:\n",
    "        ...\n",
    "    case Color.green:\n",
    "        ...\n",
    "    case Color.blue:\n",
    "        ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Templates and Gemplates\n",
    "\n",
    "Now let's explore some of the more cutting edge features of the concise API. We'll start with templates. Templates are statefull text templators. They are useful for keeping track of pronouns when generating text, such as prompts for LLMs. Let's see how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatMessagePromptTemplate\nrole\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconcise\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtemplate\u001b[39;00m \u001b[39mimport\u001b[39;00m template\n\u001b[1;32m      4\u001b[0m t \u001b[39m=\u001b[39m template(\u001b[39m\"\u001b[39m\u001b[39mYou are \u001b[39m\u001b[39m{role}\u001b[39;00m\u001b[39mGPT. You can \u001b[39m\u001b[39m{capabilities}\u001b[39;00m\u001b[39m. If you do not understand the user, you should \u001b[39m\u001b[39m{fallback}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(t(role\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchitchat\u001b[39m\u001b[39m\"\u001b[39m, capabilities\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchat about anything\u001b[39m\u001b[39m\"\u001b[39m, fallback\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapologize and ask for clarification\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/github/langchain/langchain/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimportlib\u001b[39;00m \u001b[39mimport\u001b[39;00m metadata\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m \u001b[39mimport\u001b[39;00m MRKLChain, ReActChain, SelfAskWithSearchChain\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcache\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseCache\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     ConversationChain,\n\u001b[1;32m     10\u001b[0m     LLMBashChain,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     VectorDBQAWithSourcesChain,\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/__init__.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconversational\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversationalAgent\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconversational_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversationalChatAgent\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minitialize\u001b[39;00m \u001b[39mimport\u001b[39;00m initialize_agent\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload_tools\u001b[39;00m \u001b[39mimport\u001b[39;00m get_all_tool_names, load_tools\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloading\u001b[39;00m \u001b[39mimport\u001b[39;00m load_agent\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/initialize.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentExecutor\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent_types\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentType\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloading\u001b[39;00m \u001b[39mimport\u001b[39;00m AGENT_TO_CLASS, load_agent\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseCallbackManager\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/loading.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseSingleActionAgent\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m Tool\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m AGENT_TO_CLASS\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloading\u001b[39;00m \u001b[39mimport\u001b[39;00m load_chain, load_chain_from_config\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/types.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreact\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ReActDocstoreAgent\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mself_ask_with_search\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m SelfAskWithSearchAgent\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m StructuredChatAgent\n\u001b[1;32m     13\u001b[0m AGENT_TO_CLASS: Dict[AgentType, Type[BaseSingleActionAgent]] \u001b[39m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     AgentType\u001b[39m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION: ZeroShotAgent,\n\u001b[1;32m     15\u001b[0m     AgentType\u001b[39m.\u001b[39mREACT_DOCSTORE: ReActDocstoreAgent,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     AgentType\u001b[39m.\u001b[39mSTRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION: StructuredChatAgent,\n\u001b[1;32m     21\u001b[0m }\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/structured_chat/base.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydantic\u001b[39;00m \u001b[39mimport\u001b[39;00m Field\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m Agent, AgentOutputParser\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parser\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     StructuredChatOutputParserWithRetries,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompt\u001b[39;00m \u001b[39mimport\u001b[39;00m FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/structured_chat/output_parser.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompt\u001b[39;00m \u001b[39mimport\u001b[39;00m FORMAT_INSTRUCTIONS\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m \u001b[39mimport\u001b[39;00m OutputFixingParser\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentAction, AgentFinish, OutputParserException\n\u001b[1;32m     16\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/github/langchain/langchain/output_parsers/__init__.py:21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mremove_quotes\u001b[39;00m \u001b[39mimport\u001b[39;00m RemoveQuotesOutputParser\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mretry\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     MultiAttemptRetryWithErrorOutputParser,\n\u001b[1;32m     18\u001b[0m     RetryOutputParser,\n\u001b[1;32m     19\u001b[0m     RetryWithErrorOutputParser,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstitched\u001b[39;00m \u001b[39mimport\u001b[39;00m StitchedOutputParser\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured\u001b[39;00m \u001b[39mimport\u001b[39;00m ResponseSchema, StructuredOutputParser\n\u001b[1;32m     24\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mBooleanOutputParser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mRegexParser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mStitchedOutputParser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m ]\n",
      "File \u001b[0;32m~/github/langchain/langchain/output_parsers/stitched.py:32\u001b[0m\n\u001b[1;32m     21\u001b[0m MERGE_INCOMPLETE_RESPONSES_PROMPT_TEMPLATE \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[39mPlease stitch the following inputs together into a single coherent response:\u001b[39m\n\u001b[1;32m     23\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[39mCombined: \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     28\u001b[0m CONTINUE_INCOMPLETE_PLEASE_CONTINUE_PROMPT_TEMPLATE \u001b[39m=\u001b[39m (\n\u001b[1;32m     29\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSorry, your response was incomplete. Please finish your response:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 32\u001b[0m SYSTEM_PROMPT \u001b[39m=\u001b[39m ChatMessagePromptTemplate\u001b[39m.\u001b[39;49mfrom_template(template\u001b[39m=\u001b[39;49mSYSTEM_PROMPT_TEMPLATE)\n\u001b[1;32m     33\u001b[0m CONTINUE_INCOMPLETE_PROMPT \u001b[39m=\u001b[39m HumanMessagePromptTemplate\u001b[39m.\u001b[39mfrom_template(\n\u001b[1;32m     34\u001b[0m     template\u001b[39m=\u001b[39mCONTINUE_INCOMPLETE_PROMPT_TEMPLATE\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m MERGE_INCOMPLETE_RESPONSES_PROMPT \u001b[39m=\u001b[39m HumanMessagePromptTemplate\u001b[39m.\u001b[39mfrom_template(\n\u001b[1;32m     37\u001b[0m     template\u001b[39m=\u001b[39mMERGE_INCOMPLETE_RESPONSES_PROMPT_TEMPLATE\n\u001b[1;32m     38\u001b[0m )\n",
      "File \u001b[0;32m~/github/langchain/langchain/prompts/chat.py:68\u001b[0m, in \u001b[0;36mBaseStringMessagePromptTemplate.from_template\u001b[0;34m(cls, template, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_template\u001b[39m(\u001b[39mcls\u001b[39m, template: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessagePromptTemplate:\n\u001b[1;32m     67\u001b[0m     prompt \u001b[39m=\u001b[39m PromptTemplate\u001b[39m.\u001b[39mfrom_template(template)\n\u001b[0;32m---> 68\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(prompt\u001b[39m=\u001b[39;49mprompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/github/langchain/.venv/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatMessagePromptTemplate\nrole\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "from langchain.concise.template import template\n",
    "\n",
    "\n",
    "t = template(\"You are {role}GPT. You can {capabilities}. If you do not understand the user, you should {fallback}.\")\n",
    "print(t(role=\"chitchat\", capabilities=\"chat about anything\", fallback=\"apologize and ask for clarification\"))\n",
    "print(t(role=\"book\", capabilities=\"discuss literature\", fallback=\"recommend a book about the topic\"))\n",
    "print(t(role=\"scholar\"))\n",
    "print(t(fallback=\"apologize and ask for clarification\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the template keeps track of the pronouns so that the user only has to enter changes to the template. This is convenient in some cases.\n",
    "\n",
    "Gemplates are similar to templates, but they are for semantic kernels. In computer science, a kernel is a function that transforms one data structure into another. In LangChain, a semantic kernel is a function that transforms one semantic structure into another. Let's see how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (560798519.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[24], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    gem(sentence=)\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from langchain.concise import gemplate\n",
    "\n",
    "\n",
    "gem = gemplate(\"Rewrite this sentence in the style of {name}: {sentence}\")\n",
    "\n",
    "print(gem(name=\"Steve Jobs\", sentence=\"Oh Romeo, Romeo, wherefore art thou Romeo?\"))\n",
    "print(gem(name=\"Elon Musk\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rulex: Semantic pattern matching and replacement\n",
    "\n",
    "Think regex, but with natural language. Rulex is a class for performing replacements on natural language with rules written in natural language.\n",
    "\n",
    "Start by defining the rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.concise.rulex import Rule\n",
    "\n",
    "\n",
    "rules = [\n",
    "    Rule(name=\"add comments\", pattern=\"A block of code without any comments\", replacement=\"The same code but with comments\"),\n",
    "    Rule(name=\"elaborate on comments\", pattern=\"All comments\", replacement=\"The same comment but explained with more detail\"),\n",
    "    Rule(name=\"decompose list comprehensions\", pattern=\"List comprehension\", replacement=\"The overall semantics of the list comprehension, but using a for loop\"),\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, these rules will be self-explanatory. Now, let's use them to make some code more readable.\n",
    "\n",
    "Here's the complex code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.concise.config import get_default_max_tokens\n",
    "from langchain.output_parsers.code import CodeOutputParser\n",
    "from langchain.output_parsers.incomplete import IncompleteOutputParser\n",
    "\n",
    "\n",
    "code = generate(\"Generate a long complex python script that has very few comments and uses lots of list comprehensions. Answer inside a triple backtick code block.\", parser=IncompleteOutputParser(parser=CodeOutputParser(), llm=get_default_max_tokens())\n",
    "print(code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the simple code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatMessagePromptTemplate\nrole\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconcise\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrulex\u001b[39;00m \u001b[39mimport\u001b[39;00m RulEx\n\u001b[1;32m      4\u001b[0m ru \u001b[39m=\u001b[39m RulEx(rules\u001b[39m=\u001b[39mrules)\n\u001b[1;32m      6\u001b[0m simplified_code \u001b[39m=\u001b[39m ru(code)\n",
      "File \u001b[0;32m~/github/langchain/langchain/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimportlib\u001b[39;00m \u001b[39mimport\u001b[39;00m metadata\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m \u001b[39mimport\u001b[39;00m MRKLChain, ReActChain, SelfAskWithSearchChain\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcache\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseCache\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     ConversationChain,\n\u001b[1;32m     10\u001b[0m     LLMBashChain,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     VectorDBQAWithSourcesChain,\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/__init__.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconversational\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversationalAgent\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconversational_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversationalChatAgent\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minitialize\u001b[39;00m \u001b[39mimport\u001b[39;00m initialize_agent\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload_tools\u001b[39;00m \u001b[39mimport\u001b[39;00m get_all_tool_names, load_tools\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloading\u001b[39;00m \u001b[39mimport\u001b[39;00m load_agent\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/initialize.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentExecutor\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent_types\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentType\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloading\u001b[39;00m \u001b[39mimport\u001b[39;00m AGENT_TO_CLASS, load_agent\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseCallbackManager\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/loading.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseSingleActionAgent\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m Tool\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m AGENT_TO_CLASS\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloading\u001b[39;00m \u001b[39mimport\u001b[39;00m load_chain, load_chain_from_config\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/types.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreact\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ReActDocstoreAgent\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mself_ask_with_search\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m SelfAskWithSearchAgent\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m StructuredChatAgent\n\u001b[1;32m     13\u001b[0m AGENT_TO_CLASS: Dict[AgentType, Type[BaseSingleActionAgent]] \u001b[39m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     AgentType\u001b[39m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION: ZeroShotAgent,\n\u001b[1;32m     15\u001b[0m     AgentType\u001b[39m.\u001b[39mREACT_DOCSTORE: ReActDocstoreAgent,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     AgentType\u001b[39m.\u001b[39mSTRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION: StructuredChatAgent,\n\u001b[1;32m     21\u001b[0m }\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/structured_chat/base.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydantic\u001b[39;00m \u001b[39mimport\u001b[39;00m Field\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m Agent, AgentOutputParser\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parser\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     StructuredChatOutputParserWithRetries,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompt\u001b[39;00m \u001b[39mimport\u001b[39;00m FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/structured_chat/output_parser.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompt\u001b[39;00m \u001b[39mimport\u001b[39;00m FORMAT_INSTRUCTIONS\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m \u001b[39mimport\u001b[39;00m OutputFixingParser\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentAction, AgentFinish, OutputParserException\n\u001b[1;32m     16\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/github/langchain/langchain/output_parsers/__init__.py:21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mremove_quotes\u001b[39;00m \u001b[39mimport\u001b[39;00m RemoveQuotesOutputParser\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mretry\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     MultiAttemptRetryWithErrorOutputParser,\n\u001b[1;32m     18\u001b[0m     RetryOutputParser,\n\u001b[1;32m     19\u001b[0m     RetryWithErrorOutputParser,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstitched\u001b[39;00m \u001b[39mimport\u001b[39;00m StitchedOutputParser\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured\u001b[39;00m \u001b[39mimport\u001b[39;00m ResponseSchema, StructuredOutputParser\n\u001b[1;32m     24\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mBooleanOutputParser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mRegexParser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mStitchedOutputParser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m ]\n",
      "File \u001b[0;32m~/github/langchain/langchain/output_parsers/stitched.py:32\u001b[0m\n\u001b[1;32m     21\u001b[0m MERGE_INCOMPLETE_RESPONSES_PROMPT_TEMPLATE \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[39mPlease stitch the following inputs together into a single coherent response:\u001b[39m\n\u001b[1;32m     23\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[39mCombined: \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     28\u001b[0m CONTINUE_INCOMPLETE_PLEASE_CONTINUE_PROMPT_TEMPLATE \u001b[39m=\u001b[39m (\n\u001b[1;32m     29\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSorry, your response was incomplete. Please finish your response:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 32\u001b[0m SYSTEM_PROMPT \u001b[39m=\u001b[39m ChatMessagePromptTemplate\u001b[39m.\u001b[39;49mfrom_template(template\u001b[39m=\u001b[39;49mSYSTEM_PROMPT_TEMPLATE)\n\u001b[1;32m     33\u001b[0m CONTINUE_INCOMPLETE_PROMPT \u001b[39m=\u001b[39m HumanMessagePromptTemplate\u001b[39m.\u001b[39mfrom_template(\n\u001b[1;32m     34\u001b[0m     template\u001b[39m=\u001b[39mCONTINUE_INCOMPLETE_PROMPT_TEMPLATE\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m MERGE_INCOMPLETE_RESPONSES_PROMPT \u001b[39m=\u001b[39m HumanMessagePromptTemplate\u001b[39m.\u001b[39mfrom_template(\n\u001b[1;32m     37\u001b[0m     template\u001b[39m=\u001b[39mMERGE_INCOMPLETE_RESPONSES_PROMPT_TEMPLATE\n\u001b[1;32m     38\u001b[0m )\n",
      "File \u001b[0;32m~/github/langchain/langchain/prompts/chat.py:68\u001b[0m, in \u001b[0;36mBaseStringMessagePromptTemplate.from_template\u001b[0;34m(cls, template, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_template\u001b[39m(\u001b[39mcls\u001b[39m, template: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessagePromptTemplate:\n\u001b[1;32m     67\u001b[0m     prompt \u001b[39m=\u001b[39m PromptTemplate\u001b[39m.\u001b[39mfrom_template(template)\n\u001b[0;32m---> 68\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(prompt\u001b[39m=\u001b[39;49mprompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/github/langchain/.venv/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatMessagePromptTemplate\nrole\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "from langchain.concise.rulex import RulEx\n",
    "\n",
    "\n",
    "ru = RulEx(rules=rules)\n",
    "\n",
    "simplified_code = ru(code)\n",
    "\n",
    "print(simplified_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retaining flexbillity\n",
    "\n",
    "Even though the `concise` API is, well, concise, it's still flexible. For example, you can pass a custom `LLM` or `TextSplitter` to any function that uses it. You can also change the default `LLM`, `TextSplitter`, and max tokens in `langchain.concise.config`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1677b440931f40d89ef8be7bf03acb108ce003de0ac9b18e8d43753ea2e7103"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
