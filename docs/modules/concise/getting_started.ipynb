{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%colors nocolor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "In this tutorial, we will learn about LangChain's concise API. We will cover:\n",
    "- Using the `generate`, `decide`, and `choice` functions to generate text, construct python objects, make decision, and select options.\n",
    "- Using the `template` and `gemplate` functions to create reusable text templators and semantic kernels.\n",
    "- Using `rulex` to perform semantic pattern matching and replacement.\n",
    "\n",
    "The `langchain.concise` submodule contains the following functions\n",
    "- choice.py  which provides a function for choosing an option from a list of options based on a query and examples.\n",
    "- chunk.py  which splits text into smaller chunks.\n",
    "- config.py  which provides functions for setting and getting default values for the language model, text splitter, and maximum tokens.\n",
    "- decide.py  which determines whether a statement is true or false based on a query and examples.\n",
    "- gemplate.py which defines a function for creating reusable semantic kernels.\n",
    "- generate.py  which generates text using a language model and provides convenience options for parsing, removing quotes, and retrying failed attempts.\n",
    "- pattern.py which provides a function for prompting an LLM to complete a pattern.\n",
    "- rulex.py  which provides a class for defining natural language replacement rules for text.\n",
    "- template.py  which defines a function for creating reusable text templators."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need a concise API?\n",
    "\n",
    "The concise API provides many one-liners for common use cases. For example, before the concise API existed, generating a templated completion involved several steps:\n",
    "1. creates a prompt template,\n",
    "2. renders it into a prompt,\n",
    "3. formats it with variables,\n",
    "4. constructs an LLM,\n",
    "5. calls the llm with the formatted prompt\n",
    "6. and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Hello, my name is John. What is your name?\n",
      "output: \n",
      "\n",
      "My name is Jane. Nice to meet you!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "prompt_template = 'Hello, my name is {name}. What is your name?'\n",
    "character_0_prompt = PromptTemplate(template=prompt_template, input_variables=['name'])\n",
    "prompt_str = character_0_prompt.format(name='John')\n",
    "print('input:', prompt_str)\n",
    "llm = OpenAI(verbose=False, cache=False)\n",
    "output = llm(prompt_str)\n",
    "print('output:', output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Albiet, 1-3 could be combined with f strings, but still, that's a lot of steps for developers to learn. Now, see how the concise API makes prompting much more concise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain.concise import generate\n",
    "\n",
    "country = 'France'\n",
    "output = generate(f\"What is the capital of {country}?\")\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many use cases, the concise API is all you need. However, if you need more control, you can use the full API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Must-learn: `generate`\n",
    "\n",
    "If there's one function in LangChain's concise API that you must learn, it's `generate`. `generate` is the Swiss Army knife of text generation. It can be used to generate text, construct python objects, make decisions, and select options. Let's see how it works.\n",
    "\n",
    "### Generating text\n",
    "\n",
    "The simplest use case for `generate` is to generate text. Let's generate a sentence about a dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A dog.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"What has four legs and barks?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `generate` produced a sentence about a dog. It's that simple. No templates, no prompts, no 5 hours learning langchain abstractions. \n",
    "\n",
    "### Generating python objects\n",
    "\n",
    "But what if we want to actually *generate* a dog? Well, we can do that too. Let's generate a dog. To do this, we'll need to first define a class for dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Dog(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the dog\")\n",
    "    age: int = Field(..., description=\"Age of the dog\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we have to do is pass the class to `generate` using the `type` parameter, and it will generate a dog for us! Like so,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123123\n",
      "456456\n",
      "789789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dog(name='Spot', age=5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"Make a dog named Spot that is 5 years old.\", type=Dog)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use generate on any `pydantic` model. If you're not familiar with `pydantic`, it's a library for defining data models in python. It's used by FastAPI and many other libraries. You can learn more about it here: https://pydantic-docs.helpmanual.io/.\n",
    "\n",
    "You can likewise generate `int`'s, `float`'s, and `bool`'s by passing the type to `generate`. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123123\n",
      "456456\n",
      "789789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"My name is Sam and I am 24 years old. How old am I?\", type=int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123123\n",
      "456456\n",
      "789789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"What is 2 divided by 3? Round to the hundredths place.\", type=float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating other objects\n",
    "\n",
    "Not all LLM outputs are best parsed into pydantic models. For example, if we want to generate a list of dogs, we can't use a pydantic model because pydantic models are for single objects. However, we can manually instantiate an `ItemParsedListOutputParser` and pass that to the `parser` arg in `generate`. Let's see how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123123\n",
      "456456\n",
      "789789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Dog(name='Buddy', age=4), Dog(name='Luna', age=2), Dog(name='Max', age=6)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers.item_parsed_list import ItemParsedListOutputParser\n",
    "from langchain.output_parsers.pydantic import PydanticOutputParser\n",
    "\n",
    "dog_parser = PydanticOutputParser(pydantic_object=Dog)\n",
    "dog_list_parser = ItemParsedListOutputParser(item_parser=dog_parser)\n",
    "\n",
    "generate(\"Generate 3 dogs. You pick the names and ages.\", parser=dog_list_parser, attempts=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metaprompting\n",
    "\n",
    "`generate`'s conciseness makes it very convenient for meta-prompting. Check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123123\n",
      "456456\n",
      "789789\n",
      "Avery meta meta prompt: Generate a character bio for Avery. Address them in the 2nd person, eg, 'You are _. You love _. ...'. Tell the girl who they are. What their name is. Their likes, dislike, emotions, etc, etc. Begin by stating: 'You are a...'.\n",
      "Avery meta prompt: Generate a character bio for Avery. Address her in the 2nd person, eg, 'You are _. You love _. ...'. Tell the girl who she is. What her name is. Her likes, dislike, emotions, etc, etc. Begin by stating: 'You are a...'.\n",
      "Avery prompt: You are a young woman named Avery. You love spending time with your friends, exploring new places, and trying new foods. Your favorite color is green and you have a passion for photography.\n",
      "\n",
      "You are a very empathetic person and you always try to see things from other people's perspectives. You are a good listener and enjoy helping others when they need it. You are also very creative and love expressing yourself through your art.\n",
      "\n",
      "However, you can be quite hard on yourself and often struggle with feelings of self-doubt. You have a fear of failure and sometimes have trouble taking risks or trying new things because of it.\n",
      "\n",
      "You dislike confrontation and will do anything to avoid it. You also have a tendency to hold grudges when someone has wronged you, but you are working on letting go of those negative feelings.\n",
      "\n",
      "Overall, you are a kind and caring person with a zest for life. You have your ups and downs, but you always come out stronger in the end.\n",
      "Ethan Smith message: Hello, my name is Ethan Smith. I am 17 years old. My sister is 5 years younger than me (She's 12 years old). I am a student at Crestview High School. I like to spend my free time reading books and getting lost in different stories and characters.. What are some of your favorite hobbies or activities? \n",
      "Have you traveled anywhere recently that you enjoyed? \n",
      "What kind of music do you listen to? \n",
      "What is your favorite book or movie? \n",
      "What is something you're passionate about? \n",
      "Do you have any pets? \n",
      "What is your favorite type of food? \n",
      "What kind of things do you like to do in your free time? \n",
      "What is your favorite thing about your job/school? \n",
      "Have you tried any new restaurants or recipes lately?\n",
      "Avery response: Hi Ethan, it's nice to meet you. As for my hobbies, I enjoy photography, hiking, and trying out new recipes in the kitchen. I recently traveled to Japan and absolutely loved it. The food and culture were both amazing. In terms of music, I listen to a wide variety but some of my favorites are indie and alternative. My favorite book is The Secret Life of Bees by Sue Monk Kidd and my favorite movie is The Grand Budapest Hotel. I am passionate about environmentalism and sustainability. I have a cat named Luna who is my constant companion. My favorite type of food is Italian. In my free time, I like to spend time with friends, watch movies, and explore new places. My favorite thing about school is the opportunity to learn and grow intellectually. I haven't tried any new restaurants or recipes lately, but I am always on the lookout for new things to try. How about you?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from textwrap import dedent\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "genders = ['boy', 'girl']\n",
    "random.shuffle(genders)\n",
    "age = generate(f\"Generate an age for a technically inclined high school {genders[0]}.\", type=int)\n",
    "\n",
    "character_0 = generate(f\"Generate a name for an {age} {genders[0]} (just first name): \")\n",
    "character_0_meta_meta_prompt = f\"Generate a character bio for {character_0}. Address them in the 2nd person, eg, 'You are _. You love _. ...'. Tell the {genders[0]} who they are. What their name is. Their likes, dislike, emotions, etc, etc. Begin by stating: 'You are a...'.\"\n",
    "print(f'{character_0} meta meta prompt:', character_0_meta_meta_prompt)\n",
    "character_0_meta_prompt = generate(dedent(\n",
    "    f\"\"\"\n",
    "    Instructions: Rewrite the prompt with pronouns changed from them/their into the standard form for a {genders[0]}.\n",
    "    \n",
    "    Input: {character_0_meta_meta_prompt}\n",
    "    \n",
    "    Output: \n",
    "    \"\"\"\n",
    "    )\n",
    ")\n",
    "print(f'{character_0} meta prompt:', character_0_meta_prompt)\n",
    "character_0_prompt = generate(character_0_meta_prompt)\n",
    "print(f'{character_0} prompt:', character_0_prompt)\n",
    "\n",
    "character_1 = generate(f\"Generate a name for a {age}-year-old {genders[1]}.\")\n",
    "character_1_message = f\"\"\"Hello, my name is {character_1}. I am {age} years old. My sister is 5 years younger than me (She's {age-5} years old). I am a student at {generate(\"Generate a name for a high school.\")}. {generate(\"Write an 'I like to' statement. You can decide whatever it is.\")}. {generate(\"Ask the kind of question that make for good socialization, eg, do you like ...? what do you think about ...?\")}\"\"\"\n",
    "print(f'{character_1} message:', character_1_message)\n",
    "\n",
    "response = chat_model([\n",
    "    SystemMessage(content=character_0_prompt),\n",
    "    HumanMessage(content=character_1_message)\n",
    "    ])\n",
    "print(f'{character_0} response:', response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we generate a system persona on the fly. The simulated user generated several of their characteristics on the fly as well, including a sentence about the user's likes.\n",
    "\n",
    "Perhaps a more practical example for LLM developers like yourself is using LLMs to generate prompts for other LLMs. Let's see how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  Prompt: Generate a non-puppy dog named Spot.\n",
      "123123\n",
      "456456\n",
      "789789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dog(name='Spot', age=3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "\n",
    "character_0_prompt = generate(\n",
    "    dedent(\n",
    "        \"\"\"\n",
    "        ## Introduction\n",
    "        \n",
    "        You are an expert prompt engineer.\n",
    "        \n",
    "        ## Task\n",
    "        \n",
    "        You are writing a prompt to generate a dog.\n",
    "        - the dog should be named Spot.\n",
    "        - I don't care how old the dog is as long as its old enought to not be a puppy.\n",
    "        \n",
    "        ## Notes\n",
    "        \n",
    "        Remember, tokens are expensive. So try to use the least amount of tokens possible. Write the prompt.\n",
    "        \n",
    "        ## Output\n",
    "\"\"\"))\n",
    "print('Prompt: ', character_0_prompt)\n",
    "output = generate(character_0_prompt, type=Dog)\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making decisions and selecting options\n",
    "\n",
    "See how Langchain's concise API can intergrate deeper into you code with `decide` and `choice`. These functions return a boolean and an option respectively. Let's see how they work.\n",
    "\n",
    "We'll start with `decide`. `decide` takes a query and a list of examples. It returns a boolean indicating whether the query is true or false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123123\n",
      "456456\n",
      "789789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.concise import decide\n",
    "\n",
    "\n",
    "value = decide(character_0_prompt, query=\"Was this prompt descriptive enough?\")\n",
    "value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since its signature is so concise, you can integrate `decide` directly into your code. For example, you can use it to make decisions about whether to take a certain branch of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123123\n",
      "456456\n",
      "789789\n",
      "123123\n",
      "456456\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "---------------------------------------------------------------------------",
      "KeyboardInterrupt                         Traceback (most recent call last)",
      "Cell In[15], line 3\n      1 for i in range(5):\n      2     result = generate('Write a comical story about ClosedAI taking over the world.')\n----> 3     if decide(result, query=\"Would this story be entertaining to engineers working on a prompt engineering system?\", default=False):\n      4         break\n      5 print(result)\n",
      "File ~/github/langchain/langchain/concise/decide.py:41, in decide(input, query, true_examples, false_examples, examples, llm, default)\n     35 examples = [\n     36     (input, parser.true_val if output else parser.false_val)\n     37     for input, output in examples\n     38 ]\n     40 try:\n---> 41     return pattern(\n     42         input=input,\n     43         query=query,\n     44         pattern_name=\"decide\",\n     45         parser=parser,\n     46         examples=examples,\n     47         llm=llm,\n     48     )\n     49 except (ValueError, OutputParserException) as e:\n     50     if default is not None:\n",
      "File ~/github/langchain/langchain/concise/pattern.py:40, in pattern(input, query, pattern_name, input_format_template, output_format_template, parser, examples, llm)\n     30 assert all(\n     31     len(example) == 3 for example in examples\n     32 ), \"Examples must be a list of tuples of length 3.\"\n     33 input_msgs = render_prompt_and_examples(\n     34     system_prompt=query,\n     35     input_prompt_template=input_format_template,\n   (...)\n     38     examples=examples,\n     39 )\n---> 40 return generate(input_msgs, llm=llm, parser=parser)\n",
      "File ~/github/langchain/langchain/concise/generate.py:141, in generate(input, parser, type, llm, stop, attempts, additional_validator, remove_quotes)\n    134 retry_with_error_parser = MultiAttemptRetryWithErrorOutputParser.from_llm(\n    135     parser=parser,\n    136     llm=llm,\n    137     attempts=attempts,\n    138     additional_validator=additional_validator,\n    139 )\n    140 print(456456)\n--> 141 response = _generate(input, llm=llm, stop=stop)\n    142 print(789789)\n    143 return retry_with_error_parser.parse_with_prompt(\n    144     response,\n    145     prompt_value=StringPromptValue(text=input)\n   (...)\n    149     else None,\n    150 )\n",
      "File ~/github/langchain/langchain/concise/generate.py:27, in _generate(input, llm, stop)\n     25 if isinstance(input, list) and all(isinstance(msg, BaseMessage) for msg in input):\n     26     if isinstance(llm, BaseChatModel):\n---> 27         output = llm(input, stop=stop).content\n     28     elif isinstance(llm, BaseLanguageModel):\n     29         output = ChatModelFacade.of(llm)(input, stop=stop)\n",
      "File ~/github/langchain/langchain/chat_models/base.py:177, in BaseChatModel.__call__(self, messages, stop, callbacks)\n    171 def __call__(\n    172     self,\n    173     messages: List[BaseMessage],\n    174     stop: Optional[List[str]] = None,\n    175     callbacks: Callbacks = None,\n    176 ) -> BaseMessage:\n--> 177     generation = self.generate(\n    178         [messages], stop=stop, callbacks=callbacks\n    179     ).generations[0][0]\n    180     if isinstance(generation, ChatGeneration):\n    181         return generation.message\n",
      "File ~/github/langchain/langchain/chat_models/base.py:90, in BaseChatModel.generate(self, messages, stop, callbacks)\n     88 except (KeyboardInterrupt, Exception) as e:\n     89     run_manager.on_llm_error(e)\n---> 90     raise e\n     91 llm_output = self._combine_llm_outputs([res.llm_output for res in results])\n     92 generations = [res.generations for res in results]\n",
      "File ~/github/langchain/langchain/chat_models/base.py:82, in BaseChatModel.generate(self, messages, stop, callbacks)\n     78 new_arg_supported = inspect.signature(self._generate).parameters.get(\n     79     \"run_manager\"\n     80 )\n     81 try:\n---> 82     results = [\n     83         self._generate(m, stop=stop, run_manager=run_manager)\n     84         if new_arg_supported\n     85         else self._generate(m, stop=stop)\n     86         for m in messages\n     87     ]\n     88 except (KeyboardInterrupt, Exception) as e:\n     89     run_manager.on_llm_error(e)\n",
      "File ~/github/langchain/langchain/chat_models/base.py:83, in <listcomp>(.0)\n     78 new_arg_supported = inspect.signature(self._generate).parameters.get(\n     79     \"run_manager\"\n     80 )\n     81 try:\n     82     results = [\n---> 83         self._generate(m, stop=stop, run_manager=run_manager)\n     84         if new_arg_supported\n     85         else self._generate(m, stop=stop)\n     86         for m in messages\n     87     ]\n     88 except (KeyboardInterrupt, Exception) as e:\n     89     run_manager.on_llm_error(e)\n",
      "File ~/github/langchain/langchain/chat_models/openai.py:293, in ChatOpenAI._generate(self, messages, stop, run_manager)\n    289     message = _convert_dict_to_message(\n    290         {\"content\": inner_completion, \"role\": role}\n    291     )\n    292     return ChatResult(generations=[ChatGeneration(message=message)])\n--> 293 response = self.completion_with_retry(messages=message_dicts, **params)\n    294 return self._create_chat_result(response)\n",
      "File ~/github/langchain/langchain/chat_models/openai.py:254, in ChatOpenAI.completion_with_retry(self, **kwargs)\n    250 @retry_decorator\n    251 def _completion_with_retry(**kwargs: Any) -> Any:\n    252     return self.client.create(**kwargs)\n--> 254 return _completion_with_retry(**kwargs)\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/tenacity/__init__.py:289, in BaseRetrying.wraps.<locals>.wrapped_f(*args, **kw)\n    287 @functools.wraps(f)\n    288 def wrapped_f(*args: t.Any, **kw: t.Any) -> t.Any:\n--> 289     return self(f, *args, **kw)\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/tenacity/__init__.py:379, in Retrying.__call__(self, fn, *args, **kwargs)\n    377 retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)\n    378 while True:\n--> 379     do = self.iter(retry_state=retry_state)\n    380     if isinstance(do, DoAttempt):\n    381         try:\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/tenacity/__init__.py:314, in BaseRetrying.iter(self, retry_state)\n    312 is_explicit_retry = fut.failed and isinstance(fut.exception(), TryAgain)\n    313 if not (is_explicit_retry or self.retry(retry_state)):\n--> 314     return fut.result()\n    316 if self.after is not None:\n    317     self.after(retry_state)\n",
      "File ~/.pyenv/versions/3.10.9/lib/python3.10/concurrent/futures/_base.py:451, in Future.result(self, timeout)\n    449     raise CancelledError()\n    450 elif self._state == FINISHED:\n--> 451     return self.__get_result()\n    453 self._condition.wait(timeout)\n    455 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File ~/.pyenv/versions/3.10.9/lib/python3.10/concurrent/futures/_base.py:403, in Future.__get_result(self)\n    401 if self._exception:\n    402     try:\n--> 403         raise self._exception\n    404     finally:\n    405         # Break a reference cycle with the exception in self._exception\n    406         self = None\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/tenacity/__init__.py:382, in Retrying.__call__(self, fn, *args, **kwargs)\n    380 if isinstance(do, DoAttempt):\n    381     try:\n--> 382         result = fn(*args, **kwargs)\n    383     except BaseException:  # noqa: B902\n    384         retry_state.set_exception(sys.exc_info())  # type: ignore[arg-type]\n",
      "File ~/github/langchain/langchain/chat_models/openai.py:252, in ChatOpenAI.completion_with_retry.<locals>._completion_with_retry(**kwargs)\n    250 @retry_decorator\n    251 def _completion_with_retry(**kwargs: Any) -> Any:\n--> 252     return self.client.create(**kwargs)\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25, in ChatCompletion.create(cls, *args, **kwargs)\n     23 while True:\n     24     try:\n---> 25         return super().create(*args, **kwargs)\n     26     except TryAgain as e:\n     27         if timeout is not None and time.time() > start + timeout:\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153, in EngineAPIResource.create(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\n    127 @classmethod\n    128 def create(\n    129     cls,\n   (...)\n    136     **params,\n    137 ):\n    138     (\n    139         deployment_id,\n    140         engine,\n   (...)\n    150         api_key, api_base, api_type, api_version, organization, **params\n    151     )\n--> 153     response, _, api_key = requestor.request(\n    154         \"post\",\n    155         url,\n    156         params=params,\n    157         headers=headers,\n    158         stream=stream,\n    159         request_id=request_id,\n    160         request_timeout=request_timeout,\n    161     )\n    163     if stream:\n    164         # must be an iterator\n    165         assert not isinstance(response, OpenAIResponse)\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/openai/api_requestor.py:220, in APIRequestor.request(self, method, url, params, headers, files, stream, request_id, request_timeout)\n    209 def request(\n    210     self,\n    211     method,\n   (...)\n    218     request_timeout: Optional[Union[float, Tuple[float, float]]] = None,\n    219 ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], bool, str]:\n--> 220     result = self.request_raw(\n    221         method.lower(),\n    222         url,\n    223         params=params,\n    224         supplied_headers=headers,\n    225         files=files,\n    226         stream=stream,\n    227         request_id=request_id,\n    228         request_timeout=request_timeout,\n    229     )\n    230     resp, got_stream = self._interpret_response(result, stream)\n    231     return resp, got_stream, self.api_key\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/openai/api_requestor.py:520, in APIRequestor.request_raw(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\n    518     _thread_context.session = _make_session()\n    519 try:\n--> 520     result = _thread_context.session.request(\n    521         method,\n    522         abs_url,\n    523         headers=headers,\n    524         data=data,\n    525         files=files,\n    526         stream=stream,\n    527         timeout=request_timeout if request_timeout else TIMEOUT_SECS,\n    528         proxies=_thread_context.session.proxies,\n    529     )\n    530 except requests.exceptions.Timeout as e:\n    531     raise error.Timeout(\"Request timed out: {}\".format(e)) from e\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/requests/sessions.py:587, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\n    582 send_kwargs = {\n    583     \"timeout\": timeout,\n    584     \"allow_redirects\": allow_redirects,\n    585 }\n    586 send_kwargs.update(settings)\n--> 587 resp = self.send(prep, **send_kwargs)\n    589 return resp\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/requests/sessions.py:701, in Session.send(self, request, **kwargs)\n    698 start = preferred_clock()\n    700 # Send the request\n--> 701 r = adapter.send(request, **kwargs)\n    703 # Total elapsed time of the request (approximately)\n    704 elapsed = preferred_clock() - start\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/requests/adapters.py:489, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies)\n    487 try:\n    488     if not chunked:\n--> 489         resp = conn.urlopen(\n    490             method=request.method,\n    491             url=url,\n    492             body=request.body,\n    493             headers=request.headers,\n    494             redirect=False,\n    495             assert_same_host=False,\n    496             preload_content=False,\n    497             decode_content=False,\n    498             retries=self.max_retries,\n    499             timeout=timeout,\n    500         )\n    502     # Send the request.\n    503     else:\n    504         if hasattr(conn, \"proxy_pool\"):\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:703, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\n    700     self._prepare_proxy(conn)\n    702 # Make the request on the httplib connection object.\n--> 703 httplib_response = self._make_request(\n    704     conn,\n    705     method,\n    706     url,\n    707     timeout=timeout_obj,\n    708     body=body,\n    709     headers=headers,\n    710     chunked=chunked,\n    711 )\n    713 # If we're going to release the connection in ``finally:``, then\n    714 # the response doesn't need to know about the connection. Otherwise\n    715 # it will also try to release it and we'll have a double-release\n    716 # mess.\n    717 response_conn = conn if not release_conn else None\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:449, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n    444             httplib_response = conn.getresponse()\n    445         except BaseException as e:\n    446             # Remove the TypeError from the exception chain in\n    447             # Python 3 (including for exceptions like SystemExit).\n    448             # Otherwise it looks like a bug in the code.\n--> 449             six.raise_from(e, None)\n    450 except (SocketTimeout, BaseSSLError, SocketError) as e:\n    451     self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "File <string>:3, in raise_from(value, from_value)\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:444, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\n    441 except TypeError:\n    442     # Python 3\n    443     try:\n--> 444         httplib_response = conn.getresponse()\n    445     except BaseException as e:\n    446         # Remove the TypeError from the exception chain in\n    447         # Python 3 (including for exceptions like SystemExit).\n    448         # Otherwise it looks like a bug in the code.\n    449         six.raise_from(e, None)\n",
      "File ~/.pyenv/versions/3.10.9/lib/python3.10/http/client.py:1374, in HTTPConnection.getresponse(self)\n   1372 try:\n   1373     try:\n-> 1374         response.begin()\n   1375     except ConnectionError:\n   1376         self.close()\n",
      "File ~/.pyenv/versions/3.10.9/lib/python3.10/http/client.py:318, in HTTPResponse.begin(self)\n    316 # read until we get a non-100 response\n    317 while True:\n--> 318     version, status, reason = self._read_status()\n    319     if status != CONTINUE:\n    320         break\n",
      "File ~/.pyenv/versions/3.10.9/lib/python3.10/http/client.py:279, in HTTPResponse._read_status(self)\n    278 def _read_status(self):\n--> 279     line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n    280     if len(line) > _MAXLINE:\n    281         raise LineTooLong(\"status line\")\n",
      "File ~/.pyenv/versions/3.10.9/lib/python3.10/socket.py:705, in SocketIO.readinto(self, b)\n    703 while True:\n    704     try:\n--> 705         return self._sock.recv_into(b)\n    706     except timeout:\n    707         self._timeout_occurred = True\n",
      "File ~/.pyenv/versions/3.10.9/lib/python3.10/ssl.py:1274, in SSLSocket.recv_into(self, buffer, nbytes, flags)\n   1270     if flags != 0:\n   1271         raise ValueError(\n   1272           \"non-zero flags not allowed in calls to recv_into() on %s\" %\n   1273           self.__class__)\n-> 1274     return self.read(nbytes, buffer)\n   1275 else:\n   1276     return super().recv_into(buffer, nbytes, flags)\n",
      "File ~/.pyenv/versions/3.10.9/lib/python3.10/ssl.py:1130, in SSLSocket.read(self, len, buffer)\n   1128 try:\n   1129     if buffer is not None:\n-> 1130         return self._sslobj.read(len, buffer)\n   1131     else:\n   1132         return self._sslobj.read(len)\n",
      "KeyboardInterrupt: "
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    result = generate('Write a comical story about ClosedAI taking over the world.')\n",
    "    if decide(result, query=\"Would this story be entertaining to engineers working on a prompt engineering system?\", default=False):\n",
    "        break\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a natural language game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are at position 0, 0 on a map of size 10 x 10. You can move up down left or right. You can also ask questions about the this game. What do you want to do?\n",
      "123123\n",
      "456456\n",
      "789789\n",
      "123123\n",
      "456456\n",
      "789789\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes exactly 1 positional argument (2 given)",
     "output_type": "error",
     "traceback": [
      "---------------------------------------------------------------------------",
      "TypeError                                 Traceback (most recent call last)",
      "Cell In[19], line 23\n     21     print(generate(f\"Answer the user's question.\\n\\nFor reference, here is the code for the game:\\n\\n{inspect.getsource(inspect.currentframe())}\\n\\nUser: {user}\\nAI: \"))\n     22 elif decide(query='Did the user move? EG, \"go up\"?', input=user, default=False):\n---> 23     match choice(query='Which direction did the user move?', input=user, options=Direction):\n     24         case Direction.UP:\n     25             y += 1\n",
      "File ~/github/langchain/langchain/concise/choice.py:34, in choice(input, query, options, examples, llm, default)\n     32 llm = llm or config.get_default_model()\n     33 if isinstance(options, type(Enum)):\n---> 34     parser = EnumOutputParser(options)\n     35 elif isinstance(options, list) and all(isinstance(option, str) for option in options):\n     36     parser = ChoiceOutputParser(options=options, min_distance=1)\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/pydantic/main.py:332, in pydantic.main.BaseModel.__init__()\n",
      "TypeError: __init__() takes exactly 1 positional argument (2 given)"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from enum import Enum\n",
    "import inspect\n",
    "\n",
    "from langchain.concise import choice\n",
    "\n",
    "x,y=0,0\n",
    "h,w=10,10\n",
    "\n",
    "class Direction(Enum):\n",
    "    UP = (0,1)\n",
    "    DOWN = (0,-1)\n",
    "    LEFT = (-1,0)\n",
    "    RIGHT = (1,0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(f'You are at position {x}, {y} on a map of size {h} x {w}. You can move up down left or right. You can also ask questions about the this game. What do you want to do?')\n",
    "    user = input(\">>> \")\n",
    "    if decide(query='Did the user ask a question?', input=user, default=False):\n",
    "        print(generate(f\"Answer the user's question.\\n\\nFor reference, here is the code for the game:\\n\\n{inspect.getsource(inspect.currentframe())}\\n\\nUser: {user}\\nAI: \"))\n",
    "    elif decide(query='Did the user move? EG, \"go up\"?', input=user, default=False):\n",
    "        match choice(query='Which direction did the user move?', input=user, options=Direction):\n",
    "            case Direction.UP:\n",
    "                y += 1\n",
    "            case Direction.DOWN:\n",
    "                y -= 1\n",
    "            case Direction.LEFT:\n",
    "                x -= 1\n",
    "            case Direction.RIGHT:\n",
    "                x += 1\n",
    "        x, y = max(0, min(h, x)), max(0, min(w, y))\n",
    "    else:\n",
    "        print(generate(f'Apolagize for not understanding the user and ask them to try again.\\n\\nUser: {user}\\nAI: '))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, `choice` gives the LLM the ability to select an option from a list of options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HumanMessage\ncontent\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "---------------------------------------------------------------------------",
      "ValidationError                           Traceback (most recent call last)",
      "Cell In[20], line 4\n      1 from langchain.concise import choice\n----> 4 flavor = choice(\"Sam loves ice cream. He enjoy chocolate and vanilla, but he really really loves strawberry.\", query=\"Pick Sams favorite color\", options=['chocolate', 'vanilla', 'strawberry'])\n      5 print(flavor)\n",
      "File ~/github/langchain/langchain/concise/choice.py:55, in choice(input, query, options, examples, llm, default)\n     53     return default\n     54 else:\n---> 55     raise e\n",
      "File ~/github/langchain/langchain/concise/choice.py:43, in choice(input, query, options, examples, llm, default)\n     38     raise ValueError(\n     39         f\"options must be a list of strings or an enum, not {type(options)}\"\n     40     )\n     42 try:\n---> 43     return pattern(\n     44         input=input,\n     45         query=query,\n     46         pattern_name=\"choice\",\n     47         parser=parser,\n     48         examples=examples,\n     49         llm=llm,\n     50     )\n     51 except (ValueError, OutputParserException) as e:\n     52     if default is not None:\n",
      "File ~/github/langchain/langchain/concise/pattern.py:40, in pattern(input, query, pattern_name, input_format_template, output_format_template, parser, examples, llm)\n     30 assert all(\n     31     len(example) == 3 for example in examples\n     32 ), \"Examples must be a list of tuples of length 3.\"\n     33 input_msgs = render_prompt_and_examples(\n     34     system_prompt=query,\n     35     input_prompt_template=input_format_template,\n   (...)\n     38     examples=examples,\n     39 )\n---> 40 return generate(input_msgs, llm=llm, parser=parser)\n",
      "File ~/github/langchain/langchain/concise/generate.py:120, in generate(input, parser, type, llm, stop, attempts, additional_validator, remove_quotes)\n    117 try:\n    118     if isinstance(input, list):\n    119         input.append(\n--> 120             HumanMessage(\n    121                 text=f\"Formatting directions: {parser.get_format_instructions()}\"\n    122             )\n    123         )\n    124     elif isinstance(input, str):\n    125         input += f\"\\n\\n## Formatting directions\\n\\n{parser.get_format_instructions()}\\n\\n## Output\\n\\n\"\n",
      "File ~/github/langchain/.venv/lib/python3.10/site-packages/pydantic/main.py:341, in pydantic.main.BaseModel.__init__()\n",
      "ValidationError: 1 validation error for HumanMessage\ncontent\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "from langchain.concise import choice\n",
    "\n",
    "\n",
    "flavor = choice(\"Sam loves ice cream. He enjoy chocolate and vanilla, but he really really loves strawberry.\", query=\"Pick Sams favorite color\", options=['chocolate', 'vanilla', 'strawberry'])\n",
    "print(flavor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And likewise for Enum's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Color(Enum):\n",
    "    red = \"red\"\n",
    "    yellow = \"yellow\"\n",
    "    green = \"green\"\n",
    "    blue = \"blue\"\n",
    "\n",
    "user_name = \"Steeve\"\n",
    "user_dossier = \"Steve is our 10 year loyal customer. He is a big fan of our product and has been a great advocate for us. He drives a red truck and loves to eat pizza.\"\n",
    "\n",
    "match choice(f\"What color product is {user_name} most likely to buy?\", options=Color):\n",
    "    case Color.red:\n",
    "        print('Correct!')  # I'm not a marketing person, this may be wrong\n",
    "    case Color.yellow:\n",
    "        print('Sorry, that is incorrect')\n",
    "    case Color.green:\n",
    "        print('Sorry, that is incorrect')\n",
    "    case Color.blue:\n",
    "        print('Sorry, that is incorrect')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Templates and Gemplates\n",
    "\n",
    "Now let's explore some of the more cutting edge features of the concise API. We'll start with templates. Templates are statefull text templators. They are useful for keeping track of pronouns when generating text, such as prompts for LLMs. Let's see how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatMessagePromptTemplate\nrole\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconcise\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtemplate\u001b[39;00m \u001b[39mimport\u001b[39;00m template\n\u001b[1;32m      4\u001b[0m t \u001b[39m=\u001b[39m template(\u001b[39m\"\u001b[39m\u001b[39mYou are \u001b[39m\u001b[39m{role}\u001b[39;00m\u001b[39mGPT. You can \u001b[39m\u001b[39m{capabilities}\u001b[39;00m\u001b[39m. If you do not understand the user, you should \u001b[39m\u001b[39m{fallback}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(t(role\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchitchat\u001b[39m\u001b[39m\"\u001b[39m, capabilities\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchat about anything\u001b[39m\u001b[39m\"\u001b[39m, fallback\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapologize and ask for clarification\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/github/langchain/langchain/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimportlib\u001b[39;00m \u001b[39mimport\u001b[39;00m metadata\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m \u001b[39mimport\u001b[39;00m MRKLChain, ReActChain, SelfAskWithSearchChain\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcache\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseCache\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     ConversationChain,\n\u001b[1;32m     10\u001b[0m     LLMBashChain,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     VectorDBQAWithSourcesChain,\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/__init__.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconversational\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversationalAgent\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconversational_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversationalChatAgent\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minitialize\u001b[39;00m \u001b[39mimport\u001b[39;00m initialize_agent\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload_tools\u001b[39;00m \u001b[39mimport\u001b[39;00m get_all_tool_names, load_tools\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloading\u001b[39;00m \u001b[39mimport\u001b[39;00m load_agent\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/initialize.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentExecutor\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent_types\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentType\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloading\u001b[39;00m \u001b[39mimport\u001b[39;00m AGENT_TO_CLASS, load_agent\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseCallbackManager\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/loading.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseSingleActionAgent\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m Tool\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m AGENT_TO_CLASS\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloading\u001b[39;00m \u001b[39mimport\u001b[39;00m load_chain, load_chain_from_config\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/types.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreact\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ReActDocstoreAgent\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mself_ask_with_search\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m SelfAskWithSearchAgent\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m StructuredChatAgent\n\u001b[1;32m     13\u001b[0m AGENT_TO_CLASS: Dict[AgentType, Type[BaseSingleActionAgent]] \u001b[39m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     AgentType\u001b[39m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION: ZeroShotAgent,\n\u001b[1;32m     15\u001b[0m     AgentType\u001b[39m.\u001b[39mREACT_DOCSTORE: ReActDocstoreAgent,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     AgentType\u001b[39m.\u001b[39mSTRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION: StructuredChatAgent,\n\u001b[1;32m     21\u001b[0m }\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/structured_chat/base.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydantic\u001b[39;00m \u001b[39mimport\u001b[39;00m Field\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m Agent, AgentOutputParser\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parser\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     StructuredChatOutputParserWithRetries,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompt\u001b[39;00m \u001b[39mimport\u001b[39;00m FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/structured_chat/output_parser.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompt\u001b[39;00m \u001b[39mimport\u001b[39;00m FORMAT_INSTRUCTIONS\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m \u001b[39mimport\u001b[39;00m OutputFixingParser\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentAction, AgentFinish, OutputParserException\n\u001b[1;32m     16\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/github/langchain/langchain/output_parsers/__init__.py:21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mremove_quotes\u001b[39;00m \u001b[39mimport\u001b[39;00m RemoveQuotesOutputParser\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mretry\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     MultiAttemptRetryWithErrorOutputParser,\n\u001b[1;32m     18\u001b[0m     RetryOutputParser,\n\u001b[1;32m     19\u001b[0m     RetryWithErrorOutputParser,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstitched\u001b[39;00m \u001b[39mimport\u001b[39;00m StitchedOutputParser\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured\u001b[39;00m \u001b[39mimport\u001b[39;00m ResponseSchema, StructuredOutputParser\n\u001b[1;32m     24\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mBooleanOutputParser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mRegexParser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mStitchedOutputParser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m ]\n",
      "File \u001b[0;32m~/github/langchain/langchain/output_parsers/stitched.py:32\u001b[0m\n\u001b[1;32m     21\u001b[0m MERGE_INCOMPLETE_RESPONSES_PROMPT_TEMPLATE \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[39mPlease stitch the following inputs together into a single coherent response:\u001b[39m\n\u001b[1;32m     23\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[39mCombined: \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     28\u001b[0m CONTINUE_INCOMPLETE_PLEASE_CONTINUE_PROMPT_TEMPLATE \u001b[39m=\u001b[39m (\n\u001b[1;32m     29\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSorry, your response was incomplete. Please finish your response:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 32\u001b[0m SYSTEM_PROMPT \u001b[39m=\u001b[39m ChatMessagePromptTemplate\u001b[39m.\u001b[39;49mfrom_template(template\u001b[39m=\u001b[39;49mSYSTEM_PROMPT_TEMPLATE)\n\u001b[1;32m     33\u001b[0m CONTINUE_INCOMPLETE_PROMPT \u001b[39m=\u001b[39m HumanMessagePromptTemplate\u001b[39m.\u001b[39mfrom_template(\n\u001b[1;32m     34\u001b[0m     template\u001b[39m=\u001b[39mCONTINUE_INCOMPLETE_PROMPT_TEMPLATE\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m MERGE_INCOMPLETE_RESPONSES_PROMPT \u001b[39m=\u001b[39m HumanMessagePromptTemplate\u001b[39m.\u001b[39mfrom_template(\n\u001b[1;32m     37\u001b[0m     template\u001b[39m=\u001b[39mMERGE_INCOMPLETE_RESPONSES_PROMPT_TEMPLATE\n\u001b[1;32m     38\u001b[0m )\n",
      "File \u001b[0;32m~/github/langchain/langchain/prompts/chat.py:68\u001b[0m, in \u001b[0;36mBaseStringMessagePromptTemplate.from_template\u001b[0;34m(cls, template, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_template\u001b[39m(\u001b[39mcls\u001b[39m, template: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessagePromptTemplate:\n\u001b[1;32m     67\u001b[0m     prompt \u001b[39m=\u001b[39m PromptTemplate\u001b[39m.\u001b[39mfrom_template(template)\n\u001b[0;32m---> 68\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(prompt\u001b[39m=\u001b[39;49mprompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/github/langchain/.venv/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatMessagePromptTemplate\nrole\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "from langchain.concise.template import template\n",
    "\n",
    "\n",
    "t = template(\"You are {role}GPT. You can {capabilities}. If you do not understand the user, you should {fallback}.\")\n",
    "print(t(role=\"chitchat\", capabilities=\"chat about anything\", fallback=\"apologize and ask for clarification\"))\n",
    "print(t(role=\"book\", capabilities=\"discuss literature\", fallback=\"recommend a book about the topic\"))\n",
    "print(t(role=\"scholar\"))\n",
    "print(t(fallback=\"apologize and ask for clarification\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the template keeps track of the pronouns so that the user only has to enter changes to the template. This is convenient in some cases.\n",
    "\n",
    "Gemplates are similar to templates, but they are for semantic kernels. In computer science, a kernel is a function that transforms one data structure into another. In LangChain, a semantic kernel is a function that transforms one semantic structure into another. Let's see how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (560798519.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[24], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    gem(sentence=)\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from langchain.concise import gemplate\n",
    "\n",
    "\n",
    "gem = gemplate(\"Rewrite this sentence in the style of {name}: {sentence}\")\n",
    "\n",
    "print(gem(name=\"Steve Jobs\", sentence=\"Oh Romeo, Romeo, wherefore art thou Romeo?\"))\n",
    "print(gem(name=\"Elon Musk\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rulex: Semantic pattern matching and replacement\n",
    "\n",
    "Think regex, but with natural language. Rulex is a class for performing replacements on natural language with rules written in natural language.\n",
    "\n",
    "Start by defining the rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.concise.rulex import Rule\n",
    "\n",
    "\n",
    "rules = [\n",
    "    Rule(name=\"add comments\", pattern=\"A block of code without any comments\", replacement=\"The same code but with comments\"),\n",
    "    Rule(name=\"elaborate on comments\", pattern=\"All comments\", replacement=\"The same comment but explained with more detail\"),\n",
    "    Rule(name=\"decompose list comprehensions\", pattern=\"List comprehension\", replacement=\"The overall semantics of the list comprehension, but using a for loop\"),\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, these rules will be self-explanatory. Now, let's use them to make some code more readable.\n",
    "\n",
    "Here's the complex code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.concise.config import get_default_max_tokens\n",
    "from langchain.output_parsers.code import CodeOutputParser\n",
    "from langchain.output_parsers.incomplete import IncompleteOutputParser\n",
    "\n",
    "\n",
    "code = generate(\"Generate a long complex python script that has very few comments and uses lots of list comprehensions. Answer inside a triple backtick code block.\", parser=IncompleteOutputParser(parser=CodeOutputParser(), llm=get_default_max_tokens())\n",
    "print(code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the simple code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatMessagePromptTemplate\nrole\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconcise\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrulex\u001b[39;00m \u001b[39mimport\u001b[39;00m RulEx\n\u001b[1;32m      4\u001b[0m ru \u001b[39m=\u001b[39m RulEx(rules\u001b[39m=\u001b[39mrules)\n\u001b[1;32m      6\u001b[0m simplified_code \u001b[39m=\u001b[39m ru(code)\n",
      "File \u001b[0;32m~/github/langchain/langchain/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimportlib\u001b[39;00m \u001b[39mimport\u001b[39;00m metadata\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m \u001b[39mimport\u001b[39;00m MRKLChain, ReActChain, SelfAskWithSearchChain\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcache\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseCache\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     ConversationChain,\n\u001b[1;32m     10\u001b[0m     LLMBashChain,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     VectorDBQAWithSourcesChain,\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/__init__.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconversational\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversationalAgent\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconversational_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversationalChatAgent\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minitialize\u001b[39;00m \u001b[39mimport\u001b[39;00m initialize_agent\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload_tools\u001b[39;00m \u001b[39mimport\u001b[39;00m get_all_tool_names, load_tools\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloading\u001b[39;00m \u001b[39mimport\u001b[39;00m load_agent\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/initialize.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentExecutor\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent_types\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentType\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloading\u001b[39;00m \u001b[39mimport\u001b[39;00m AGENT_TO_CLASS, load_agent\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseCallbackManager\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/loading.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseSingleActionAgent\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m Tool\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m AGENT_TO_CLASS\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloading\u001b[39;00m \u001b[39mimport\u001b[39;00m load_chain, load_chain_from_config\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/types.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreact\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ReActDocstoreAgent\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mself_ask_with_search\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m SelfAskWithSearchAgent\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m StructuredChatAgent\n\u001b[1;32m     13\u001b[0m AGENT_TO_CLASS: Dict[AgentType, Type[BaseSingleActionAgent]] \u001b[39m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     AgentType\u001b[39m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION: ZeroShotAgent,\n\u001b[1;32m     15\u001b[0m     AgentType\u001b[39m.\u001b[39mREACT_DOCSTORE: ReActDocstoreAgent,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     AgentType\u001b[39m.\u001b[39mSTRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION: StructuredChatAgent,\n\u001b[1;32m     21\u001b[0m }\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/structured_chat/base.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydantic\u001b[39;00m \u001b[39mimport\u001b[39;00m Field\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m Agent, AgentOutputParser\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parser\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     StructuredChatOutputParserWithRetries,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompt\u001b[39;00m \u001b[39mimport\u001b[39;00m FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n",
      "File \u001b[0;32m~/github/langchain/langchain/agents/structured_chat/output_parser.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_chat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompt\u001b[39;00m \u001b[39mimport\u001b[39;00m FORMAT_INSTRUCTIONS\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_language\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLanguageModel\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m \u001b[39mimport\u001b[39;00m OutputFixingParser\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentAction, AgentFinish, OutputParserException\n\u001b[1;32m     16\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/github/langchain/langchain/output_parsers/__init__.py:21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mremove_quotes\u001b[39;00m \u001b[39mimport\u001b[39;00m RemoveQuotesOutputParser\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mretry\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     MultiAttemptRetryWithErrorOutputParser,\n\u001b[1;32m     18\u001b[0m     RetryOutputParser,\n\u001b[1;32m     19\u001b[0m     RetryWithErrorOutputParser,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstitched\u001b[39;00m \u001b[39mimport\u001b[39;00m StitchedOutputParser\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput_parsers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured\u001b[39;00m \u001b[39mimport\u001b[39;00m ResponseSchema, StructuredOutputParser\n\u001b[1;32m     24\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mBooleanOutputParser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mRegexParser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mStitchedOutputParser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m ]\n",
      "File \u001b[0;32m~/github/langchain/langchain/output_parsers/stitched.py:32\u001b[0m\n\u001b[1;32m     21\u001b[0m MERGE_INCOMPLETE_RESPONSES_PROMPT_TEMPLATE \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[39mPlease stitch the following inputs together into a single coherent response:\u001b[39m\n\u001b[1;32m     23\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[39mCombined: \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     28\u001b[0m CONTINUE_INCOMPLETE_PLEASE_CONTINUE_PROMPT_TEMPLATE \u001b[39m=\u001b[39m (\n\u001b[1;32m     29\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSorry, your response was incomplete. Please finish your response:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 32\u001b[0m SYSTEM_PROMPT \u001b[39m=\u001b[39m ChatMessagePromptTemplate\u001b[39m.\u001b[39;49mfrom_template(template\u001b[39m=\u001b[39;49mSYSTEM_PROMPT_TEMPLATE)\n\u001b[1;32m     33\u001b[0m CONTINUE_INCOMPLETE_PROMPT \u001b[39m=\u001b[39m HumanMessagePromptTemplate\u001b[39m.\u001b[39mfrom_template(\n\u001b[1;32m     34\u001b[0m     template\u001b[39m=\u001b[39mCONTINUE_INCOMPLETE_PROMPT_TEMPLATE\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m MERGE_INCOMPLETE_RESPONSES_PROMPT \u001b[39m=\u001b[39m HumanMessagePromptTemplate\u001b[39m.\u001b[39mfrom_template(\n\u001b[1;32m     37\u001b[0m     template\u001b[39m=\u001b[39mMERGE_INCOMPLETE_RESPONSES_PROMPT_TEMPLATE\n\u001b[1;32m     38\u001b[0m )\n",
      "File \u001b[0;32m~/github/langchain/langchain/prompts/chat.py:68\u001b[0m, in \u001b[0;36mBaseStringMessagePromptTemplate.from_template\u001b[0;34m(cls, template, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_template\u001b[39m(\u001b[39mcls\u001b[39m, template: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessagePromptTemplate:\n\u001b[1;32m     67\u001b[0m     prompt \u001b[39m=\u001b[39m PromptTemplate\u001b[39m.\u001b[39mfrom_template(template)\n\u001b[0;32m---> 68\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(prompt\u001b[39m=\u001b[39;49mprompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/github/langchain/.venv/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatMessagePromptTemplate\nrole\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "from langchain.concise.rulex import RulEx\n",
    "\n",
    "\n",
    "ru = RulEx(rules=rules)\n",
    "\n",
    "simplified_code = ru(code)\n",
    "\n",
    "print(simplified_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retaining flexbillity\n",
    "\n",
    "Even though the `concise` API is, well, concise, it's still flexible. For example, you can pass a custom `LLM` or `TextSplitter` to any function that uses it. You can also change the default `LLM`, `TextSplitter`, and max tokens in `langchain.concise.config`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1677b440931f40d89ef8be7bf03acb108ce003de0ac9b18e8d43753ea2e7103"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
