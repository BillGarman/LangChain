{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argilla\n",
    "\n",
    "![Argilla - Open-source data platform for LLMs](https://argilla.io/og.png)\n",
    "\n",
    ">[Argilla](https://argilla.io/) is an open-source data curation platform for LLMs.\n",
    "> Using Argilla, everyone can build robust language models through faster data curation \n",
    "> using both human and machine feedback. We provide support for each step in the MLOps cycle, \n",
    "> from data labeling to model monitoring.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/hwchase17/langchain/blob/master/docs/modules/callbacks/examples/argilla.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this guide we will demonstrate how to track the inputs and reponses of your LLM to generate a dataset in Argilla, using the `ArgillaCallbackHandler`.\n",
    "\n",
    "It's useful to keep track of the inputs and outputs of your LLMs to generate datasets for future fine-tuning. This is especially useful when you're using a LLM to generate data for a specific task, such as question answering, summarization, or translation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install argilla --upgrade\n",
    "!pip install openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting API Credentials\n",
    "\n",
    "To get the Argilla API credentials, follow the next steps:\n",
    "\n",
    "1. Go to your Argilla UI.\n",
    "2. Click on your profile picture and go to \"My settings\".\n",
    "3. Then copy the API Key.\n",
    "\n",
    "In Argilla the API URL will be the same as the URL of your Argilla UI.\n",
    "\n",
    "To get the OpenAI API credentials, please visit https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ARGILLA_API_URL\"] = \"\"\n",
    "os.environ[\"ARGILLA_API_KEY\"] = \"\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Argilla\n",
    "\n",
    "To use the `ArgillaCallbackHandler` we will need to create a new `FeedbackDataset` in Argilla to keep track of your LLM experiments. To do so, please use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging.version import parse as parse_version\n",
    "\n",
    "if parse_version(rg.__version__) < parse_version(\"1.8.0\"):\n",
    "    raise RuntimeError(\n",
    "        \"`FeedbackDataset` is only available in Argilla v1.8.0 or higher, please \"\n",
    "        \"upgrade `argilla` as `pip install argilla --upgrade`.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rg.FeedbackDataset(\n",
    "    fields=[\n",
    "        rg.TextField(name=\"prompt\"),\n",
    "        rg.TextField(name=\"response\"),\n",
    "    ],\n",
    "    questions=[\n",
    "        rg.RatingQuestion(\n",
    "            name=\"response-rating\",\n",
    "            description=\"How would you rate the quality of the response?\",\n",
    "            values=[1, 2, 3, 4, 5],\n",
    "            required=True,\n",
    "        ),\n",
    "        rg.TextQuestion(\n",
    "            name=\"response-feedback\",\n",
    "            description=\"What feedback do you have for the response?\",\n",
    "            required=False,\n",
    "        ),\n",
    "    ],\n",
    "    guidelines=\"You're asked to rate the quality of the response and provide feedback.\",\n",
    ")\n",
    "\n",
    "rg.init(\n",
    "    api_url=os.environ[\"ARGILLA_API_URL\"],\n",
    "    api_key=os.environ[\"ARGILLA_API_KEY\"],\n",
    ")\n",
    "\n",
    "dataset.push_to_argilla(\"langchain-dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“Œ NOTE: at the moment, just the prompt-response pairs are supported as `FeedbackDataset.fields`, so the `ArgillaCallbackHandler` will just track the prompt i.e. the LLM input, and the response i.e. the LLM output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `ArgillaCallbackHandler` you can either use the following code, or just reproduce one of the examples presented in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import ArgillaCallbackHandler\n",
    "\n",
    "argilla_callback = ArgillaCallbackHandler(\n",
    "    dataset_name=\"langchain-dataset\",\n",
    "    api_url=os.environ[\"ARGILLA_API_URL\"],\n",
    "    api_key=os.environ[\"ARGILLA_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1: Tracking an LLM\n",
    "\n",
    "First, let's just run a single LLM a few times and capture the resulting prompt-response pairs in Argilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import ArgillaCallbackHandler, StdOutCallbackHandler\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "argilla_callback = ArgillaCallbackHandler(\n",
    "    dataset_name=\"langchain-dataset\",\n",
    "    api_url=os.environ[\"ARGILLA_API_URL\"],\n",
    "    api_key=os.environ[\"ARGILLA_API_KEY\"],\n",
    ")\n",
    "callbacks = [StdOutCallbackHandler(), argilla_callback]\n",
    "\n",
    "llm = OpenAI(temperature=0.9, callbacks=callbacks)\n",
    "llm.generate([\"Tell me a joke\", \"Tell me a poem\"] * 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: Tracking an LLM in a chain\n",
    "\n",
    "Then we can create a chain using a prompt template, and then track the initial prompt and the final response in Argilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import ArgillaCallbackHandler, StdOutCallbackHandler\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "argilla_callback = ArgillaCallbackHandler(\n",
    "    dataset_name=\"langchain-dataset\",\n",
    "    api_url=os.environ[\"ARGILLA_API_URL\"],\n",
    "    api_key=os.environ[\"ARGILLA_API_KEY\"],\n",
    ")\n",
    "callbacks = [StdOutCallbackHandler(), argilla_callback]\n",
    "llm = OpenAI(temperature=0.9, callbacks=callbacks)\n",
    "\n",
    "template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n",
    "Title: {title}\n",
    "Playwright: This is a synopsis for the above play:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
    "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template, callbacks=callbacks)\n",
    "\n",
    "test_prompts = [{\"title\": \"Documentary about Bigfoot in Paris\"}]\n",
    "synopsis_chain.apply(test_prompts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3: Using an Agent with Tools\n",
    "\n",
    "Finally, as a more advanced workflow, you can create an agent that uses some tools. So that `ArgillaCallbackHandler` will keep track of the input and the output, but not about the intermediate steps/thoughts, so that given a prompt we log the original prompt and the final response to that given prompt."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that for this scenario we'll be using Google Search API (Serp API) so you will need to both install `google-search-results` as `pip install google-search-results`, and to set the Serp API Key as `os.environ[\"SERPAPI_API_KEY\"] = \"...\"` (you can find it at https://serpapi.com/dashboard), otherwise the example below won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.callbacks import ArgillaCallbackHandler, StdOutCallbackHandler\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "argilla_callback = ArgillaCallbackHandler(\n",
    "    dataset_name=\"langchain-dataset\",\n",
    "    api_url=os.environ[\"ARGILLA_API_URL\"],\n",
    "    api_key=os.environ[\"ARGILLA_API_KEY\"],\n",
    ")\n",
    "callbacks = [StdOutCallbackHandler(), argilla_callback]\n",
    "llm = OpenAI(temperature=0.9, callbacks=callbacks)\n",
    "\n",
    "tools = load_tools([\"serpapi\"], llm=llm, callbacks=callbacks)\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "agent.run(\"Who was the first president of the United States of America?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "a53ebf4a859167383b364e7e7521d0add3c2dbbdecce4edf676e8c4634ff3fbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
