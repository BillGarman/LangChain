{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca883d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6aab1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SQL Chain example\n",
    "\n",
    "This example demonstrates the use of the `SQLDatabaseChain` for answering questions over a database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f66479",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Under the hood, LangChain uses SQLAlchemy to connect to SQL databases. The `SQLDatabaseChain` can therefore be used with any SQL dialect supported by SQLAlchemy, such as MS SQL, MySQL, MariaDB, PostgreSQL, Oracle SQL, and SQLite. Please refer to the SQLAlchemy documentation for more information about requirements for connecting to your database. For example, a connection to MySQL requires an appropriate connector such as PyMySQL. A URI for a MySQL connection might look like: `mysql+pymysql://user:pass@some_mysql_db_address/db_name`\n",
    "\n",
    "This demonstration uses SQLite and the example Chinook database.\n",
    "To set it up, follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the `.db` file in a notebooks folder at the root of this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e287fa3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e27d88",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI, SQLDatabase, SQLDatabaseChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ede462",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")\n",
    "llm = OpenAI(temperature=0, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e692e",
   "metadata": {},
   "source": [
    "**NOTE:** For data-sensitive projects, you can specify `return_direct=True` in the `SQLDatabaseChain` initialization to directly return the output of the SQL query without any additional formatting. This prevents the LLM from seeing any contents within the database. Note, however, the LLM still has access to the database scheme (i.e. dialect, table and key names) by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fc8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff81df",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "db_chain.run(\"How many employees are there?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf8a5248",
   "metadata": {},
   "source": [
    "## Use Query Checker\n",
    "Sometimes the Language Model generates invalid SQL with small mistakes that can be self-corrected using the same technique used by the SQL Database Agent to try and fix the SQL using the LLM. You can simply specify this option when creating the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e395db",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True, use_query_checker=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain.run(\"List all the albums by Aerosmith?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad2cba6",
   "metadata": {},
   "source": [
    "## Customize Prompt\n",
    "You can also customize the prompt that is used. Here is an example prompting it to understand that foobar is the same as the Employee table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\n",
    "Use the following format:\n",
    "\n",
    "Question: \"Question here\"\n",
    "SQLQuery: \"SQL Query to run\"\n",
    "SQLResult: \"Result of the SQLQuery\"\n",
    "Answer: \"Final answer here\"\n",
    "\n",
    "Only use the following tables:\n",
    "\n",
    "{table_info}\n",
    "\n",
    "If someone asks for the table foobar, they really mean the employee table.\n",
    "\n",
    "Question: {input}\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"table_info\", \"dialect\"], template=_DEFAULT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec47a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain(llm=llm, database=db, prompt=PROMPT, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain.run(\"How many employees are there in the foobar table?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d8b969",
   "metadata": {},
   "source": [
    "## Return Intermediate Steps\n",
    "\n",
    "You can also return the intermediate steps of the SQLDatabaseChain. This allows you to access the SQL statement that was generated, as well as the result of running that against the SQL Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38559487",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain(llm=llm, database=db, prompt=PROMPT, verbose=True, use_query_checker=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b6af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = db_chain(\"How many employees are there in the foobar table?\")\n",
    "result[\"intermediate_steps\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408f800",
   "metadata": {},
   "source": [
    "## Choosing how to limit the number of rows returned\n",
    "If you are querying for several rows of a table you can select the maximum number of results you want to get by using the 'top_k' parameter (default is 10). This is useful for avoiding query results that exceed the prompt max length or consume tokens unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adaa799",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True, use_query_checker=True, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain.run(\"What are some example tracks by composer Johann Sebastian Bach?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc5e936",
   "metadata": {},
   "source": [
    "## Adding example rows from each table\n",
    "Sometimes, the format of the data is not obvious and it is optimal to include a sample of rows from the tables in the prompt to allow the LLM to understand the data before providing a final query. Here we will use this feature to let the LLM know that artists are saved with their full names by providing two rows from the `Track` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a22ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(\n",
    "    \"sqlite:///../../../../notebooks/Chinook.db\",\n",
    "    include_tables=['Track'], # we include only one table to save tokens in the prompt :)\n",
    "    sample_rows_in_table_info=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952c0b4d",
   "metadata": {},
   "source": [
    "The sample rows are added to the prompt after each corresponding table's column information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de86267",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db.table_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain(llm=llm, database=db, use_query_checker=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e05d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain.run(\"What are some example tracks by Bach?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94e948",
   "metadata": {},
   "source": [
    "### Custom Table Info\n",
    "In some cases, it can be useful to provide custom table information instead of using the automatically generated table definitions and the first `sample_rows_in_table_info` sample rows. For example, if you know that the first few rows of a table are uninformative, it could help to manually provide example rows that are more diverse or provide more information to the model. It is also possible to limit the columns that will be visible to the model if there are unnecessary columns. \n",
    "\n",
    "This information can be provided as a dictionary with table names as the keys and table information as the values. For example, let's provide a custom definition and sample rows for the Track table with only a few columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad33ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_table_info = {\n",
    "    \"Track\": \"\"\"CREATE TABLE Track (\n",
    "\t\"TrackId\" INTEGER NOT NULL, \n",
    "\t\"Name\" NVARCHAR(200) NOT NULL,\n",
    "\t\"Composer\" NVARCHAR(220),\n",
    "\tPRIMARY KEY (\"TrackId\")\n",
    ")\n",
    "/*\n",
    "3 rows from Track table:\n",
    "TrackId\tName\tComposer\n",
    "1\tFor Those About To Rock (We Salute You)\tAngus Young, Malcolm Young, Brian Johnson\n",
    "2\tBalls to the Wall\tNone\n",
    "3\tMy favorite song ever\tThe coolest composer of all time\n",
    "*/\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db144352",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(\n",
    "    \"sqlite:///../../../../notebooks/Chinook.db\",\n",
    "    include_tables=['Track', 'Playlist'],\n",
    "    sample_rows_in_table_info=2,\n",
    "    custom_table_info=custom_table_info)\n",
    "\n",
    "print(db.table_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc6f507",
   "metadata": {},
   "source": [
    "Note how our custom table definition and sample rows for `Track` overrides the `sample_rows_in_table_info` parameter. Tables that are not overridden by `custom_table_info`, in this example `Playlist`, will have their table info gathered automatically as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbda4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)\n",
    "db_chain.run(\"What are some example tracks by Bach?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ae15a",
   "metadata": {},
   "source": [
    "## SQLDatabaseSequentialChain\n",
    "\n",
    "Chain for querying SQL database that is a sequential chain.\n",
    "\n",
    "The chain is as follows:\n",
    "\n",
    "    1. Based on the query, determine which tables to use.\n",
    "    2. Based on those tables, call the normal SQL database chain.\n",
    "\n",
    "This is useful in cases where the number of tables in the database is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a4740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SQLDatabaseSequentialChain\n",
    "db = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bb49b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = SQLDatabaseSequentialChain.from_llm(llm, db, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95017b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"How many employees are also customers?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5eb39db6",
   "metadata": {},
   "source": [
    "## Using Local Language Models\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ab11cb9",
   "metadata": {},
   "source": [
    "Sometimes you may not have the luxury of using OpenAI or other service-hosted large language model. You can, ofcourse, try to use the `SQLDatabaseChain` with a local model, but will quickly realize that most models you can run locally even with a large GPU struggle to generate the right output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd21d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2TokenizerFast, pipeline, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "# Note: This model requires a large GPU, e.g. an 80GB A100. See documentation for other ways to run private non-OpenAI models.\n",
    "model_id = \"google/flan-ul2\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, temperature=0)\n",
    "\n",
    "device_id = -1  # default to no-GPU, but use GPU and half precision mode if available\n",
    "if torch.cuda.is_available():\n",
    "    device_id = 0\n",
    "    try:\n",
    "        model = model.half()\n",
    "    except RuntimeError as exc:\n",
    "        logging.warn(f\"Could not run model in half precision mode: {str(exc)}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pipe = pipeline(task=\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=1024, device=device_id)\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89fd8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SQLDatabase, SQLDatabaseChain\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\", include_tables=['Customer'])\n",
    "local_chain = SQLDatabaseChain(llm=local_llm, database=db, verbose=True, return_intermediate_steps=True, use_query_checker=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a339eed",
   "metadata": {},
   "source": [
    "This model should work for very simple SQL queries, as long as you use the query checker as specified above, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a43200",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_chain(\"How many customers are there?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6858f758",
   "metadata": {},
   "source": [
    "Even this relatively large model will most likely fail to generate more complicated SQL by itself. However, you can log its inputs and outputs so that you can hand-correct them and use the corrected examples for few shot prompt examples later. In practice, you could log any executions of your chain that raise exceptions (as shown in the example below) or get direct user feedback in cases where the results are incorrect (but did not raise an exception)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1633c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many customers are not from Brazil?\"\n",
    "try:\n",
    "    result = local_chain(query)\n",
    "    print(result)\n",
    "except Exception as exc:\n",
    "    example = {}\n",
    "    for step in exc.intermediate_steps:\n",
    "        if isinstance(step, dict):\n",
    "            for key in step:\n",
    "                if key == \"input\":\n",
    "                    example[key] = query  # use the original query since the llm adds a suffix\n",
    "                elif key == \"table_info\":\n",
    "                    example[key] = step[key]\n",
    "        else:\n",
    "            example[\"sql_cmd\"] = step\n",
    "\n",
    "    # print for now, in reality you may want to write this out to a YAML file or database for manual fix-ups offline\n",
    "    for key in example:\n",
    "        print(key + \":\")\n",
    "        print(example[key])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da618de6",
   "metadata": {},
   "source": [
    "Run the snippet above a few times, or log exceptions in your deployed environment, to collect lots of examples of inputs, table_info and sql_cmd generated by your language model. The sql_cmd values will be incorrect and you can manually fix them up to build a collection of examples, e.g. here we are using YAML to keep a neat record of our inputs and corrected SQL output that we can build up over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b9d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run pip install pyyaml chromadb\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_EXAMPLES = \"\"\"\n",
    "- input: How many customers are not from Brazil?\n",
    "  table_info: |\n",
    "    CREATE TABLE \"Customer\" (\n",
    "      \"CustomerId\" INTEGER NOT NULL, \n",
    "      \"FirstName\" NVARCHAR(40) NOT NULL, \n",
    "      \"LastName\" NVARCHAR(20) NOT NULL, \n",
    "      \"Company\" NVARCHAR(80), \n",
    "      \"Address\" NVARCHAR(70), \n",
    "      \"City\" NVARCHAR(40), \n",
    "      \"State\" NVARCHAR(40), \n",
    "      \"Country\" NVARCHAR(40), \n",
    "      \"PostalCode\" NVARCHAR(10), \n",
    "      \"Phone\" NVARCHAR(24), \n",
    "      \"Fax\" NVARCHAR(24), \n",
    "      \"Email\" NVARCHAR(60) NOT NULL, \n",
    "      \"SupportRepId\" INTEGER, \n",
    "      PRIMARY KEY (\"CustomerId\"), \n",
    "      FOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\n",
    "    )\n",
    "  sql_cmd: SELECT count(*) from \"Customer\" WHERE NOT \"Country\" = \"Brazil\";\n",
    "- input: list all the genres\n",
    "  table_info: |\n",
    "    CREATE TABLE \"Genre\" (\n",
    "      \"GenreId\" INTEGER NOT NULL, \n",
    "      \"Name\" NVARCHAR(120), \n",
    "      PRIMARY KEY (\"GenreId\")\n",
    "    )\n",
    "\n",
    "    /*\n",
    "    3 rows from Genre table:\n",
    "    GenreId\tName\n",
    "    1\tRock\n",
    "    2\tJazz\n",
    "    3\tMetal\n",
    "    */\n",
    "  sql_cmd: SELECT * from \"Genre\";\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b13b4d5",
   "metadata": {},
   "source": [
    "Now that you have some examples (with manually corrected output SQL), you can do few shot prompt seeding the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d174f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chains.sql_database.prompt import _sqlite_prompt, PROMPT_SUFFIX\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts.example_selector.semantic_similarity import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"table_info\", \"input\", \"sql_cmd\"],\n",
    "    template=\"{table_info}\\n\\nQuestion: {input}\\nSQLQuery: {sql_cmd}\",\n",
    ")\n",
    "\n",
    "examples_dict = yaml.safe_load(YAML_EXAMPLES)\n",
    "\n",
    "local_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "                        # This is the list of examples available to select from.\n",
    "                        examples_dict,\n",
    "                        # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "                        local_embeddings,\n",
    "                        # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "                        Chroma,  # type: ignore\n",
    "                        # This is the number of examples to produce and include per prompt\n",
    "                        k=min(3, len(examples_dict)),\n",
    "                    )\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=_sqlite_prompt + \"Here are some examples:\",\n",
    "    suffix=PROMPT_SUFFIX,\n",
    "    input_variables=[\"table_info\", \"input\", \"top_k\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2324e41f",
   "metadata": {},
   "source": [
    "The model should do better now, especially for inputs similar to the examples you have seeded it with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dcf76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_chain = SQLDatabaseChain(llm=local_llm, database=db, prompt=few_shot_prompt, use_query_checker=True, verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee7a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = local_chain(\"How many customers are not from Brazil?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead61709",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = local_chain(\"How many customers are from Brazil?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af127d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = local_chain(\"How many customers there in total?\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
