{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68f1269",
   "metadata": {},
   "source": [
    "# Evaluating an OpenAPI Chain\n",
    "\n",
    "This notebook goes over ways to semantically evaluate an OpenAPI Chain, which calls an endpoint defined by the OpenAPI specification using purely natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d75d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import OpenAPISpec, APIOperation\n",
    "from langchain.chains import OpenAPIEndpointChain, LLMChain\n",
    "from langchain.requests import Requests\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ff736",
   "metadata": {},
   "source": [
    "## Load the API Chain\n",
    "\n",
    "Load a wrapper of the spec (so we can work with it more easily). You can load from a url or from a local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fae7e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n"
     ]
    }
   ],
   "source": [
    "# Load and parse the OpenAPI Spec\n",
    "spec = OpenAPISpec.from_url(\"https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\")\n",
    "# Load a single endpoint operation\n",
    "operation = APIOperation.from_openapi_spec(spec, '/public/openai/v0/products', \"get\")\n",
    "# Select any LangChain LLM\n",
    "llm = OpenAI()\n",
    "# Create the endpoint chain\n",
    "api_chain = OpenAPIEndpointChain.from_api_operation(\n",
    "    operation, \n",
    "    llm, \n",
    "    requests=Requests(), \n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True # Return request and response text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f740fcd",
   "metadata": {},
   "source": [
    "### *Optional*: Generate Input Questions and Request Ground Truth Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaebf5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "# template = \"\"\"Below is a service description:\n",
    "\n",
    "# {spec}\n",
    "\n",
    "# Imagine you're a new user trying to use {operation} through a search bar. What are 10 different things you want to request?\n",
    "# Wants/Questions:\n",
    "# 1. \"\"\"\n",
    "\n",
    "# prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# generation_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# questions_ = generation_chain.run(spec=operation.to_typescript(), operation=operation.operation_id).split('\\n')\n",
    "# # Strip preceding numeric bullets\n",
    "# questions = [re.sub(r'^\\d+\\. ', '', q).strip() for q in questions_]\n",
    "# questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfb49c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truths = [\n",
    "# {\"q\": ...} # What are the best queries for each input?\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cddf90",
   "metadata": {},
   "source": [
    "## Run the API Chain\n",
    "\n",
    "The two simplest questions a user of the API Chain are:\n",
    "- Did the chain succesfully access the endpoint?\n",
    "- Did the action accomplish the correct result?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a424c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Collect metrics to report at completion\n",
    "scores = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42be069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "     'What iPhone models are available?',\n",
    "     'Are there any budget laptops?',\n",
    "     'Show me the cheapest gaming PC.',\n",
    "     'Are there any tablets under $400?',\n",
    "     'What are the best headphones?',\n",
    "     'What are the top rated laptops?',\n",
    "     'I want to buy some shoes. I like Adidas and Nike.',\n",
    "     'I want to buy a new skirt',\n",
    "     'My company is asking me to get a professional Deskopt PC - money is no object.',\n",
    "     'What are the best budget cameras?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf20737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new OpenAPIEndpointChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new APIRequesterChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful AI Assistant. Please provide JSON arguments to agentFunc() based on the user's instructions.\n",
      "\n",
      "API_SCHEMA: ```typescript\n",
      "type productsUsingGET = (_: {\n",
      "/* A precise query that matches one very small category or product that needs to be searched for to find the products the user is looking for. If the user explicitly stated what they want, use that as a query. The query is as specific as possible to the product name or category mentioned by the user in its singular form, and don't contain any clarifiers like latest, newest, cheapest, budget, premium, expensive or similar. The query is always taken from the latest topic, if there is a new topic a new query is started. */\n",
      "\t\tq: string,\n",
      "/* number of products returned */\n",
      "\t\tsize?: number,\n",
      "/* (Optional) Minimum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for. */\n",
      "\t\tmin_price?: number,\n",
      "/* (Optional) Maximum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for. */\n",
      "\t\tmax_price?: number,\n",
      "}) => any;\n",
      "```\n",
      "\n",
      "USER_INSTRUCTIONS: \"What iPhone models are available?\"\n",
      "\n",
      "Your arguments must be plain json provided in a markdown block:\n",
      "\n",
      "ARGS: ```json\n",
      "{valid json conforming to API_SCHEMA}\n",
      "```\n",
      "\n",
      "Example\n",
      "-----\n",
      "\n",
      "ARGS: ```json\n",
      "{\"foo\": \"bar\", \"baz\": {\"qux\": \"quux\"}}\n",
      "```\n",
      "\n",
      "The block must be no more than 1 line long, and all arguments must be valid JSON. All string arguments must be wrapped in double quotes.\n",
      "You MUST strictly comply to the types indicated by the provided schema, including all required args.\n",
      "\n",
      "If you don't have sufficient information to call the function due to things like requiring specific uuid's, you can reply with the following message:\n",
      "\n",
      "Message: ```text\n",
      "Concise response requesting the additional information that would make calling the function successful.\n",
      "```\n",
      "\n",
      "Begin\n",
      "-----\n",
      "ARGS:\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Run the the API chain itself\n",
    "chain_outputs = []\n",
    "failed_examples = []\n",
    "for question in questions:\n",
    "    try:\n",
    "        chain_outputs.append(api_chain(question))\n",
    "        scores[\"completed\"].append(1.0)\n",
    "    except Exception as e:\n",
    "        failed_examples.append({'q': question, 'error': e})\n",
    "        scores[\"completed\"].append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c615d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the chain failed to run, show the failing examples\n",
    "failed_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44c36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [res['output'] for res in chain_outputs]\n",
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b3a6cf",
   "metadata": {},
   "source": [
    "## Evaluate the requests chain\n",
    "\n",
    "The API Chain has two main components:\n",
    "1. Translate the user query to an API request\n",
    "2. Translate the API response to a natural language response\n",
    "\n",
    "Here, we construct an evaluation chain to grade the request synthesizer against selected human queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54890ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define ground truth labels\n",
    "truth_queries = [\n",
    "    {\"q\": \"iPhone\"},\n",
    "    {\"q\": \"laptop\", \"max_price\": 300},\n",
    "    {\"q\": \"tablet\"},\n",
    "    {\"q\": \"headphone\"},\n",
    "    {\"q\": \"laptop\", \"max_price\": 400},\n",
    "    {\"q\": \"shoe\"},\n",
    "    {\"q\": \"skirt\"},\n",
    "    {\"q\": \"professional desktop PC\", \"max_price\": 10000},\n",
    "    {\"q\": \"camera\", \"max_price\": 300},\n",
    "]\n",
    "truth_queries = [json.dumps(q) for q in truth_queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d03b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the API queries generated by the chain\n",
    "predicted_queries = [output[\"intermediate_steps\"][\"request_args\"] for output in chain_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4741fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are trying to answer the following question by querying an API:\n",
    "\n",
    "> Question: {question}\n",
    "\n",
    "The query you know you should be executing against the API is:\n",
    "\n",
    "> Query: {truth_query}\n",
    "\n",
    "Is the following predicted query semantically the same (eg likely to produce the same answer)?\n",
    "\n",
    "> Predicted Query: {predict_query}\n",
    "\n",
    "Please give the Predicted Query a grade of either an A, B, C, D, or F, along with an explanation of why. End the evaluation with 'Final Grade: <the letter>'\n",
    "\n",
    "> Explanation: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "eval_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7652094",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_eval_results = []\n",
    "for question, predict_query, truth_query in list(zip(questions, predicted_queries, truth_queries))[:3]:\n",
    "    eval_output = eval_chain.run(\n",
    "        question=question,\n",
    "        truth_query=truth_query,\n",
    "        predict_query=predict_query,\n",
    "    )\n",
    "    request_eval_results.append(eval_output)\n",
    "request_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ddfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "# Parse the evaluation chain responses into a rubric\n",
    "def parse_eval_results(results: List[str]) -> List[float]:\n",
    "    rubric = {\n",
    "        \"A\": 1.0,\n",
    "        \"B\": 0.75,\n",
    "        \"C\": 0.5,\n",
    "        \"D\": 0.25,\n",
    "        \"F\": 0\n",
    "    }\n",
    "    return [rubric[re.search(r'Final Grade: (\\w+)', res).group(1)] for res in results]\n",
    "\n",
    "\n",
    "parsed_results = parse_eval_results(request_eval_results)\n",
    "# Collect the scores for a final evaluation table\n",
    "scores['request_synthesizer'].extend(parsed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13750d",
   "metadata": {},
   "source": [
    "## Evaluate the Response Chain\n",
    "\n",
    "The second component translated the structured API response to a natural language response.\n",
    "Evaluate this against the user's original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5183424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are trying to answer the following question by querying an API:\n",
    "\n",
    "> Question: {question}\n",
    "\n",
    "The API returned a response of:\n",
    "\n",
    "> Response: {api_response}\n",
    "\n",
    "You told the user: {answer}\n",
    "\n",
    "Please evaluate the accuracy and utility of your response to the user's original question.\n",
    "Give a letter grade of either an A, B, C, D, or F, along with an explanation of why. End the evaluation with 'Final Grade: <the letter>'\n",
    "\n",
    "> Explanation: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "eval_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the API responses from the chain\n",
    "api_responses = [output[\"intermediate_steps\"][\"response_text\"] for output in chain_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bcbc01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the grader chain\n",
    "response_eval_results = []\n",
    "for question, api_response, answer in list(zip(questions, api_responses, answers))[:3]:\n",
    "    request_eval_results.append(eval_chain.run(question=question, api_response=api_response, answer=answer))\n",
    "request_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1367984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusing the rubric from above, parse the evaluation chain responses\n",
    "parsed_response_results = parse_eval_results(request_eval_results)\n",
    "# Collect the scores for a final evaluation table\n",
    "scores['result_synthesizer'].extend(parsed_response_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b3478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out Score statistics for the evaluation session\n",
    "header = \"{:<20}\\t{:<10}\\t{:<10}\\t{:<10}\".format(\"Metric\", \"Min\", \"Mean\", \"Max\")\n",
    "print(header)\n",
    "for metric, metric_scores in scores.items():\n",
    "    mean_scores = sum(metric_scores) / len(metric_scores) if len(metric_scores) > 0 else float('nan')\n",
    "    row = \"{:<20}\\t{:<10.2f}\\t{:<10.2f}\\t{:<10.2f}\".format(metric, min(metric_scores), mean_scores, max(metric_scores))\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3baef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-show the examples for which the chain failed to complete\n",
    "failed_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114dee3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
