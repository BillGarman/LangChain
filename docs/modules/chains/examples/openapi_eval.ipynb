{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692f3256",
   "metadata": {},
   "source": [
    "# Evaluating an OpenAPI Chain\n",
    "\n",
    "This notebook goes over ways to semantically evaluate an [OpenAPI Chain](openapi.ipynb), which calls an endpoint defined by the OpenAPI specification using purely natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a457106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import OpenAPISpec, APIOperation\n",
    "from langchain.chains import OpenAPIEndpointChain, LLMChain\n",
    "from langchain.requests import Requests\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b0954",
   "metadata": {},
   "source": [
    "## Load the API Chain\n",
    "\n",
    "Load a wrapper of the spec (so we can work with it more easily). You can load from a url or from a local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794142ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n"
     ]
    }
   ],
   "source": [
    "# Load and parse the OpenAPI Spec\n",
    "spec = OpenAPISpec.from_url(\"https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\")\n",
    "# Load a single endpoint operation\n",
    "operation = APIOperation.from_openapi_spec(spec, '/public/openai/v0/products', \"get\")\n",
    "verbose = False\n",
    "raise_error = False # Stop on first failed example - useful for development\n",
    "# Select any LangChain LLM\n",
    "llm = OpenAI(temperature=0, max_tokens=1000)\n",
    "# Create the endpoint chain\n",
    "api_chain = OpenAPIEndpointChain.from_api_operation(\n",
    "    operation, \n",
    "    llm, \n",
    "    requests=Requests(), \n",
    "    verbose=verbose,\n",
    "    return_intermediate_steps=True # Return request and response text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c05ba5b",
   "metadata": {},
   "source": [
    "### *Optional*: Generate Input Questions and Request Ground Truth Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c0cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "# template = \"\"\"Below is a service description:\n",
    "\n",
    "# {spec}\n",
    "\n",
    "# Imagine you're a new user trying to use {operation} through a search bar. What are 10 different things you want to request?\n",
    "# Wants/Questions:\n",
    "# 1. \"\"\"\n",
    "\n",
    "# prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# generation_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# questions_ = generation_chain.run(spec=operation.to_typescript(), operation=operation.operation_id).split('\\n')\n",
    "# # Strip preceding numeric bullets\n",
    "# questions = [re.sub(r'^\\d+\\. ', '', q).strip() for q in questions_]\n",
    "# questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3d767ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truths = [\n",
    "# {\"q\": ...} # What are the best queries for each input?\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81098a05",
   "metadata": {},
   "source": [
    "## Run the API Chain\n",
    "\n",
    "The two simplest questions a user of the API Chain are:\n",
    "- Did the chain succesfully access the endpoint?\n",
    "- Did the action accomplish the correct result?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64bc7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Collect metrics to report at completion\n",
    "scores = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfd2d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "     'What iPhone models are available?',\n",
    "     'Are there any budget laptops?',\n",
    "     'Show me the cheapest gaming PC.',\n",
    "     'Are there any tablets under $400?',\n",
    "     'What are the best headphones?',\n",
    "     'What are the top rated laptops?',\n",
    "     'I want to buy some shoes. I like Adidas and Nike.',\n",
    "     'I want to buy a new skirt',\n",
    "     'My company is asking me to get a professional Deskopt PC - money is no object.',\n",
    "     'What are the best budget cameras?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00511f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the the API chain itself\n",
    "chain_outputs = []\n",
    "failed_examples = []\n",
    "for question in questions:\n",
    "    try:\n",
    "        chain_outputs.append(api_chain(question))\n",
    "        scores[\"completed\"].append(1.0)\n",
    "    except Exception as e:\n",
    "        if raise_error:\n",
    "            raise e\n",
    "        failed_examples.append({'q': question, 'error': e})\n",
    "        scores[\"completed\"].append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the chain failed to run, show the failing examples\n",
    "failed_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e7587",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [res['output'] for res in chain_outputs]\n",
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484f0587",
   "metadata": {},
   "source": [
    "## Evaluate the requests chain\n",
    "\n",
    "The API Chain has two main components:\n",
    "1. Translate the user query to an API request\n",
    "2. Translate the API response to a natural language response\n",
    "\n",
    "Here, we construct an evaluation chain to grade the request synthesizer against selected human queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define ground truth labels\n",
    "truth_queries = [\n",
    "    {\"q\": \"iPhone\"},\n",
    "    {\"q\": \"laptop\", \"max_price\": 300},\n",
    "    {\"q\": \"tablet\"},\n",
    "    {\"q\": \"headphone\"},\n",
    "    {\"q\": \"laptop\", \"max_price\": 400},\n",
    "    {\"q\": \"shoe\"},\n",
    "    {\"q\": \"skirt\"},\n",
    "    {\"q\": \"professional desktop PC\", \"max_price\": 10000},\n",
    "    {\"q\": \"camera\", \"max_price\": 300},\n",
    "]\n",
    "truth_queries = [json.dumps(q) for q in truth_queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the API queries generated by the chain\n",
    "predicted_queries = [output[\"intermediate_steps\"][\"request_args\"] for output in chain_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are trying to answer the following question by querying an API:\n",
    "\n",
    "> Question: {question}\n",
    "\n",
    "The query you know you should be executing against the API is:\n",
    "\n",
    "> Query: {truth_query}\n",
    "\n",
    "Is the following predicted query semantically the same (eg likely to produce the same answer)?\n",
    "\n",
    "> Predicted Query: {predict_query}\n",
    "\n",
    "Please give the Predicted Query a grade of either an A, B, C, D, or F, along with an explanation of why. End the evaluation with 'Final Grade: <the letter>'\n",
    "\n",
    "> Explanation: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "eval_chain = LLMChain(llm=llm, prompt=prompt, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_eval_results = []\n",
    "for question, predict_query, truth_query in list(zip(questions, predicted_queries, truth_queries)):\n",
    "    eval_output = eval_chain.run(\n",
    "        question=question,\n",
    "        truth_query=truth_query,\n",
    "        predict_query=predict_query,\n",
    "    )\n",
    "    request_eval_results.append(eval_output)\n",
    "request_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d76f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "# Parse the evaluation chain responses into a rubric\n",
    "def parse_eval_results(results: List[str]) -> List[float]:\n",
    "    rubric = {\n",
    "        \"A\": 1.0,\n",
    "        \"B\": 0.75,\n",
    "        \"C\": 0.5,\n",
    "        \"D\": 0.25,\n",
    "        \"F\": 0\n",
    "    }\n",
    "    return [rubric[re.search(r'Final Grade: (\\w+)', res).group(1)] for res in results]\n",
    "\n",
    "\n",
    "parsed_results = parse_eval_results(request_eval_results)\n",
    "# Collect the scores for a final evaluation table\n",
    "scores['request_synthesizer'].extend(parsed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ee8ea",
   "metadata": {},
   "source": [
    "## Evaluate the Response Chain\n",
    "\n",
    "The second component translated the structured API response to a natural language response.\n",
    "Evaluate this against the user's original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b97847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are trying to answer the following question by querying an API:\n",
    "\n",
    "> Question: {question}\n",
    "\n",
    "The API returned a response of:\n",
    "\n",
    "> API result: {api_response}\n",
    "\n",
    "Your response to the user: {answer}\n",
    "\n",
    "Please evaluate the accuracy and utility of your response to the user's original question, conditioned on the information available.\n",
    "Give a letter grade of either an A, B, C, D, or F, along with an explanation of why. End the evaluation with 'Final Grade: <the letter>'\n",
    "\n",
    "> Explanation: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "eval_chain = LLMChain(llm=llm, prompt=prompt, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642852ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the API responses from the chain\n",
    "api_responses = [output[\"intermediate_steps\"][\"response_text\"] for output in chain_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a5eb4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the grader chain\n",
    "response_eval_results = []\n",
    "for question, api_response, answer in list(zip(questions, api_responses, answers)):\n",
    "    request_eval_results.append(eval_chain.run(question=question, api_response=api_response, answer=answer))\n",
    "request_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusing the rubric from above, parse the evaluation chain responses\n",
    "parsed_response_results = parse_eval_results(request_eval_results)\n",
    "# Collect the scores for a final evaluation table\n",
    "scores['result_synthesizer'].extend(parsed_response_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95042bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out Score statistics for the evaluation session\n",
    "header = \"{:<20}\\t{:<10}\\t{:<10}\\t{:<10}\".format(\"Metric\", \"Min\", \"Mean\", \"Max\")\n",
    "print(header)\n",
    "for metric, metric_scores in scores.items():\n",
    "    mean_scores = sum(metric_scores) / len(metric_scores) if len(metric_scores) > 0 else float('nan')\n",
    "    row = \"{:<20}\\t{:<10.2f}\\t{:<10.2f}\\t{:<10.2f}\".format(metric, min(metric_scores), mean_scores, max(metric_scores))\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-show the examples for which the chain failed to complete\n",
    "failed_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee43877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
