{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31df93e",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "This notebook walks through how LangChain thinks about memory. \n",
    "\n",
    "Memory involves keeping a concept of state around throughout a user's interactions with an language model. A user's interactions with a language model are captured in the concept of ChatMessages, so this boils down to ingesting, capturing, transforming and extracting knowledge from a sequence of chat messages. There are many different ways to do this, each of which exists as its own memory type.\n",
    "\n",
    "In general, for each type of memory there are two ways to understanding using memory. These are the standalone functions which extract information from a sequence of messages, and then there is the way you can use this type of memory in a chain. \n",
    "\n",
    "Memory can return multiple pieces of information (for example, the most recent N messages and a summary of all previous messages). The returned information can either be a string or a list of messages.\n",
    "\n",
    "In this notebook, we will walk through the simplest form of memory: \"buffer\" memory, which just involves keeping a buffer of all prior messages. We will show how to use the modular utility functions here, then show how it can be used in a chain (both returning a string as well as a list of messages).\n",
    "\n",
    "## ChatMessageHistory\n",
    "One of the core utility classes underpinning most (if not all) memory modules is the `ChatMessageHistory` class. This is a super lightweight wrapper which exposes convienence methods for saving Human messages, AI messages, and then fetching them all. \n",
    "\n",
    "You may want to use this class directly if you are managing memory outside of a chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87235cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")\n",
    "\n",
    "history.add_ai_message(\"whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be030822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi!', additional_kwargs={}, example=False),\n",
       " AIMessage(content='whats up?', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0328fb",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory\n",
    "\n",
    "We now show how to use this simple concept in a chain. We first showcase `ConversationBufferMemory` which is just a wrapper around ChatMessageHistory that extracts the messages in a variable.\n",
    "\n",
    "We can first extract it as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a382b160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a280d337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"hi!\")\n",
    "memory.chat_memory.add_ai_message(\"whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b739c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: hi!\\nAI: whats up?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e9425",
   "metadata": {},
   "source": [
    "We can also get the history as a list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798ceb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.chat_memory.add_user_message(\"hi!\")\n",
    "memory.chat_memory.add_ai_message(\"whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "698688fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi!', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='whats up?', additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb68bb9e",
   "metadata": {},
   "source": [
    "## Saving Message History\n",
    "\n",
    "You may often have to save messages, and then load them to use again. This can be done easily by first converting the messages to normal python dictionaries, saving those (as json or something) and then loading those. Here is an example of doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5acbc4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")\n",
    "\n",
    "history.add_ai_message(\"whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7812ee21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dicts = messages_to_dict(history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ed6e6a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'human',\n",
       "  'data': {'content': 'hi!', 'additional_kwargs': {}, 'example': False}},\n",
       " {'type': 'ai',\n",
       "  'data': {'content': 'whats up?', 'additional_kwargs': {}, 'example': False}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdf4ebd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_messages = messages_from_dict(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9724e24b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi!', additional_kwargs={}, example=False),\n",
       " AIMessage(content='whats up?', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7651c-8de2-4548-8a69-d6e086ab22c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using Memory\n",
    "\n",
    "You can use a memory on a `Chain` object that accepts a memory object such as `LLMChain`. This is the recommended way in most use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a38e8268-c55a-43f2-8881-345aa2a76afc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = \"\"\"\\\n",
    "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = LLMChain(\n",
    "    prompt=PromptTemplate.from_template(prompt),\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b7be86a-d807-4f01-9461-b2df4192a6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fff2f7ce-a983-4acc-b986-db3f782a0269",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! It's nice to meet you. How can I help you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09678da-c046-456e-afaf-ad6f80a92be9",
   "metadata": {},
   "source": [
    "Under the hood, `LLMChain` format the prompt template using both the input variables passed by the users as well as the memory key values provided by the memory object. You can inspect the memory variable name through `memory_variables` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42192ad2-2dd3-4d0a-be2a-eed930f42aba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['history']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.memory_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aec057a-d14f-40fe-ac23-285944be1020",
   "metadata": {},
   "source": [
    "If you wish to have a different memory key name, it is recommended to do it during initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62df66ff-53ba-4efe-bd08-02200672de51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{conversation_history}\n",
    "Human: {input}\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"conversation_history\")\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = LLMChain(\n",
    "    prompt=PromptTemplate.from_template(prompt),\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddc90ba6-6b16-4e62-a9d1-373f295eb701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello\n",
      "AI:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello there! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd5dd0-fc5c-4ceb-ae3f-d8b3cab8016a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Aside from using it in a `Chain`, you can also use a memory by directly calling its exposed methods (`load_memory_variables` and `save_context`). It is not recommended to do so unless you have good understanding of the underlying implementation of the memory class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c27c932-a260-4d0c-9512-7dac8bda1670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversation_history': \"Human: Hello\\nAI: Hello there! It's nice to meet you. How can I help you today?\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f9d4a-1aa2-446d-a1e0-14d81a22d1a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Category of memory\n",
    "\n",
    "Conceptually, it is helpful to categorize the memory available in LangChain into three categories, based on their target use cases.\n",
    "\n",
    "1. Chat Message Memory\n",
    "\n",
    "Majority of the memory class in LangChain is chat message memory. This class of memory is specialized in storing chat message history in a conversation with an LLM model. The class wraps around a `BaseChatMessageHistory` object, controlling how the chat message history is presented to the LLM model.\n",
    "\n",
    "You can easily distinguish this type of memory class through the presence of `chat_memory` field during initialization. Essentially, `chat_memory` variable holds the `BaseChatMessageHistory` object being wrapped by the memory class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32c086cc-9572-4a9b-aa6b-5db6a9fcf893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ChatMessageHistory\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "# ConversationBufferMemory is a chat message memory object.\n",
    "memory = ConversationBufferMemory(\n",
    "    chat_memory=chat_history # <-- This is the `BaseChatMessageHistory` object\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2e5d7f9-2f02-4d5c-9039-881e122ab36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=ChatOpenAI(temperature=0),\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78c7c7bc-f0fc-4bf0-a4ce-35c06f6d1ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello there! How can I assist you today?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9de24519-e97d-4915-b567-24a10d57c869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello!\n",
      "AI: Hello there! How can I assist you today?\n",
      "Human: How is your day?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As an AI, I don\\'t really have \"days\" in the traditional sense, but I\\'m functioning well and ready to help you with whatever you need. How can I assist you today?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"How is your day?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c738bcd7-376d-4f1d-83bb-352f3f76aa28",
   "metadata": {},
   "source": [
    "There is a lot more about chat message memory. You can learn more [here](../tutorials/chat_message_history.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12812c36-367b-476d-bc91-96d0c39ba9ee",
   "metadata": {},
   "source": [
    "2. Generic Memory\n",
    "\n",
    "Memory class that is not tied to any use case. You can use this type of memory class to store just about anything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac988b22-52de-4a2e-ad7a-0173788553b2",
   "metadata": {},
   "source": [
    "You can find a list of generic memory classes [here](./how_to_guides.rst)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e340c8a8-2075-4be5-b6a5-4eefe61f0e29",
   "metadata": {
    "tags": []
   },
   "source": [
    "3. Decorator Memory\n",
    "\n",
    "Decorator memory wraps around other memory classes and add additional functionalities to them, much like a Python decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08a190b-fab0-4053-9966-94df7eecff67",
   "metadata": {},
   "source": [
    "You can find a list of decorator memory classes [here](./how_to_guides.rst)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
