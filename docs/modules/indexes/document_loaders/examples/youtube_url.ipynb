{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2523f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.blob_splitters.split_audio import AudioSplitter\n",
    "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9102500",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.youtube.com/watch?v=VMj-3S1tku0&list=UUXUPKJO5MZQN11PqgIvyuvQ&index=10&pp=iAQB\"\n",
    "save_dir = \"/Users/31treehaus/Desktop/AI/langchain-fork/\"\n",
    "loader = YoutubeAudioLoader([url],save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e05c60d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=VMj-3S1tku0&list=UUXUPKJO5MZQN11PqgIvyuvQ&index=10&pp=iAQB\n",
      "[youtube:tab] Downloading just the video VMj-3S1tku0 because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=VMj-3S1tku0\n",
      "[youtube] VMj-3S1tku0: Downloading webpage\n",
      "[youtube] VMj-3S1tku0: Downloading android player API JSON\n",
      "Writing file: The spelled-out intro to neural networks and backpropagation: building micrograd to /Users/31treehaus/Desktop/AI/langchain-fork/\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=VMj-3S1tku0&list=UUXUPKJO5MZQN11PqgIvyuvQ&index=10&pp=iAQB\n",
      "[youtube:tab] Downloading just the video VMj-3S1tku0 because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=VMj-3S1tku0\n",
      "[youtube] VMj-3S1tku0: Downloading webpage\n",
      "[youtube] VMj-3S1tku0: Downloading android player API JSON\n",
      "[info] VMj-3S1tku0: Downloading 1 format(s): 140\n",
      "[download] /Users/31treehaus/Desktop/AI/langchain-fork//The spelled-out intro to neural networks and backpropagation： building micrograd.m4a has already been downloaded\n",
      "[download] 100% of  134.98MiB\n",
      "[ExtractAudio] Not converting audio /Users/31treehaus/Desktop/AI/langchain-fork//The spelled-out intro to neural networks and backpropagation： building micrograd.m4a; file is already in target format m4a\n",
      "data=None mimetype='audio/mp4a-latm' encoding='utf-8' path=PosixPath('/Users/31treehaus/Desktop/AI/langchain-fork/The spelled-out intro to neural networks and backpropagation： building micrograd.m4a')\n"
     ]
    }
   ],
   "source": [
    "for blob in loader.yield_blobs():\n",
    "    print(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b96411e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data=None mimetype='audio/mp4a-latm' encoding='utf-8' path=PosixPath('/Users/31treehaus/Desktop/AI/langchain-fork/The spelled-out intro to neural networks and backpropagation： building micrograd.m4a')\n"
     ]
    }
   ],
   "source": [
    "print(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caafffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import openai\n",
    "for split_blob in AudioSplitter().lazy_split(blob):\n",
    "    file_obj = io.BytesIO(split_blob.data)\n",
    "    file_obj.name = \"input_split.wav\"\n",
    "    transcript = openai.Audio.transcribe(\"whisper-1\",file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1ede3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x11bc1bd10> JSON: {\n",
       "  \"text\": \"The one that we used was the mean squared error loss because it's the simplest one. There's also the binary cross-entropy loss. All of them can be used for binary classification and don't make too much of a difference in the simple examples that we looked at so far. There's something called L2 regularization used here. This has to do with generalization of the neural net and controls the overfitting in machine learning setting, but I did not cover these concepts in this video, potentially later. And the training loop you should recognize. So forward, backward, with, zero grad, and update, and so on. You'll notice that in the update here, the learning rate is scaled as a function of number of iterations and it shrinks. And this is something called learning rate decay. So in the beginning, you have a high learning rate. And as the network sort of stabilizes near the end, you bring down the learning rate to get some of the fine details in the end. And in the end, we see the decision surface of the neural net and we see that it learned to separate out the red and the blue area based on the data points. So that's the slightly more complicated example in the demo.ipy.yamb that you're free to go over. But yeah, as of today, that is micrograd. I also wanted to show you a little bit of real stuff so that you get to see how this is actually implemented in a production-grade library like PyTorch. So in particular, I wanted to show, I wanted to find and show you the backward pass for 10h in PyTorch. So here in micrograd, we see that the backward pass for 10h is 1 minus t square, where t is the output of the 10h of x, times out that grad, which is the chain rule. So we're looking for something that looks like this. Now, I went to PyTorch, which has an open source GitHub code base, and I looked through a lot of its code. And honestly, I spent about 15 minutes and I couldn't find 10h. And that's because these libraries, unfortunately, they grow in size and entropy. And if you just search for 10h, you get apparently 2,800 results and 406 files. So I don't know what these files are doing, honestly, and why there are so many mentions of 10h. But unfortunately, these libraries are quite complex. They're meant to be used, not really inspected. Eventually, I did stumble on someone who tries to change the 10h backward code for some reason. And someone here pointed to the CPU kernel and the CUDA kernel for 10h backward. So this, so basically, it depends on if you're using PyTorch on a CPU device or on a GPU, which these are different devices, and I haven't covered this. But this is the 10h backward kernel for CPU. And the reason it's so large is that, number one, this is like if you're using a complex type, which we haven't even talked about. If you're using a specific data type of BFloat16, which we haven't talked about. And then if you're not, then this is the kernel. And deep here, we see something that resembles our backward pass. So they have A times 1 minus B squared. So this B here must be the output of the 10h, and this is the out.grad. So here we found it deep inside PyTorch on this location, for some reason inside binary ops kernel, when 10h is not actually a binary op. And then this is the GPU kernel. We're not complex. We're here, and here we go with one line of code. So we did find it, but basically, unfortunately, these code bases are very large, and micrograd is very, very simple. But if you actually want to use real stuff, finding the code for it, you'll actually find that difficult. I also wanted to show you a little example here, where PyTorch is showing you how you can register a new type of function that you want to add to PyTorch as a Lego building block. So here, if you want to, for example, add a Legendre polynomial 3, here's how you could do it. You will register it as a class that subclasses Torch.rgrad.function. And then you have to tell PyTorch how to forward your new function and how to backward through it. So as long as you can do the forward pass of this little function piece that you want to add, and as long as you know the local derivative, the local gradients, which are implemented in the backward, PyTorch will be able to back propagate through your function, and then you can use this as a Lego block in a larger Lego castle of all the different Lego blocks that PyTorch already has. And so that's the only thing you have to tell PyTorch, and everything would just work. And you can register new types of functions in this way, following this example. And that is everything that I wanted to cover in this lecture. So I hope you enjoyed building out micrograd with me. I hope you find it interesting, insightful, and yeah, I will probably see you in the next video. And yeah, I will post a lot of the links that are related to this video in the video description below. I will also probably post a link to a discussion forum or discussion group where you can ask questions related to this video, and then I can answer or someone else can answer your questions. And I may also do a follow-up video that answers some of the most common questions. But for now, that's it. I hope you enjoyed it. If you did, then please like and subscribe so that YouTube knows to feature this video to more people. And that's it for now. I'll see you later. Now here's the problem. We know DL by... Wait, what is the problem? And that's everything I wanted to cover in this lecture. So I hope you enjoyed us building out micrograd... micrograb? Okay, now let's do the exact same thing for multiply because we can't do something like A times 2. Oops. I know what happened there.\"\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
