---
sidebar_position: 4
sidebar_label: Trajectory Evaluators
---
# Trajectory Evaluators

Trajectory Evaluators in LangChain provide a holistic approach to evaluating an agent. These evaluators assess the full sequence of actions taken by an agent and their corresponding responses, which we refer to as the "trajectory". This comprehensive approach allows you to better gauge an agent's effectiveness and capabilities.

A Trajectory Evaluator implements the `AgentTrajectoryEvaluator` interface, which requires two main methods:

- `evaluate_agent_trajectory`: This method synchronously evaluates an agent's trajectory.
- `aevaluate_agent_trajectory`: This asynchronous counterpart allows evaluations to be run in parallel for efficiency.

Both methods accept three main parameters:

- `input`: The initial input given to the agent.
- `prediction`: The final predicted response from the agent.
- `agent_trajectory`: The intermediate steps taken by the agent, given as a list of tuples.

These methods return a dictionary with a `score` (a float from 0 to 1 indicating the effectiveness of the agent, where 1 means "most effective" and 0 means "least effective") and `reasoning` (a string explaining the reasoning behind the score).

You can capture an agent's trajectory by initializing the agent with the `return_intermediate_steps=True` parameter. This lets you collect all intermediate steps without relying on special tracing callbacks.

Additionally, you can specify the list of valid tools an agent is permitted to use via the `agent_tools` argument. 

For a deeper dive into the implementation and use of Trajectory Evaluators, refer to the sections below.

import DocCardList from "@theme/DocCardList";

<DocCardList />

