{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "681a5d1e",
   "metadata": {},
   "source": [
    "## Run Template\n",
    "\n",
    "In `server.py`, set -\n",
    "```\n",
    "add_routes(app, chain_rag_conv, path=\"/rag-chroma-multi-modal-multi-vector\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve.client import RemoteRunnable\n",
    "\n",
    "rag_app = RemoteRunnable(\"http://localhost:8001/rag-chroma-multi-modal-multi-vector\")\n",
    "rag_app.invoke(\"What is the projected TAM for observability expected for each year through 2026?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a94c02-1f0e-4e38-a1df-572d95913e01",
   "metadata": {},
   "source": [
    "## TMP (TODO: Remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9a58eb6-fcd5-4f2b-ae86-891ebf6735c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema.messages import HumanMessage\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.storage import UpstashRedisByteStore\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string.\n",
    "\n",
    "    :param base64_string: A Base64 encoded string of the image to be resized.\n",
    "    :param size: A tuple representing the new size (width, height) for the image.\n",
    "    :return: A Base64 encoded string of the resized image.\n",
    "    \"\"\"\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_resized_images(docs):\n",
    "    \"\"\"\n",
    "    Resize images from base64-encoded strings.\n",
    "\n",
    "    :param docs: A list of base64-encoded image to be resized.\n",
    "    :return: Dict containing a list of resized base64-encoded strings.\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    for doc in docs:\n",
    "        # Convert from bytes and get b64 str from the Document JSON\n",
    "        doc = json.loads(doc.decode('utf-8'))['kwargs']['page_content']\n",
    "        resized_image = resize_base64_image(doc, size=(1280, 720))\n",
    "        b64_images.append(resized_image)\n",
    "    return {\"images\": b64_images}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict, num_images=2):\n",
    "    \"\"\"\n",
    "    GPT-4V prompt for image analysis.\n",
    "\n",
    "    :param data_dict: A dict with images and a user-provided question.\n",
    "    :param num_images: Number of images to include in the prompt.\n",
    "    :return: A list containing message objects for each image and the text prompt.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"][:num_images]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are an analyst tasked with answering questions about visual content.\\n\"\n",
    "            \"You will be give a set of image(s) from a slide deck / presentation.\\n\"\n",
    "            \"Use this information to answer the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain,\n",
    "\n",
    "    :param retriever: A function that retrieves the necessary context for the model.\n",
    "    :return: A chain of functions representing the multi-modal RAG process.\n",
    "    \"\"\"\n",
    "    # Initialize the multi-modal Large Language Model with specific parameters\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "    # Define the RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(get_resized_images),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Load chroma\n",
    "vectorstore_mvr = Chroma(\n",
    "    collection_name=\"image_summaries\",\n",
    "    persist_directory=\"chroma_db_multi_modal\",\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Load redis\n",
    "UPSTASH_URL = \"https://usw1-bright-beagle-34178.upstash.io\"\n",
    "UPSTASH_TOKEN = \"AYWCACQgNzk3OTJjZTItMGIxNy00MTEzLWIyZTAtZWI0ZmI1ZGY0NjFhNGRhMGZjNDE4YjgxNGE4MTkzOWYxMzllM2MzZThlOGY=\"\n",
    "store = UpstashRedisByteStore(url=UPSTASH_URL,\n",
    "                            token=UPSTASH_TOKEN)\n",
    "\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Create the multi-vector retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore_mvr,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Create RAG chain\n",
    "chain = multi_modal_rag_chain(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbe8cf0c-91c6-4bb8-8514-342199260559",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"How many total customers does Datadog have?\"\n",
    "docs = retriever.get_relevant_documents(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71d008e0-9629-4967-9063-dce31f8b5412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Datadog has approximately 26,800 total customers.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
