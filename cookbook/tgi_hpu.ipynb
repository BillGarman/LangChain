{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc935871-7640-41c6-b798-58514d860fe0",
   "metadata": {},
   "source": [
    "# Using Text Generation Inference as Serving framework on Habana Gaudi\n",
    "\n",
    "To use [ðŸ¤— text-generation-inference](https://github.com/huggingface/text-generation-inference) on Habana Gaudi/Gaudi2, please follow these steps:\n",
    "\n",
    "## Build the Docker image located in the tgi-gaudi repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81adcf8b-395a-4f02-8749-ac976942b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/huggingface/tgi-gaudi.git\n",
    "% cd tgi-gaudi/\n",
    "! docker build -t tgi_gaudi ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13ed66-300b-4a23-b8ac-44df68ee4733",
   "metadata": {},
   "source": [
    "If you need to set proxy settings, add `--build-arg https_proxy=$https_proxy --build-arg http_proxy=$http_proxy` like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75a5c6-34ee-4ab9-a664-d9b432d812ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker build -t tgi_gaudi . --build-arg https_proxy=$https_proxy --build-arg http_proxy=$http_proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b724e5",
   "metadata": {},
   "source": [
    "## Launch a local server instance on 1 Gaudi card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f86cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! model=Intel/neural-chat-7b-v3-3\n",
    "! volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a24ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -p 8080:80 -v $volume:/data --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --ipc=host tgi_gaudi --model-id $model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5093cee4",
   "metadata": {},
   "source": [
    "If you need to set proxy settings, add `--build-arg https_proxy=$https_proxy --build-arg http_proxy=$http_proxy` like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -p 8080:80 -v $volume:/data --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --ipc=host -e HTTPS_PROXY=$https_proxy -e HTTP_PROXY=$https_proxy tgi_gaudi --model-id $model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6cf512",
   "metadata": {},
   "source": [
    "For gated models such as `LLAMA-2`, you will have to pass -e HUGGING_FACE_HUB_TOKEN=\\<token\\> to the docker run command above with a valid Hugging Face Hub read token.\n",
    "\n",
    "Send a request to check if the endpoint is wokring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl localhost:8080/generate   -X POST   -d '{\"inputs\":\"Which NFL team won the Super Bowl in the 2010 season?\",\"parameters\":{\"max_new_tokens\":128, \"do_sample\": true}}'   -H 'Content-Type: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43312ca",
   "metadata": {},
   "source": [
    "The first call will be slower as the model is compiled.\n",
    "\n",
    "More details please refer to [tgi-gaudi](https://github.com/huggingface/tgi-gaudi/blob/v1.2-release/README.md).\n",
    "\n",
    "For more information and documentation about Text Generation Inference, checkout the [README](https://github.com/huggingface/text-generation-inference#text-generation-inference) of the original repo.\n",
    "\n",
    "## Install intel optimized langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e4ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/lvliang-intel/intel_genai_kit_langchain.git\n",
    "% cd intel_genai_kit_langchain/libs/langchain/\n",
    "! pip install -e .\n",
    "% cd ../community/\n",
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ddde7",
   "metadata": {},
   "source": [
    "## Install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef54a303",
   "metadata": {},
   "source": [
    "## Access Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "% cd ../examples/serving/tgi/\n",
    "! export HUGGINGFACEHUB_API_TOKEN=<token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3628a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "\n",
    "ENDPOINT_URL = \"http://localhost:8080\"\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    max_new_tokens=512,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
    "    \"Who was the 12th person on the moon?\" +\n",
    "    \"How many eyes does a blade of grass have?\"\n",
    ")\n",
    "\n",
    "print(llm_chain.invoke(qs_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
