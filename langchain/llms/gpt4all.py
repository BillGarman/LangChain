"""Wrapper for the GPT4All model."""
from enum import Enum
from functools import partial
from typing import Any, Dict, List, Optional, Union

from pydantic import Extra, Field, root_validator

from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.base import LLM


class GPT4AllBackendOptions(str, Enum):
    """GPT4's backend model. gpt-j is the default and newer model."""

    gpt = "gpt"
    gpt_j = "gpt-j"


class GPT4All(LLM):
    r"""Wrapper around GPT4All language models.

    To use, you should have the ``pygpt4all`` python package installed, the
    pre-trained model file, and the model's config information.

    Example:
        .. code-block:: python

            from langchain.llms import GPT4All
            model = GPT4All(model_path="~/ggml-gpt4all-j-v1.3-groovy.bin")

            # Simplest invocation
            response = model(prompt="Once upon a time, ")
    """
    client: Any = None

    backend: GPT4AllBackendOptions = Field(GPT4AllBackendOptions.gpt_j)
    """GPT-J based models are newer models and the default option."""

    model_path: str
    """Path to the pre-trained GPT4All model file."""

    antiprompt: str
    """The stop word, the generation will stop if this word is predicted"""

    n_predict: Optional[Union[None, int]] = None
    """The maximum number of tokens to generate."""

    seed: Optional[int] = None
    """Seed. If -1, a random seed is used."""

    n_threads: Optional[int] = 4
    """Number of threads to use."""

    top_k: Optional[int] = 40
    """The top-k value to use for sampling."""

    top_p: Optional[float] = 0.9
    """The top-p value to use for sampling."""

    temp: Optional[float] = 0.9
    """The temperature to use for sampling."""

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid

    @property
    def _llm_type(self) -> str:
        """Return the type of llm."""
        return "gpt4all"

    def _gptj_default_params(self) -> Dict[str, Any]:
        """Get the default parameters when calling the GPT4All model."""
        return {
            "antiprompt": self.antiprompt,
            "seed": self.seed,
            "n_predict": self.n_predict,
            "n_threads": self.n_threads,
            "top_k": self.top_k,
            "top_p": self.top_p,
            "temp": self.temp,
        }

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that the python package exists in the environment."""
        try:
            backend = values["backend"]
            if backend == GPT4AllBackendOptions.gpt:
                from pygpt4all import GPT4All as GPT4AllModel
            elif backend == GPT4AllBackendOptions.gpt_j:
                from pygpt4all import GPT4All_J as GPT4AllModel
            else:
                raise ValueError(f"Incorrect gpt4all backend {cls.backend}")

            values["client"] = GPT4AllModel(model_path=values["model_path"])

        except ImportError:
            raise ImportError(
                "Could not import pygpt4all python package. "
                "Please install it with `pip install pygpt4all`."
            )

        return values

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
    ) -> str:
        r"""Call out to GPT4All's generate method.

        Args:
            prompt: The prompt to pass into the model.
            stop: A list of strings to stop generation when encountered.
                  Only one string is supported in GPT4All model.

        Returns:
            The string generated by the model.
        """
        if stop is not None:
            if len(stop) > 1:
                raise ValueError(
                    "The 'stop' parameter should not contain more than one value."
                )
            else:
                self.antiprompt = stop[0]

        text_callback = None
        if run_manager:
            text_callback = partial(run_manager.on_llm_new_token, verbose=self.verbose)

        text = ""
        for token in self.client.generate(prompt, **self._gptj_default_params()):
            if text_callback:
                text_callback(token)
            text += token
        return text
