"""Wrapper around Sagemaker InvokeEndpoint API."""
import json
from typing import Any, List, Mapping, Optional

from pydantic import BaseModel, Extra

from langchain.llms.base import LLM
from langchain.llms.utils import enforce_stop_tokens

VALID_TASKS = ("text2text-generation", "text-generation")


class SagemakerEndpoint(LLM, BaseModel):
    """Wrapper around custom Sagemaker Inference Endpoints.

    To use, you should pass the AWS IAM Role and Role Session Name as
    named parameters to the constructor.

    Only supports `text-generation` and `text2text-generation` for now.
    """

    """
    Example:
        .. code-block:: python

            from langchain import SagemakerEndpoint
            endpoint_name = (
                "https://runtime.sagemaker.us-west-2.amazonaws.com/endpoints/abcdefghijklmnop/invocations"
            )
            se = SagemakerEndpoint(
                endpoint_name=endpoint_name,
                role_arn="role_arn",
                role_session_name="role_session_name"
            )
    """

    endpoint_name: str = ""
    """# The name of the endpoint. Must be unique within an AWS Region."""
    task: Optional[str] = None
    """Task to call the model with. Should be a task that returns `generated_text`."""
    model_kwargs: Optional[dict] = None
    """Key word arguments to pass to the model."""

    role_arn: Optional[str] = None
    role_session_name: Optional[str] = None

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        _model_kwargs = self.model_kwargs or {}
        return {
            **{"endpoint_name": self.endpoint_name, "task": self.task},
            **{"model_kwargs": _model_kwargs},
        }

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "sagemaker_endpoint"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """Call out to Sagemaker inference endpoint.

        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.

        Returns:
            The string generated by the model.

        Example:
            .. code-block:: python

                response = se("Tell me a joke.")
        """
        import boto3

        session = boto3.Session(profile_name="test-profile-name")
        sagemaker_runtime = session.client("sagemaker-runtime", region_name="us-west-2")

        # TODO: use AWS IAM assumed roles to authenticate from the EC2 instance
        # def role_arn_to_session(**args):
        #     """
        #     Usage :
        #         session = role_arn_to_session(
        #             RoleArn='arn:aws:iam::012345678901:role/example-role',
        #             RoleSessionName='ExampleSessionName')
        #         client = session.client('sqs')
        #     """
        #     client = boto3.client('sts')
        #     response = client.assume_role(**args)
        #     return boto3.Session(
        #         aws_access_key_id=response['Credentials']['AccessKeyId'],
        #         aws_secret_access_key=response['Credentials']['SecretAccessKey'],
        #         aws_session_token=response['Credentials']['SessionToken'])

        # session = role_arn_to_session(RoleArn="$role-arn",
        # RoleSessionName="test-role-session-name")
        # sagemaker_runtime = session.client(
        # "sagemaker-runtime", region_name="us-west-2"
        # )

        _model_kwargs = self.model_kwargs or {}

        # payload samples
        parameter_payload = {"inputs": prompt, "parameters": _model_kwargs}

        input_en = json.dumps(parameter_payload).encode("utf-8")

        # send request
        try:
            response = sagemaker_runtime.invoke_endpoint(
                EndpointName=self.endpoint_name,
                Body=input_en,
                ContentType="application/json",
            )
        except Exception as e:
            raise ValueError(f"Error raised by inference endpoint: {e}")

        response_json = json.loads(response["Body"].read().decode("utf-8"))
        text = response_json[0]["generated_text"]
        if stop is not None:
            # This is a bit hacky, but I can't figure out a better way to enforce
            # stop tokens when making calls to huggingface_hub.
            text = enforce_stop_tokens(text, stop)

        return text
