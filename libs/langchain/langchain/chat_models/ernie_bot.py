from typing import Any, AsyncIterator, Dict, Iterator, List, Mapping, Optional

from langchain.callbacks.manager import (
    AsyncCallbackManagerForLLMRun,
    CallbackManagerForLLMRun,
)
from langchain.chat_models.base import BaseChatModel
from langchain.pydantic_v1 import root_validator
from langchain.schema import ChatGeneration, ChatResult
from langchain.schema.messages import (
    AIMessage,
    AIMessageChunk,
    BaseMessage,
    ChatMessage,
    FunctionMessage,
    HumanMessage,
)
from langchain.schema.output import ChatGenerationChunk
from langchain.utils import get_from_dict_or_env


def _convert_message_to_dict(message: BaseMessage) -> dict:
    message_dict: Dict[str, Any]
    if isinstance(message, ChatMessage):
        message_dict = {"role": message.role, "content": message.content}
    elif isinstance(message, HumanMessage):
        message_dict = {"role": "user", "content": message.content}
    elif isinstance(message, AIMessage):
        message_dict = {"role": "assistant", "content": message.content}
        if "function_call" in message.additional_kwargs:
            message_dict["function_call"] = message.additional_kwargs["function_call"]
            # If function call only, content is None not empty string
            if message_dict["content"] == "":
                message_dict["content"] = None
    elif isinstance(message, FunctionMessage):
        message_dict = {
            "role": "function",
            "content": message.content,
            "name": message.name,
        }
    else:
        raise TypeError(f"Got unknown type {message}")

    return message_dict


def _convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:
    content = _dict.get("result", "") or ""
    if _dict.get("function_call"):
        additional_kwargs = {"function_call": dict(_dict["function_call"])}
    else:
        additional_kwargs = {}
    return AIMessage(content=content, additional_kwargs=additional_kwargs)


SUPPORTED_MODELS = {"ERNIE-Bot": "ernie-bot-3.5", "ERNIE-Bot-turbo": "ernie-bot-turbo"}


class ErnieBotChat(BaseChatModel):
    """`ERNIE-Bot` large language model for ChatModel.

    This ChatModel is the implement only about Baidu ERNIE ErnieBot.
    Use Baidu `langchain.chat_models.QianfanChatEndpoint` for more
    chat models.

    ERNIE-Bot is a large language model developed by Baidu,
    covering a huge amount of Chinese data.

    To use, you should have the `ernie_client_id` and `ernie_client_secret` set,
    or set the environment variable `ERNIE_CLIENT_ID` and `ERNIE_CLIENT_SECRET`.

    Note:
    access_token will be automatically generated based on client_id and client_secret,
    and will be regenerated after expiration (30 days).

    Default model is `ERNIE-Bot-turbo`,
    currently supported models are `ERNIE-Bot-turbo`, `ERNIE-Bot`

    Example:
        .. code-block:: python

            from langchain.chat_models import ErnieBotChat
            chat = ErnieBotChat(model_name='ERNIE-Bot')

    """

    ernie_api_base: Optional[str] = None
    """Baidu application custom endpoints"""

    ernie_client_id: Optional[str] = None
    """Baidu application client id"""

    ernie_client_secret: Optional[str] = None
    """Baidu application client secret"""

    access_token: Optional[str] = None
    """access token is generated by client id and client secret, 
    setting this value directly will cause an error"""

    model_name: str = "ERNIE-Bot-turbo"
    """model name of ernie, default is `ernie-bot-turbo`.
      Currently supported `ERNIE-Bot`, `ERNIE-Bot-turbo`.
      Models below are supported by `langchain.chat_models.QianfanEndpoint`
        "BLOOMZ-7B"ï¼Œ"Llama-2-7b-chat", "Llama-2-13b-chat", "Llama-2-70b-chat",
      """

    request_timeout: Optional[int] = 60
    """request timeout for chat http requests"""

    streaming: Optional[bool] = False
    """streaming mode. not supported yet."""

    top_p: Optional[float] = 0.8
    temperature: Optional[float] = 0.95
    penalty_score: Optional[float] = 1
    """common model arguments"""

    client: Any
    """ERNIE-BOT SDK resource module"""

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        values["ernie_api_base"] = get_from_dict_or_env(
            values, "ernie_api_base", "ERNIE_API_BASE", "https://aip.baidubce.com"
        )
        values["ernie_client_id"] = get_from_dict_or_env(
            values,
            "ernie_client_id",
            "ERNIE_CLIENT_ID",
        )
        values["ernie_client_secret"] = get_from_dict_or_env(
            values,
            "ernie_client_secret",
            "ERNIE_CLIENT_SECRET",
        )
        if values.get("model_name") not in SUPPORTED_MODELS:
            raise ValueError(
                "model '{self.model_name}' not supported,"
                "try use QianfanEndpoint instead."
            )
        try:
            import erniebot

            erniebot.ak = values["ernie_client_id"]
            erniebot.sk = values["ernie_client_secret"]

            values["client"] = erniebot.ChatCompletion
        except ImportError:
            raise ValueError(
                "erniebot package not found, please install it with "
                "`pip install erniebot`"
            )

        return values

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        if self.streaming:
            completion = ""
            additional_kwargs: Dict[str, Any] = {}
            for chunk in self._stream(messages, stop, run_manager, **kwargs):
                completion += chunk.text
                additional_kwargs = {
                    **additional_kwargs,
                    **chunk.message.additional_kwargs,
                }
            lc_msg = AIMessage(content=completion, additional_kwargs=additional_kwargs)
            gen = ChatGeneration(
                message=lc_msg,
                generation_info=dict(finish_reason="stop"),
            )
            return ChatResult(
                generations=[gen],
                llm_output={"token_usage": {}, "model_name": self.model_name},
            )
        payload = self._create_client_param(messages, **kwargs)
        resp = self.client.create(**payload)
        return self._create_chat_result(resp)

    def _create_chat_result(self, response: Mapping[str, Any]) -> ChatResult:
        generations = [
            ChatGeneration(
                message=_convert_dict_to_message(response),
                generation_info=dict(finish_reason="stop"),
            )
        ]
        token_usage = response.get("usage", {})
        llm_output = {"token_usage": token_usage, "model_name": self.model_name}
        return ChatResult(generations=generations, llm_output=llm_output)

    def _create_client_param(self, messages: List[BaseMessage], **kwargs: Any) -> dict:
        messages_list = [_convert_message_to_dict(m) for m in messages]
        return {
            **{"messages": messages_list},
            **kwargs,
            **self._default_params,
        }

    async def _agenerate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        streaming: Optional[bool] = None,
        **kwargs: Any,
    ) -> ChatResult:
        if streaming if streaming is not None else self.streaming:
            completion = ""
            additional_kwargs: Dict[str, Any] = {}
            async for chunk in self._astream(
                messages, stop, run_manager, streaming=True, **kwargs
            ):
                completion += chunk.text
                additional_kwargs = {
                    **additional_kwargs,
                    **chunk.message.additional_kwargs,
                }
            lc_msg = AIMessage(content=completion, additional_kwargs=additional_kwargs)
            gen = ChatGeneration(
                message=lc_msg,
                generation_info=dict(finish_reason="stop"),
            )
            return ChatResult(
                generations=[gen],
                llm_output={"token_usage": {}, "model_name": self.model_name},
            )
        payload = self._create_client_param(messages, **kwargs)
        resp = await self.client.acreate(**payload)
        return self._create_chat_result(resp)

    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[ChatGenerationChunk]:
        params = self._convert_prompt_msg_params(messages, **{**kwargs, "stream": True})
        for res in self.client.create(**params):
            if res:
                msg = _convert_dict_to_message(res)
                chunk = ChatGenerationChunk(
                    text=res["result"],
                    message=AIMessageChunk(
                        content=msg.content,
                        role="assistant",
                        additional_kwargs=msg.additional_kwargs,
                    ),
                )
                yield chunk
                if run_manager:
                    run_manager.on_llm_new_token(chunk.text, chunk=chunk)

    async def _astream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> AsyncIterator[ChatGenerationChunk]:
        params = self._convert_prompt_msg_params(messages, **{**kwargs, "stream": True})
        async for res in await self.client.acreate(**params):
            if res:
                msg = _convert_dict_to_message(res)
                chunk = ChatGenerationChunk(
                    text=res["result"],
                    message=AIMessageChunk(
                        content=msg.content,
                        role="assistant",
                        additional_kwargs=msg.additional_kwargs,
                    ),
                )
                yield chunk
                if run_manager:
                    await run_manager.on_llm_new_token(chunk.text, chunk=chunk)

    def _convert_prompt_msg_params(
        self,
        messages: List[BaseMessage],
        **kwargs: Any,
    ) -> dict:
        """
        Converts a list of messages into a dictionary containing the message
        content and default parameters.

        Args:
            messages (List[BaseMessage]): The list of messages.
            **kwargs (Any): Optional arguments to add additional parameters
                to the resulting dictionary.

        Returns:
            Dict[str, Any]: A dictionary containing the message content and
                default parameters.

        """
        messages_list = []
        for m in messages:
            messages_list += [_convert_message_to_dict(m)]

        if "streaming" in kwargs:
            kwargs["stream"] = kwargs.pop("streaming")
        return {
            **{"messages": messages_list},
            **self._default_params,
            **kwargs,
        }

    @property
    def _llm_type(self) -> str:
        return "ernie-bot-chat"

    @property
    def _default_params(self) -> Dict[str, Any]:
        """Get the default parameters for calling OpenAI API."""
        normal_params = {
            "model": SUPPORTED_MODELS[self.model_name],
            "stream": self.streaming,
            "request_timeout": self.request_timeout,
            "top_p": self.top_p,
            "temperature": self.temperature,
            "penalty_score": self.penalty_score,
        }

        return normal_params
