"""BigFramesChain that just formats a prompt and calls an LLM."""
from __future__ import annotations

import warnings
import logging
from typing import Any, Dict, List, Optional, Sequence, Tuple, Union

import bigframes
import bigframes.pandas as bf
import inspect

from langchain.callbacks.manager import (
    CallbackManager,
    CallbackManagerForChainRun,
    Callbacks,
)
from langchain.chains.base import Chain
from langchain.load.dump import dumpd
from langchain.prompts.prompt import PromptTemplate
from langchain.pydantic_v1 import Extra, Field
from langchain.schema import (
    BaseLLMOutputParser,
    BasePromptTemplate,
    LLMResult,
    PromptValue,
    StrOutputParser,
)
from langchain.schema.language_model import BaseLanguageModel
from langchain.utils.input import get_colored_text


logger = logging.getLogger(__name__)

class BigFramesChain(Chain):
    @classmethod
    def is_lc_serializable(self) -> bool:
        return True
    
    prompt: BasePromptTemplate
    """Prompt object to use."""
    llm: BaseLanguageModel
    """Language model to call."""
    output_key: str = "text"  #: :meta private:
    output_parser: BaseLLMOutputParser = Field(default_factory=StrOutputParser)
    """Output parser to use.
    Defaults to one that takes the most likely string but does not change it 
    otherwise."""
    return_final_only: bool = True
    """Whether to return only the final parsed result. Defaults to True.
    If false, will return a bunch of extra information about the generation."""
    llm_kwargs: dict = Field(default_factory=dict)

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid
        arbitrary_types_allowed = True

    @property
    def input_keys(self) -> List[str]:
        """Will be whatever keys the prompt expects.

        :meta private:
        """
        return self.prompt.input_variables
    
    @property
    def output_keys(self) -> List[str]:
        """Will always return text key.

        :meta private:
        """
        if self.return_final_only:
            return [self.output_key]
        else:
            return [self.output_key, "full_generation"]
    
    def __call__(
        self,
        inputs: Union[Dict[str, Any], Any],
        return_only_outputs: bool = False,
        callbacks: Callbacks = None,
        *,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        run_name: Optional[str] = None,
        include_run_info: bool = False,
    ) -> bf.DataFrame:
        """Execute the chain.

        Args:
            inputs: Dictionary of inputs, or single input if chain expects
                only one param. Should contain all inputs specified in
                `Chain.input_keys` except for inputs that will be set by the chain's
                memory.
            return_only_outputs: Whether to return only outputs in the
                response. If True, only new keys generated by this chain will be
                returned. If False, both input keys and new keys generated by this
                chain will be returned. Defaults to False.
            callbacks: Callbacks to use for this chain run. These will be called in
                addition to callbacks passed to the chain during construction, but only
                these runtime callbacks will propagate to calls to other objects.
            tags: List of string tags to pass to all callbacks. These will be passed in
                addition to tags passed to the chain during construction, but only
                these runtime tags will propagate to calls to other objects.
            metadata: Optional metadata associated with the chain. Defaults to None
            include_run_info: Whether to include run info in the response. Defaults
                to False.

        Returns:
            bf.pandas.DataFrame
        """
        if isinstance(inputs, (Dict, str)):
            inputs = self.prep_inputs(inputs)
            callback_manager = CallbackManager.configure(
                callbacks,
                self.callbacks,
                self.verbose,
                tags,
                self.tags,
                metadata,
                self.metadata,
            )
            new_arg_supported = inspect.signature(self._call).parameters.get("run_manager")
            run_manager = callback_manager.on_chain_start(
                dumpd(self),
                inputs,
                name=run_name,
            )
        else:
            inputs = inputs
            return self._call(inputs)
        try:
            outputs = (
                self._call(inputs, run_manager=run_manager)
                if new_arg_supported
                else self._call(inputs)
            )
        except BaseException as e:
            run_manager.on_chain_error(e)
            raise e
        run_manager.on_chain_end(outputs)
        return outputs
    

    def run(
        self,
        *args: Any,
        callbacks: Callbacks = None,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> Any:
        """Convenience method for executing chain.

        The main difference between this method and `Chain.__call__` is that this
        method expects inputs to be passed directly in as positional arguments or
        keyword arguments, whereas `Chain.__call__` expects a single input dictionary
        with all the inputs

        Args:
            *args: If the chain expects a single input, it can be passed in as the
                sole positional argument.
            callbacks: Callbacks to use for this chain run. These will be called in
                addition to callbacks passed to the chain during construction, but only
                these runtime callbacks will propagate to calls to other objects.
            tags: List of string tags to pass to all callbacks. These will be passed in
                addition to tags passed to the chain during construction, but only
                these runtime tags will propagate to calls to other objects.
            **kwargs: If the chain expects multiple inputs, they can be passed in
                directly as keyword arguments.

        Returns:
            The chain output.

        Example:
            .. code-block:: python

                # Suppose we have a single-input chain that takes a 'question' string:
                chain.run("What's the temperature in Boise, Idaho?")
                # -> "The temperature in Boise is..."

                # Suppose we have a multi-input chain that takes a 'question' string
                # and 'context' string:
                question = "What's the temperature in Boise, Idaho?"
                context = "Weather report for Boise, Idaho on 07/03/23..."
                chain.run(question=question, context=context)
                # -> "The temperature in Boise is..."
        """
        if args and not kwargs:
            if len(args) != 1:
                raise ValueError("`run` supports only one positional argument.")
            logger.error("hello 1")
            return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)

        if kwargs and not args:
            logger.error("hello 2")
            return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)

        if not kwargs and not args:
            raise ValueError(
                "`run` supported with either positional arguments or keyword arguments,"
                " but none were provided."
            )
        else:
            raise ValueError(
                f"`run` supported with either positional arguments or keyword arguments"
                f" but not both. Got args: {args} and kwargs: {kwargs}."
            )
    

    def _call(
        self,
        inputs: Union[Dict[str, Any], bf.DataFrame, bf.Series],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> bf.DataFrame:
        if not isinstance(inputs, (bf.DataFrame, bf.Series)):
            response = self.generate([inputs], run_manager=run_manager)
        else:
            response = self.generate(inputs, run_manager=run_manager)
        return response

    
    def generate(
        self,
        input: Union[Dict[str, Any], bf.DataFrame, bf.Series],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> bf.DataFrame:
        """Generate LLM result from inputs."""
        if not isinstance(input, (bf.DataFrame, bf.Series)):
            prompts, stop = self.prep_prompts(input, run_manager=run_manager)
        else:
            prompts = self.prep_df_prompts(input)
            stop = None
        return self.llm.generate_df_prompt(
            prompts,
            stop,
            callbacks=run_manager.get_child() if run_manager else None,
            **self.llm_kwargs,
        )
    

    def prep_df_prompts(self,
        input: Union[bf.DataFrame, bf.Series]
    ) -> Union[bf.DataFrame, bf.Series]:
        prompt_template = self.prompt.template
        # Replace {input_variable}
        formatted1 = prompt_template[:prompt_template.find("{")]
        formatted2 = prompt_template[prompt_template.find("{"):]
        df_prompt = (formatted1 + input + formatted2)
        return df_prompt
    
    
    def prep_prompts(
        self,
        input_list: List[Dict[str, Any]],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> Tuple[List[PromptValue], Optional[List[str]]]:
        """Prepare prompts from inputs."""
        stop = None
        if len(input_list) == 0:
            return [], stop
        if "stop" in input_list[0]:
            stop = input_list[0]["stop"]
        prompts = []
        for inputs in input_list:
            selected_inputs = {k: inputs[k] for k in self.prompt.input_variables}
            prompt = self.prompt.format_prompt(**selected_inputs)
            _colored_text = get_colored_text(prompt.to_string(), "green")
            _text = "Prompt after formatting:\n" + _colored_text
            if run_manager:
                run_manager.on_text(_text, end="\n", verbose=self.verbose)
            if "stop" in inputs and inputs["stop"] != stop:
                raise ValueError(
                    "If `stop` is present in any inputs, should be present in all."
                )
            prompts.append(prompt)
        return prompts, stop
    

    def predict(self, callbacks: Callbacks = None, **kwargs: Any) -> bf.DataFrame:
        """Format prompt with kwargs and pass to LLM.

        Args:
            callbacks: Callbacks to pass to LLMChain
            **kwargs: Keys to pass to prompt template.

        Returns:
            Completion from LLM.

        Example:
            .. code-block:: python

                completion = llm.predict(adjective="funny")
        """
        return self(kwargs, callbacks=callbacks)
    
    @property
    def _chain_type(self) -> str:
        return "bigframes_chain"

    @classmethod
    def from_string(cls, llm: BaseLanguageModel, template: str) -> BigFramesChain:
        """Create LLMChain from LLM and template."""
        prompt_template = PromptTemplate.from_template(template)
        return cls(llm=llm, prompt=prompt_template)